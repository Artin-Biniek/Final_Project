input_filename,input_content,output_filename,output_content
/content/Record1.rst,"{
Scenario:Tag-Expressions v2
-------------------------------------------------------------------------------

Tag-Expressions v2 are based on :pypi:`cucumber-tag-expressions` with some extensions:

* Tag-Expressions v2 provide `boolean logic expression`
  (with ``and``, ``or`` and ``not`` operators and parenthesis for grouping expressions)
* Tag-Expressions v2 are far more readable and composable than Tag-Expressions v1
* Some boolean-logic-expressions where not possible with Tag-Expressions v1
* Therefore, Tag-Expressions v2 supersedes the old-style tag-expressions.


.. code-block:: gherkin
    :caption: TAG-EXPRESSION EXAMPLES

    # -- EXAMPLE 1: Select features/scenarios that have the tags: @a and @b
    @a and @b

    # -- EXAMPLE 2: Select features/scenarios that have the tag: @a or @b
    @a or @b

    # -- EXAMPLE 3: Select features/scenarios that do not have the tag: @a
    not @a

    # -- EXAMPLE 4: Select features/scenarios that have the tags: @a but not @b
    @a and not @b

    # -- EXAMPLE 5: Select features/scenarios that have the tags: (@a or @b) but not @c
    # HINT: Boolean expressions can be grouped with parenthesis.
    (@a or @b) and not @c

Given:COMMAND-LINE EXAMPLE:

.. code-block:: sh
    :caption: USING: Tag-Expressions v2 with ``behave``

    # -- SELECT-BY-TAG-EXPRESSION (with tag-expressions v2):
    # Select all features / scenarios with both ""@foo"" and ""@bar"" tags.
    $ behave --tags=""@foo and @bar"" features/

    # -- EXAMPLE: Use default_tags from config-file ""behave.ini"".
    # Use placeholder ""{config.tags}"" to refer to this tag-expression.
    # HERE: config.tags = ""not (@xfail or @not_implemented)""
    $ behave --tags=""(@foo or @bar) and {config.tags}"" --tags-help
    ...
    CURRENT TAG_EXPRESSION: ((foo or bar) and not (xfail or not_implemented))

    # -- EXAMPLE: Uses Tag-Expression diagnostics with --tags-help option
    $ behave --tags=""(@foo and @bar) or @baz"" --tags-help
    $ behave --tags=""(@foo and @bar) or @baz"" --tags-help --verbose

.. seealso::

    * https://docs.cucumber.io/cucumber/api/#tag-expressions
    * :pypi:`cucumber-tag-expressions` (Python package)


Then:Tag Matching with Tag-Expressions
-------------------------------------------------------------------------------

Tag-Expressions v2 support **partial string/tag matching** with wildcards.
This supports tag-expressions:

=================== =========== =========== ===================================================
Tag Matching Idiom  Example 1   Example 2   Description
=================== =========== =========== ===================================================
``tag.starts_with`` ``@foo.*``  ``foo.*``   Search for tags that start with a ``prefix``.
``tag.ends_with``   ``@*.one``  ``*.one``   Search for tags that end with a ``suffix``.
``tag.contains``    ``@*foo*``  ``*foo*``   Search for tags that contain a ``part``.
=================== =========== =========== ===================================================

.. code-block:: gherkin
    :caption: FILE: features/one.feature

    Feature: Alice

      @foo.one
      Scenario: Alice.1
        ...

      @foo.two
      Scenario: Alice.2
        ...

      @bar
      Scenario: Alice.3
        ...

The following command-line will select all features / scenarios with tags
that start with ""@foo."":

.. code-block:: sh
    :caption: USAGE EXAMPLE: Run behave with tag-matching expressions

    $ behave -f plain --tags=""@foo.*"" features/one.feature
    Feature: Alice

      Scenario: Alice.1
        ...

      Scenario: Alice.2
        ...

    # -- HINT: Only Alice.1 and Alice.2 are matched (not: Alice.3).

.. note::

    * Filename matching wildcards are supported.
      See :mod:`fnmatch` (Unix style filename matching).

    * The tag matching functionality is an extension to :pypi:`cucumber-tag-expressions`.


Select the Tag-Expression Version to Use
-------------------------------------------------------------------------------

The tag-expression version, that should be used by :pypi:`behave`,
can be specified in the :pypi:`behave` config-file.

This allows a user to select:

* Tag-Expressions v1 (if needed)
* Tag-Expressions v2 when it is feasible

EXAMPLE:

.. code-block:: ini
    :caption: FILE: behave.ini

    # SPECIFY WHICH TAG-EXPRESSION-PROTOCOL SHOULD BE USED:
    #   SUPPORTED VALUES: v1, v2, auto_detect
    #   CURRENT DEFAULT:  auto_detect
    [behave]
    tag_expression_protocol = v1    # -- Use Tag-Expressions v1.


Tag-Expressions v1
-------------------------------------------------------------------------------

Tag-Expressions v1 are becoming deprecated (but are currently still supported).
Use **Tag-Expressions v2** instead.

.. note::

    Tag-Expressions v1 support will be dropped in ``behave v1.4.0``.

}

",/content/Feature1.feature,"{
Scenario: Select @foo
    Given: the tag expression ""@foo""
    Then: the tag expression selects elements with tags:
        | tags         | selected? |
        |              |   no      |
        | @foo         |   yes     |
        | @other       |   no      |
        | @foo @other  |   yes     |

}"
/content/Record2.rst,"{
Scenario:Tag-Expressions v2
-------------------------------------------------------------------------------

Tag-Expressions v2 are based on :pypi:`cucumber-tag-expressions` with some extensions:

* Tag-Expressions v2 provide `boolean logic expression`
  (with ``and``, ``or`` and ``not`` operators and parenthesis for grouping expressions)
* Tag-Expressions v2 are far more readable and composable than Tag-Expressions v1
* Some boolean-logic-expressions where not possible with Tag-Expressions v1
* Therefore, Tag-Expressions v2 supersedes the old-style tag-expressions.


.. code-block:: gherkin
    :caption: TAG-EXPRESSION EXAMPLES

    # -- EXAMPLE 1: Select features/scenarios that have the tags: @a and @b
    @a and @b

    # -- EXAMPLE 2: Select features/scenarios that have the tag: @a or @b
    @a or @b

    # -- EXAMPLE 3: Select features/scenarios that do not have the tag: @a
    not @a

    # -- EXAMPLE 4: Select features/scenarios that have the tags: @a but not @b
    @a and not @b

    # -- EXAMPLE 5: Select features/scenarios that have the tags: (@a or @b) but not @c
    # HINT: Boolean expressions can be grouped with parenthesis.
    (@a or @b) and not @c

Given:COMMAND-LINE EXAMPLE:

.. code-block:: sh
    :caption: USING: Tag-Expressions v2 with ``behave``

    # -- SELECT-BY-TAG-EXPRESSION (with tag-expressions v2):
    # Select all features / scenarios with both ""@foo"" and ""@bar"" tags.
    $ behave --tags=""@foo and @bar"" features/

    # -- EXAMPLE: Use default_tags from config-file ""behave.ini"".
    # Use placeholder ""{config.tags}"" to refer to this tag-expression.
    # HERE: config.tags = ""not (@xfail or @not_implemented)""
    $ behave --tags=""(@foo or @bar) and {config.tags}"" --tags-help
    ...
    CURRENT TAG_EXPRESSION: ((foo or bar) and not (xfail or not_implemented))

    # -- EXAMPLE: Uses Tag-Expression diagnostics with --tags-help option
    $ behave --tags=""(@foo and @bar) or @baz"" --tags-help
    $ behave --tags=""(@foo and @bar) or @baz"" --tags-help --verbose

.. seealso::

    * https://docs.cucumber.io/cucumber/api/#tag-expressions
    * :pypi:`cucumber-tag-expressions` (Python package)


Then:Tag Matching with Tag-Expressions
-------------------------------------------------------------------------------

Tag-Expressions v2 support **partial string/tag matching** with wildcards.
This supports tag-expressions:

=================== =========== =========== ===================================================
Tag Matching Idiom  Example 1   Example 2   Description
=================== =========== =========== ===================================================
``tag.starts_with`` ``@foo.*``  ``foo.*``   Search for tags that start with a ``prefix``.
``tag.ends_with``   ``@*.one``  ``*.one``   Search for tags that end with a ``suffix``.
``tag.contains``    ``@*foo*``  ``*foo*``   Search for tags that contain a ``part``.
=================== =========== =========== ===================================================

.. code-block:: gherkin
    :caption: FILE: features/one.feature

    Feature: Alice

      @foo.one
      Scenario: Alice.1
        ...

      @foo.two
      Scenario: Alice.2
        ...

      @bar
      Scenario: Alice.3
        ...

The following command-line will select all features / scenarios with tags
that start with ""@foo."":

.. code-block:: sh
    :caption: USAGE EXAMPLE: Run behave with tag-matching expressions

    $ behave -f plain --tags=""@foo.*"" features/one.feature
    Feature: Alice

      Scenario: Alice.1
        ...

      Scenario: Alice.2
        ...

    # -- HINT: Only Alice.1 and Alice.2 are matched (not: Alice.3).

.. note::

    * Filename matching wildcards are supported.
      See :mod:`fnmatch` (Unix style filename matching).

    * The tag matching functionality is an extension to :pypi:`cucumber-tag-expressions`.


Select the Tag-Expression Version to Use
-------------------------------------------------------------------------------

The tag-expression version, that should be used by :pypi:`behave`,
can be specified in the :pypi:`behave` config-file.

This allows a user to select:

* Tag-Expressions v1 (if needed)
* Tag-Expressions v2 when it is feasible

EXAMPLE:

.. code-block:: ini
    :caption: FILE: behave.ini

    # SPECIFY WHICH TAG-EXPRESSION-PROTOCOL SHOULD BE USED:
    #   SUPPORTED VALUES: v1, v2, auto_detect
    #   CURRENT DEFAULT:  auto_detect
    [behave]
    tag_expression_protocol = v1    # -- Use Tag-Expressions v1.


Tag-Expressions v1
-------------------------------------------------------------------------------

Tag-Expressions v1 are becoming deprecated (but are currently still supported).
Use **Tag-Expressions v2** instead.

.. note::

    Tag-Expressions v1 support will be dropped in ``behave v1.4.0``.

}

",/content/Feature2.feature,"Scenario: Tag expression with 0..1 tags
    Given the model elements with name and tags:
        | name | tags         | Comment |
        | S0   |              | Untagged    |
        | S1   | @foo         | With 1 tag  |
        | S2   | @other       |             |
        | S3   | @foo @other  | With 2 tags |
    And note that ""are all combinations of 0..2 tags""
    Then the tag expression selects model elements with:
        | tag expression | selected?      | Case comment |
        |                | S0, S1, S2, S3 | Select all (empty tag expression) |
        |  @foo          | S1, S3         | Select @foo                       |
        |  not @foo      | S0, S2         | not @foo, selects untagged elements |
    But note that ""tag expression variants are also supported""
    And the tag expression selects model elements with:
        | tag expression | selected?      | Case comment |
        |  foo           | S1, S3         |     @foo: '@' is optional     |
        | not foo        | S0, S2         | not @foo: '@' is optional     |
        | not @foo       | S0, S2         | not @foo: '~@' is supported   |"
/content/Record3.rst,"{
Scenario:Tag-Expressions v2
-------------------------------------------------------------------------------

Tag-Expressions v2 are based on :pypi:`cucumber-tag-expressions` with some extensions:

* Tag-Expressions v2 provide `boolean logic expression`
  (with ``and``, ``or`` and ``not`` operators and parenthesis for grouping expressions)
* Tag-Expressions v2 are far more readable and composable than Tag-Expressions v1
* Some boolean-logic-expressions where not possible with Tag-Expressions v1
* Therefore, Tag-Expressions v2 supersedes the old-style tag-expressions.


.. code-block:: gherkin
    :caption: TAG-EXPRESSION EXAMPLES

    # -- EXAMPLE 1: Select features/scenarios that have the tags: @a and @b
    @a and @b

    # -- EXAMPLE 2: Select features/scenarios that have the tag: @a or @b
    @a or @b

    # -- EXAMPLE 3: Select features/scenarios that do not have the tag: @a
    not @a

    # -- EXAMPLE 4: Select features/scenarios that have the tags: @a but not @b
    @a and not @b

    # -- EXAMPLE 5: Select features/scenarios that have the tags: (@a or @b) but not @c
    # HINT: Boolean expressions can be grouped with parenthesis.
    (@a or @b) and not @c

Given:COMMAND-LINE EXAMPLE:

.. code-block:: sh
    :caption: USING: Tag-Expressions v2 with ``behave``

    # -- SELECT-BY-TAG-EXPRESSION (with tag-expressions v2):
    # Select all features / scenarios with both ""@foo"" and ""@bar"" tags.
    $ behave --tags=""@foo and @bar"" features/

    # -- EXAMPLE: Use default_tags from config-file ""behave.ini"".
    # Use placeholder ""{config.tags}"" to refer to this tag-expression.
    # HERE: config.tags = ""not (@xfail or @not_implemented)""
    $ behave --tags=""(@foo or @bar) and {config.tags}"" --tags-help
    ...
    CURRENT TAG_EXPRESSION: ((foo or bar) and not (xfail or not_implemented))

    # -- EXAMPLE: Uses Tag-Expression diagnostics with --tags-help option
    $ behave --tags=""(@foo and @bar) or @baz"" --tags-help
    $ behave --tags=""(@foo and @bar) or @baz"" --tags-help --verbose

.. seealso::

    * https://docs.cucumber.io/cucumber/api/#tag-expressions
    * :pypi:`cucumber-tag-expressions` (Python package)


Then:Tag Matching with Tag-Expressions
-------------------------------------------------------------------------------

Tag-Expressions v2 support **partial string/tag matching** with wildcards.
This supports tag-expressions:

=================== =========== =========== ===================================================
Tag Matching Idiom  Example 1   Example 2   Description
=================== =========== =========== ===================================================
``tag.starts_with`` ``@foo.*``  ``foo.*``   Search for tags that start with a ``prefix``.
``tag.ends_with``   ``@*.one``  ``*.one``   Search for tags that end with a ``suffix``.
``tag.contains``    ``@*foo*``  ``*foo*``   Search for tags that contain a ``part``.
=================== =========== =========== ===================================================

.. code-block:: gherkin
    :caption: FILE: features/one.feature

    Feature: Alice

      @foo.one
      Scenario: Alice.1
        ...

      @foo.two
      Scenario: Alice.2
        ...

      @bar
      Scenario: Alice.3
        ...

The following command-line will select all features / scenarios with tags
that start with ""@foo."":

.. code-block:: sh
    :caption: USAGE EXAMPLE: Run behave with tag-matching expressions

    $ behave -f plain --tags=""@foo.*"" features/one.feature
    Feature: Alice

      Scenario: Alice.1
        ...

      Scenario: Alice.2
        ...

    # -- HINT: Only Alice.1 and Alice.2 are matched (not: Alice.3).

.. note::

    * Filename matching wildcards are supported.
      See :mod:`fnmatch` (Unix style filename matching).

    * The tag matching functionality is an extension to :pypi:`cucumber-tag-expressions`.


Select the Tag-Expression Version to Use
-------------------------------------------------------------------------------

The tag-expression version, that should be used by :pypi:`behave`,
can be specified in the :pypi:`behave` config-file.

This allows a user to select:

* Tag-Expressions v1 (if needed)
* Tag-Expressions v2 when it is feasible

EXAMPLE:

.. code-block:: ini
    :caption: FILE: behave.ini

    # SPECIFY WHICH TAG-EXPRESSION-PROTOCOL SHOULD BE USED:
    #   SUPPORTED VALUES: v1, v2, auto_detect
    #   CURRENT DEFAULT:  auto_detect
    [behave]
    tag_expression_protocol = v1    # -- Use Tag-Expressions v1.


Tag-Expressions v1
-------------------------------------------------------------------------------

Tag-Expressions v1 are becoming deprecated (but are currently still supported).
Use **Tag-Expressions v2** instead.

.. note::

    Tag-Expressions v1 support will be dropped in ``behave v1.4.0``.

}

",/content/Feature3.feature,"Scenario: Tag expression with two tags (@foo, @bar)
    Given the model elements with name and tags:
        | name | tags             | Comment |
        | S0   |                  | Untagged    |
        | S1   | @foo             | With 1 tag  |
        | S2   | @bar             |             |
        | S3   | @other           |             |
        | S4   | @foo @bar        | With 2 tags |
        | S5   | @foo @other      |             |
        | S6   | @bar @other      |             |
        | S7   | @foo @bar @other | With 3 tags |
    And note that ""are all combinations of 0..3 tags""
    Then the tag expression selects model elements with:
        | tag expression         | selected?                      | Case |
        |                        | S0, S1, S2, S3, S4, S5, S6, S7 | Select all            |
        |  @foo or @bar          | S1, S2, S4, S5, S6, S7         | @foo or @bar          |
        |  @foo or not @bar      | S0, S1, S3, S4, S5, S7         | @foo or not @bar      |
        |  not @foo or not @bar  | S0, S1, S2, S3, S5, S6         | not @foo or @not @bar |
        |  @foo  and @bar        | S4, S7                         | @foo and @bar         |
        |  @foo and     not @bar | S1, S5                         | @foo and not @bar     |
        |  not @foo and not @bar | S0, S3                         | not @foo and not @bar |"
/content/Record4.rst,"{
Scenario:Tag-Expressions v2
-------------------------------------------------------------------------------

Tag-Expressions v2 are based on :pypi:`cucumber-tag-expressions` with some extensions:

* Tag-Expressions v2 provide `boolean logic expression`
  (with ``and``, ``or`` and ``not`` operators and parenthesis for grouping expressions)
* Tag-Expressions v2 are far more readable and composable than Tag-Expressions v1
* Some boolean-logic-expressions where not possible with Tag-Expressions v1
* Therefore, Tag-Expressions v2 supersedes the old-style tag-expressions.


.. code-block:: gherkin
    :caption: TAG-EXPRESSION EXAMPLES

    # -- EXAMPLE 1: Select features/scenarios that have the tags: @a and @b
    @a and @b

    # -- EXAMPLE 2: Select features/scenarios that have the tag: @a or @b
    @a or @b

    # -- EXAMPLE 3: Select features/scenarios that do not have the tag: @a
    not @a

    # -- EXAMPLE 4: Select features/scenarios that have the tags: @a but not @b
    @a and not @b

    # -- EXAMPLE 5: Select features/scenarios that have the tags: (@a or @b) but not @c
    # HINT: Boolean expressions can be grouped with parenthesis.
    (@a or @b) and not @c

Given:COMMAND-LINE EXAMPLE:

.. code-block:: sh
    :caption: USING: Tag-Expressions v2 with ``behave``

    # -- SELECT-BY-TAG-EXPRESSION (with tag-expressions v2):
    # Select all features / scenarios with both ""@foo"" and ""@bar"" tags.
    $ behave --tags=""@foo and @bar"" features/

    # -- EXAMPLE: Use default_tags from config-file ""behave.ini"".
    # Use placeholder ""{config.tags}"" to refer to this tag-expression.
    # HERE: config.tags = ""not (@xfail or @not_implemented)""
    $ behave --tags=""(@foo or @bar) and {config.tags}"" --tags-help
    ...
    CURRENT TAG_EXPRESSION: ((foo or bar) and not (xfail or not_implemented))

    # -- EXAMPLE: Uses Tag-Expression diagnostics with --tags-help option
    $ behave --tags=""(@foo and @bar) or @baz"" --tags-help
    $ behave --tags=""(@foo and @bar) or @baz"" --tags-help --verbose

.. seealso::

    * https://docs.cucumber.io/cucumber/api/#tag-expressions
    * :pypi:`cucumber-tag-expressions` (Python package)


Then:Tag Matching with Tag-Expressions
-------------------------------------------------------------------------------

Tag-Expressions v2 support **partial string/tag matching** with wildcards.
This supports tag-expressions:

=================== =========== =========== ===================================================
Tag Matching Idiom  Example 1   Example 2   Description
=================== =========== =========== ===================================================
``tag.starts_with`` ``@foo.*``  ``foo.*``   Search for tags that start with a ``prefix``.
``tag.ends_with``   ``@*.one``  ``*.one``   Search for tags that end with a ``suffix``.
``tag.contains``    ``@*foo*``  ``*foo*``   Search for tags that contain a ``part``.
=================== =========== =========== ===================================================

.. code-block:: gherkin
    :caption: FILE: features/one.feature

    Feature: Alice

      @foo.one
      Scenario: Alice.1
        ...

      @foo.two
      Scenario: Alice.2
        ...

      @bar
      Scenario: Alice.3
        ...

The following command-line will select all features / scenarios with tags
that start with ""@foo."":

.. code-block:: sh
    :caption: USAGE EXAMPLE: Run behave with tag-matching expressions

    $ behave -f plain --tags=""@foo.*"" features/one.feature
    Feature: Alice

      Scenario: Alice.1
        ...

      Scenario: Alice.2
        ...

    # -- HINT: Only Alice.1 and Alice.2 are matched (not: Alice.3).

.. note::

    * Filename matching wildcards are supported.
      See :mod:`fnmatch` (Unix style filename matching).

    * The tag matching functionality is an extension to :pypi:`cucumber-tag-expressions`.


Select the Tag-Expression Version to Use
-------------------------------------------------------------------------------

The tag-expression version, that should be used by :pypi:`behave`,
can be specified in the :pypi:`behave` config-file.

This allows a user to select:

* Tag-Expressions v1 (if needed)
* Tag-Expressions v2 when it is feasible

EXAMPLE:

.. code-block:: ini
    :caption: FILE: behave.ini

    # SPECIFY WHICH TAG-EXPRESSION-PROTOCOL SHOULD BE USED:
    #   SUPPORTED VALUES: v1, v2, auto_detect
    #   CURRENT DEFAULT:  auto_detect
    [behave]
    tag_expression_protocol = v1    # -- Use Tag-Expressions v1.


Tag-Expressions v1
-------------------------------------------------------------------------------

Tag-Expressions v1 are becoming deprecated (but are currently still supported).
Use **Tag-Expressions v2** instead.

.. note::

    Tag-Expressions v1 support will be dropped in ``behave v1.4.0``.

}

",/content/Feature4.feature,"Scenario: Tag expression with three tags (@foo, @bar, @zap)
    Given the model elements with name and tags:
        | name | tags                   | Comment |
        | S0   |                        | Untagged    |
        | S1   | @foo                   | With 1 tag  |
        | S2   | @bar                   |             |
        | S3   | @zap                   |             |
        | S4   | @other                 |             |
        | S5   | @foo @bar              | With 2 tags |
        | S6   | @foo @zap              |             |
        | S7   | @foo @other            |             |
        | S8   | @bar @zap              |             |
        | S9   | @bar @other            |             |
        | S10  | @zap @other            |             |
        | S11  | @foo @bar @zap         | With 3 tags |
        | S12  | @foo @bar @other       |             |
        | S13  | @foo @zap @other       |             |
        | S14  | @bar @zap @other       |             |
        | S15  | @foo @bar @zap @other  | With 4 tags |
    And note that ""are all combinations of 0..4 tags""
    Then the tag expression selects model elements with:
        | tag expression                | selected?                   | Case |
        |  (@foo or @bar) and @zap      | S6, S8, S11, S13, S14, S15  | (@foo or @bar) and @zap |
        |  (@foo or @bar) and not @zap  | S1, S2, S5, S7, S9, S12     | (@foo or @bar) and not @zap |
        |  (@foo or not @bar) and @zap | S3, S6, S10, S11, S13, S15  | (@foo or not @bar) and @zap |"
/content/Record5.rst,"{
Scenario:.. _docid.fixtures:

Fixtures
==============================================================================

A common task during test execution is to:

* setup a functionality when a test-scope is entered
* cleanup (or teardown) the functionality at the end of the test-scope

**Fixtures** are provided as concept to simplify this setup/cleanup task
in `behave`_.

.. include:: _common_extlinks.rst

Providing a Fixture
-------------------

.. code-block:: python

    # -- FILE: behave4my_project/fixtures.py  (or in: features/environment.py)
    from behave import fixture
    from somewhere.browser.firefox import FirefoxBrowser

    # -- FIXTURE-VARIANT 1: Use generator-function
    @fixture
    def browser_firefox(context, timeout=30, **kwargs):
        # -- SETUP-FIXTURE PART:
        context.browser = FirefoxBrowser(timeout, **kwargs)
        yield context.browser
        # -- CLEANUP-FIXTURE PART:
        context.browser.shutdown()

.. code-block:: python

    # -- FIXTURE-VARIANT 2: Use normal function
    from somewhere.browser.chrome import ChromeBrowser

    @fixture
    def browser_chrome(context, timeout=30, **kwargs):
        # -- SETUP-FIXTURE PART: And register as context-cleanup task.
        browser = ChromeBrowser(timeout, **kwargs)
        context.browser = browser
        context.add_cleanup(browser.shutdown)
        return browser
        # -- CLEANUP-FIXTURE PART: browser.shutdown()
        # Fixture-cleanup is called when current context-layer is removed.

.. seealso::

    A *fixture* is similar to:

    * a :func:`contextlib.contextmanager`
    * a `pytest.fixture`_
    * the `scope guard`_ idiom

Given:Using a Fixture
---------------

In many cases, the usage of a fixture is triggered by the ``fixture-tag``
in a feature file. The ``fixture-tag`` marks that a fixture
should be used in this scenario/feature (as test-scope).

.. code-block:: gherkin

    # -- FILE: features/use_fixture1.feature
    Feature: Use Fixture on Scenario Level

        @fixture.browser.firefox
        Scenario: Use Web Browser Firefox
            Given I load web page ""https://somewhere.web""
            ...
        # -- AFTER-SCENARIO: Cleanup fixture.browser.firefox

.. code-block:: gherkin

    # -- FILE: features/use_fixture2.feature
    @fixture.browser.firefox
    Feature: Use Fixture on Feature Level

        Scenario: Use Web Browser Firefox
            Given I load web page ""https://somewhere.web""
            ...

        Scenario: Another Browser Test
            ...

    # -- AFTER-FEATURE: Cleanup fixture.browser.firefox


A **fixture** can be used by calling the :func:`~behave.use_fixture()` function.
The :func:`~behave.use_fixture()` call performs the ``SETUP-FIXTURE`` part and returns the
setup result. In addition, it ensures that ``CLEANUP-FIXTURE`` part is called
later-on when the current context-layer is removed.
Therefore, any manual cleanup handling in the ``after_tag()`` hook is not necessary.

.. code-block:: python

    # -- FILE: features/environment.py
    from behave import use_fixture
    from behave4my_project.fixtures import browser_firefox

    def before_tag(context, tag):
        if tag == ""fixture.browser.firefox"":
            use_fixture(browser_firefox, context, timeout=10)



Then:Realistic Example
~~~~~~~~~~~~~~~~~

A more realistic example by using a fixture registry is shown below:

.. code-block:: python

    # -- FILE: features/environment.py
    from behave.fixture import use_fixture_by_tag, fixture_call_params
    from behave4my_project.fixtures import browser_firefox, browser_chrome

    # -- REGISTRY DATA SCHEMA 1: fixture_func
    fixture_registry1 = {
        ""fixture.browser.firefox"": browser_firefox,
        ""fixture.browser.chrome"":  browser_chrome,
    }
    # -- REGISTRY DATA SCHEMA 2: (fixture_func, fixture_args, fixture_kwargs)
    fixture_registry2 = {
        ""fixture.browser.firefox"": fixture_call_params(browser_firefox),
        ""fixture.browser.chrome"":  fixture_call_params(browser_chrome, timeout=12),
    }

    def before_tag(context, tag):
        if tag.startswith(""fixture.""):
            return use_fixture_by_tag(tag, context, fixture_registry1):
        # -- MORE: Tag processing steps ...


.. code-block:: python

    # -- FILE: behave/fixture.py
    # ...
    def use_fixture_by_tag(tag, context, fixture_registry):
        fixture_data = fixture_registry.get(tag, None)
        if fixture_data is None:
            raise LookupError(""Unknown fixture-tag: %s"" % tag)

        # -- FOR DATA SCHEMA 1:
        fixture_func = fixture_data
        return use_fixture(fixture_func, context)

        # -- FOR DATA SCHEMA 2:
        fixture_func, fixture_args, fixture_kwargs = fixture_data
        return use_fixture(fixture_func, context, *fixture_args, **fixture_kwargs)



.. hint:: **Naming Convention for Fixture Tags**

    Fixture tags should start with ``""@fixture.*""`` prefix to improve readability
    and understandibilty in feature files (Gherkin).

    Tags are used for different purposes. Therefore, it should be clear
    when a ``fixture-tag`` is used.



Fixture Cleanup Points
------------------------------------------------------------------------------

The point when a fixture-cleanup is performed depends on the scope where
:func:`~behave.use_fixture()` is called (and the fixture-setup is performed).


============= =========================== ==========================================================================================
Context Layer Fixture-Setup Point         Fixture-Cleanup Point
============= =========================== ==========================================================================================
test run      In ``before_all()`` hook    After ``after_all()``       at end of test-run.
feature       In ``before_feature()``     After ``after_feature()``,  at end of feature.
feature       In ``before_tag()``         After ``after_feature()``   for feature tag.
scenario      In ``before_scenario()``    After ``after_scenario()``, at end of scenario.
scenario      In ``before_tag()``         After ``after_scenario()``  for scenario tag.
scenario      In a step                   After ``after_scenario()``. Fixture is usable until end of scenario.
============= =========================== ==========================================================================================


Fixture Setup/Cleanup Semantics
------------------------------------------------------------------------------

If an error occurs during fixture-setup (meaning an exception is raised):

* Feature/scenario execution is aborted
* Any remaining fixture-setups are skipped
* After feature/scenario hooks are processed
* All fixture-cleanups and context cleanups are performed
* The feature/scenario is marked as failed

If an error occurs during fixture-cleanup (meaning an exception is raised):

* All remaining fixture-cleanups and context cleanups are performed
* First cleanup-error is reraised to pass failure to user (test runner)
* The feature/scenario is marked as failed



Ensure Fixture Cleanups with Fixture Setup Errors
------------------------------------------------------------------------------

Fixture-setup errors are special because a cleanup of a fixture is in many
cases not necessary (or rather difficult because the fixture object
is only partly created, etc.). Therefore, if an error occurs during fixture-setup
(meaning: an exception is raised), the fixture-cleanup part is normally not called.

If you need to ensure that the fixture-cleanup is performed, you need to
provide a slightly different fixture implementation:

.. code-block:: python

    # -- FILE: behave4my_project/fixtures.py  (or: features/environment.py)
    from behave import fixture
    from somewhere.browser.firefox import FirefoxBrowser

    def setup_fixture_part2_with_error(arg):
        raise RuntimeError(""OOPS-FIXTURE-SETUP-ERROR-HERE)

    # -- FIXTURE-VARIANT 1: Use generator-function with try/finally.
    @fixture
    def browser_firefox(context, timeout=30, **kwargs):
        try:
            browser = FirefoxBrowser(timeout, **kwargs)
            browser.part2 = setup_fixture_part2_with_error(""OOPS"")
            context.browser = browser   # NOT_REACHED
            yield browser
            # -- NORMAL FIXTURE-CLEANUP PART: NOT_REACHED due to setup-error.
         finally:
            browser.shutdown()  # -- CLEANUP: When generator-function is left.

.. code-block:: python

    # -- FIXTURE-VARIANT 2: Use normal function and register cleanup-task early.
    from somewhere.browser.chrome import ChromeBrowser

    @fixture
    def browser_chrome(context, timeout=30, **kwargs):
        browser = ChromeBrowser(timeout, **kwargs)
        context.browser = browser
        context.add_cleanup(browser.shutdown)   # -- ENSURE-CLEANUP EARLY
        browser.part2 = setup_fixture_part2_with_error(""OOPS"")
        return browser  # NOT_REACHED
        # -- CLEANUP: browser.shutdown() when context-layer is removed.

.. note::

    An fixture-setup-error that occurs when the browser object is created,
    is not covered by these solutions and not so easy to solve.



Composite Fixtures
------------------------------------------------------------------------------

The last section already describes some problems when you use
complex or *composite fixtures*. It must be ensured that cleanup of already
created fixture parts is performed even when errors occur late in the creation
of a *composite fixture*. This is basically a `scope guard`_ problem.

Solution 1:
~~~~~~~~~~~

.. code-block:: python

    # -- FILE: behave4my_project/fixtures.py
    # SOLUTION 1: Use ""use_fixture()"" to ensure cleanup even in case of errors.
    from behave import fixture, use_fixture

    @fixture
    def foo(context, *args, **kwargs):
        pass    # -- FIXTURE IMPLEMENTATION: Not of interest here.

    @fixture
    def bar(context, *args, **kwargs):
        pass    # -- FIXTURE IMPLEMENTATION: Not of interest here.

    # -- SOLUTION: With use_fixture()
    # ENSURES: foo-fixture is cleaned up even when setup-error occurs later.
    @fixture
    def composite1(context, *args, **kwargs):
        the_fixture1 = use_fixture(foo, context)
        the_fixture2 = use_fixture(bar, context)
        return [the_fixture1, the_fixture2]


Solution 2:
~~~~~~~~~~~

.. code-block:: python

    # -- ALTERNATIVE SOLUTION: With use_composite_fixture_with()
    from behave import fixture
    from behave.fixture import use_composite_fixture_with, fixture_call_params

    @fixture
    def composite2(context, *args, **kwargs):
        the_composite = use_composite_fixture_with(context, [
            fixture_call_params(foo, name=""foo""),
            fixture_call_params(bar, name=""bar""),
        ])
        return the_composite

}





",/content/Feature5.feature,"Scenario: Feature Setup
    Given a new working directory
    And a file named ""features/steps/steps.py"" with:
      """"""
      from behave import step

      @step(u'the browser is ""{browser_name}""')
      def step_browser_is(context, browser_name):
          assert context.browser == browser_name

      @step(u'no browser info exists')
      def step_no_browser_info(context):
          assert not hasattr(context, ""browser"")

      @step(u'{word:w} step passes')
      def step_passes(context, word):
          pass
      """"""
    And a file named ""behave.ini"" with:
      """"""
      [behave]
      show_timings = false
      """"""
    And an empty file named ""features/environment.py"""
/content/Record6.rst,"{
Scenario:.. _docid.fixtures:

Fixtures
==============================================================================

A common task during test execution is to:

* setup a functionality when a test-scope is entered
* cleanup (or teardown) the functionality at the end of the test-scope

**Fixtures** are provided as concept to simplify this setup/cleanup task
in `behave`_.

.. include:: _common_extlinks.rst

Providing a Fixture
-------------------

.. code-block:: python

    # -- FILE: behave4my_project/fixtures.py  (or in: features/environment.py)
    from behave import fixture
    from somewhere.browser.firefox import FirefoxBrowser

    # -- FIXTURE-VARIANT 1: Use generator-function
    @fixture
    def browser_firefox(context, timeout=30, **kwargs):
        # -- SETUP-FIXTURE PART:
        context.browser = FirefoxBrowser(timeout, **kwargs)
        yield context.browser
        # -- CLEANUP-FIXTURE PART:
        context.browser.shutdown()

.. code-block:: python

    # -- FIXTURE-VARIANT 2: Use normal function
    from somewhere.browser.chrome import ChromeBrowser

    @fixture
    def browser_chrome(context, timeout=30, **kwargs):
        # -- SETUP-FIXTURE PART: And register as context-cleanup task.
        browser = ChromeBrowser(timeout, **kwargs)
        context.browser = browser
        context.add_cleanup(browser.shutdown)
        return browser
        # -- CLEANUP-FIXTURE PART: browser.shutdown()
        # Fixture-cleanup is called when current context-layer is removed.

.. seealso::

    A *fixture* is similar to:

    * a :func:`contextlib.contextmanager`
    * a `pytest.fixture`_
    * the `scope guard`_ idiom

Given:Using a Fixture
---------------

In many cases, the usage of a fixture is triggered by the ``fixture-tag``
in a feature file. The ``fixture-tag`` marks that a fixture
should be used in this scenario/feature (as test-scope).

.. code-block:: gherkin

    # -- FILE: features/use_fixture1.feature
    Feature: Use Fixture on Scenario Level

        @fixture.browser.firefox
        Scenario: Use Web Browser Firefox
            Given I load web page ""https://somewhere.web""
            ...
        # -- AFTER-SCENARIO: Cleanup fixture.browser.firefox

.. code-block:: gherkin

    # -- FILE: features/use_fixture2.feature
    @fixture.browser.firefox
    Feature: Use Fixture on Feature Level

        Scenario: Use Web Browser Firefox
            Given I load web page ""https://somewhere.web""
            ...

        Scenario: Another Browser Test
            ...

    # -- AFTER-FEATURE: Cleanup fixture.browser.firefox


A **fixture** can be used by calling the :func:`~behave.use_fixture()` function.
The :func:`~behave.use_fixture()` call performs the ``SETUP-FIXTURE`` part and returns the
setup result. In addition, it ensures that ``CLEANUP-FIXTURE`` part is called
later-on when the current context-layer is removed.
Therefore, any manual cleanup handling in the ``after_tag()`` hook is not necessary.

.. code-block:: python

    # -- FILE: features/environment.py
    from behave import use_fixture
    from behave4my_project.fixtures import browser_firefox

    def before_tag(context, tag):
        if tag == ""fixture.browser.firefox"":
            use_fixture(browser_firefox, context, timeout=10)



Then:Realistic Example
~~~~~~~~~~~~~~~~~

A more realistic example by using a fixture registry is shown below:

.. code-block:: python

    # -- FILE: features/environment.py
    from behave.fixture import use_fixture_by_tag, fixture_call_params
    from behave4my_project.fixtures import browser_firefox, browser_chrome

    # -- REGISTRY DATA SCHEMA 1: fixture_func
    fixture_registry1 = {
        ""fixture.browser.firefox"": browser_firefox,
        ""fixture.browser.chrome"":  browser_chrome,
    }
    # -- REGISTRY DATA SCHEMA 2: (fixture_func, fixture_args, fixture_kwargs)
    fixture_registry2 = {
        ""fixture.browser.firefox"": fixture_call_params(browser_firefox),
        ""fixture.browser.chrome"":  fixture_call_params(browser_chrome, timeout=12),
    }

    def before_tag(context, tag):
        if tag.startswith(""fixture.""):
            return use_fixture_by_tag(tag, context, fixture_registry1):
        # -- MORE: Tag processing steps ...


.. code-block:: python

    # -- FILE: behave/fixture.py
    # ...
    def use_fixture_by_tag(tag, context, fixture_registry):
        fixture_data = fixture_registry.get(tag, None)
        if fixture_data is None:
            raise LookupError(""Unknown fixture-tag: %s"" % tag)

        # -- FOR DATA SCHEMA 1:
        fixture_func = fixture_data
        return use_fixture(fixture_func, context)

        # -- FOR DATA SCHEMA 2:
        fixture_func, fixture_args, fixture_kwargs = fixture_data
        return use_fixture(fixture_func, context, *fixture_args, **fixture_kwargs)



.. hint:: **Naming Convention for Fixture Tags**

    Fixture tags should start with ``""@fixture.*""`` prefix to improve readability
    and understandibilty in feature files (Gherkin).

    Tags are used for different purposes. Therefore, it should be clear
    when a ``fixture-tag`` is used.



Fixture Cleanup Points
------------------------------------------------------------------------------

The point when a fixture-cleanup is performed depends on the scope where
:func:`~behave.use_fixture()` is called (and the fixture-setup is performed).


============= =========================== ==========================================================================================
Context Layer Fixture-Setup Point         Fixture-Cleanup Point
============= =========================== ==========================================================================================
test run      In ``before_all()`` hook    After ``after_all()``       at end of test-run.
feature       In ``before_feature()``     After ``after_feature()``,  at end of feature.
feature       In ``before_tag()``         After ``after_feature()``   for feature tag.
scenario      In ``before_scenario()``    After ``after_scenario()``, at end of scenario.
scenario      In ``before_tag()``         After ``after_scenario()``  for scenario tag.
scenario      In a step                   After ``after_scenario()``. Fixture is usable until end of scenario.
============= =========================== ==========================================================================================


Fixture Setup/Cleanup Semantics
------------------------------------------------------------------------------

If an error occurs during fixture-setup (meaning an exception is raised):

* Feature/scenario execution is aborted
* Any remaining fixture-setups are skipped
* After feature/scenario hooks are processed
* All fixture-cleanups and context cleanups are performed
* The feature/scenario is marked as failed

If an error occurs during fixture-cleanup (meaning an exception is raised):

* All remaining fixture-cleanups and context cleanups are performed
* First cleanup-error is reraised to pass failure to user (test runner)
* The feature/scenario is marked as failed



Ensure Fixture Cleanups with Fixture Setup Errors
------------------------------------------------------------------------------

Fixture-setup errors are special because a cleanup of a fixture is in many
cases not necessary (or rather difficult because the fixture object
is only partly created, etc.). Therefore, if an error occurs during fixture-setup
(meaning: an exception is raised), the fixture-cleanup part is normally not called.

If you need to ensure that the fixture-cleanup is performed, you need to
provide a slightly different fixture implementation:

.. code-block:: python

    # -- FILE: behave4my_project/fixtures.py  (or: features/environment.py)
    from behave import fixture
    from somewhere.browser.firefox import FirefoxBrowser

    def setup_fixture_part2_with_error(arg):
        raise RuntimeError(""OOPS-FIXTURE-SETUP-ERROR-HERE)

    # -- FIXTURE-VARIANT 1: Use generator-function with try/finally.
    @fixture
    def browser_firefox(context, timeout=30, **kwargs):
        try:
            browser = FirefoxBrowser(timeout, **kwargs)
            browser.part2 = setup_fixture_part2_with_error(""OOPS"")
            context.browser = browser   # NOT_REACHED
            yield browser
            # -- NORMAL FIXTURE-CLEANUP PART: NOT_REACHED due to setup-error.
         finally:
            browser.shutdown()  # -- CLEANUP: When generator-function is left.

.. code-block:: python

    # -- FIXTURE-VARIANT 2: Use normal function and register cleanup-task early.
    from somewhere.browser.chrome import ChromeBrowser

    @fixture
    def browser_chrome(context, timeout=30, **kwargs):
        browser = ChromeBrowser(timeout, **kwargs)
        context.browser = browser
        context.add_cleanup(browser.shutdown)   # -- ENSURE-CLEANUP EARLY
        browser.part2 = setup_fixture_part2_with_error(""OOPS"")
        return browser  # NOT_REACHED
        # -- CLEANUP: browser.shutdown() when context-layer is removed.

.. note::

    An fixture-setup-error that occurs when the browser object is created,
    is not covered by these solutions and not so easy to solve.



Composite Fixtures
------------------------------------------------------------------------------

The last section already describes some problems when you use
complex or *composite fixtures*. It must be ensured that cleanup of already
created fixture parts is performed even when errors occur late in the creation
of a *composite fixture*. This is basically a `scope guard`_ problem.

Solution 1:
~~~~~~~~~~~

.. code-block:: python

    # -- FILE: behave4my_project/fixtures.py
    # SOLUTION 1: Use ""use_fixture()"" to ensure cleanup even in case of errors.
    from behave import fixture, use_fixture

    @fixture
    def foo(context, *args, **kwargs):
        pass    # -- FIXTURE IMPLEMENTATION: Not of interest here.

    @fixture
    def bar(context, *args, **kwargs):
        pass    # -- FIXTURE IMPLEMENTATION: Not of interest here.

    # -- SOLUTION: With use_fixture()
    # ENSURES: foo-fixture is cleaned up even when setup-error occurs later.
    @fixture
    def composite1(context, *args, **kwargs):
        the_fixture1 = use_fixture(foo, context)
        the_fixture2 = use_fixture(bar, context)
        return [the_fixture1, the_fixture2]


Solution 2:
~~~~~~~~~~~

.. code-block:: python

    # -- ALTERNATIVE SOLUTION: With use_composite_fixture_with()
    from behave import fixture
    from behave.fixture import use_composite_fixture_with, fixture_call_params

    @fixture
    def composite2(context, *args, **kwargs):
        the_composite = use_composite_fixture_with(context, [
            fixture_call_params(foo, name=""foo""),
            fixture_call_params(bar, name=""bar""),
        ])
        return the_composite

}





",/content/Feature6.feature,"Scenario: Use fixture with generator-function (setup/cleanup)
    Given a file named ""features/environment.py"" with:
      """"""
      from __future__ import print_function
      from behave.fixture import fixture, use_fixture

      @fixture(name=""browser.firefox"")
      def browser_firefox(context):
          print(""FIXTURE-SETUP: browser.firefox"")
          context.browser = ""firefox""
          yield
          print(""FIXTURE-CLEANUP: browser.firefox"")
          # -- SCOPE-CLEANUP OR EXPLICIT: del context.browser

      def before_tag(context, tag):
          if tag == ""fixture.browser.firefox"":
              use_fixture(browser_firefox, context)
      """"""
    And a file named ""features/alice.feature"" with:
      """"""
      Feature: Fixture setup/teardown
        @fixture.browser.firefox
        Scenario: Fixture with browser=firefox
          Given the browser is ""firefox""

        Scenario: Fixture Cleanup check
          Then no browser info exists
      """"""
    When I run ""behave -f plain features/alice.feature""
    Then it should pass with:
      """"""
      2 scenarios passed, 0 failed, 0 skipped
      2 steps passed, 0 failed, 0 skipped, 0 undefined
      """"""
    And the command output should contain:
      """"""
      FIXTURE-SETUP: browser.firefox
        Scenario: Fixture with browser=firefox
          Given the browser is ""firefox"" ... passed
      FIXTURE-CLEANUP: browser.firefox
      """""""
/content/Record7.rst,"{
Scenario:.. _docid.fixtures:

Fixtures
==============================================================================

A common task during test execution is to:

* setup a functionality when a test-scope is entered
* cleanup (or teardown) the functionality at the end of the test-scope

**Fixtures** are provided as concept to simplify this setup/cleanup task
in `behave`_.

.. include:: _common_extlinks.rst

Providing a Fixture
-------------------

.. code-block:: python

    # -- FILE: behave4my_project/fixtures.py  (or in: features/environment.py)
    from behave import fixture
    from somewhere.browser.firefox import FirefoxBrowser

    # -- FIXTURE-VARIANT 1: Use generator-function
    @fixture
    def browser_firefox(context, timeout=30, **kwargs):
        # -- SETUP-FIXTURE PART:
        context.browser = FirefoxBrowser(timeout, **kwargs)
        yield context.browser
        # -- CLEANUP-FIXTURE PART:
        context.browser.shutdown()

.. code-block:: python

    # -- FIXTURE-VARIANT 2: Use normal function
    from somewhere.browser.chrome import ChromeBrowser

    @fixture
    def browser_chrome(context, timeout=30, **kwargs):
        # -- SETUP-FIXTURE PART: And register as context-cleanup task.
        browser = ChromeBrowser(timeout, **kwargs)
        context.browser = browser
        context.add_cleanup(browser.shutdown)
        return browser
        # -- CLEANUP-FIXTURE PART: browser.shutdown()
        # Fixture-cleanup is called when current context-layer is removed.

.. seealso::

    A *fixture* is similar to:

    * a :func:`contextlib.contextmanager`
    * a `pytest.fixture`_
    * the `scope guard`_ idiom

Given:Using a Fixture
---------------

In many cases, the usage of a fixture is triggered by the ``fixture-tag``
in a feature file. The ``fixture-tag`` marks that a fixture
should be used in this scenario/feature (as test-scope).

.. code-block:: gherkin

    # -- FILE: features/use_fixture1.feature
    Feature: Use Fixture on Scenario Level

        @fixture.browser.firefox
        Scenario: Use Web Browser Firefox
            Given I load web page ""https://somewhere.web""
            ...
        # -- AFTER-SCENARIO: Cleanup fixture.browser.firefox

.. code-block:: gherkin

    # -- FILE: features/use_fixture2.feature
    @fixture.browser.firefox
    Feature: Use Fixture on Feature Level

        Scenario: Use Web Browser Firefox
            Given I load web page ""https://somewhere.web""
            ...

        Scenario: Another Browser Test
            ...

    # -- AFTER-FEATURE: Cleanup fixture.browser.firefox


A **fixture** can be used by calling the :func:`~behave.use_fixture()` function.
The :func:`~behave.use_fixture()` call performs the ``SETUP-FIXTURE`` part and returns the
setup result. In addition, it ensures that ``CLEANUP-FIXTURE`` part is called
later-on when the current context-layer is removed.
Therefore, any manual cleanup handling in the ``after_tag()`` hook is not necessary.

.. code-block:: python

    # -- FILE: features/environment.py
    from behave import use_fixture
    from behave4my_project.fixtures import browser_firefox

    def before_tag(context, tag):
        if tag == ""fixture.browser.firefox"":
            use_fixture(browser_firefox, context, timeout=10)



Then:Realistic Example
~~~~~~~~~~~~~~~~~

A more realistic example by using a fixture registry is shown below:

.. code-block:: python

    # -- FILE: features/environment.py
    from behave.fixture import use_fixture_by_tag, fixture_call_params
    from behave4my_project.fixtures import browser_firefox, browser_chrome

    # -- REGISTRY DATA SCHEMA 1: fixture_func
    fixture_registry1 = {
        ""fixture.browser.firefox"": browser_firefox,
        ""fixture.browser.chrome"":  browser_chrome,
    }
    # -- REGISTRY DATA SCHEMA 2: (fixture_func, fixture_args, fixture_kwargs)
    fixture_registry2 = {
        ""fixture.browser.firefox"": fixture_call_params(browser_firefox),
        ""fixture.browser.chrome"":  fixture_call_params(browser_chrome, timeout=12),
    }

    def before_tag(context, tag):
        if tag.startswith(""fixture.""):
            return use_fixture_by_tag(tag, context, fixture_registry1):
        # -- MORE: Tag processing steps ...


.. code-block:: python

    # -- FILE: behave/fixture.py
    # ...
    def use_fixture_by_tag(tag, context, fixture_registry):
        fixture_data = fixture_registry.get(tag, None)
        if fixture_data is None:
            raise LookupError(""Unknown fixture-tag: %s"" % tag)

        # -- FOR DATA SCHEMA 1:
        fixture_func = fixture_data
        return use_fixture(fixture_func, context)

        # -- FOR DATA SCHEMA 2:
        fixture_func, fixture_args, fixture_kwargs = fixture_data
        return use_fixture(fixture_func, context, *fixture_args, **fixture_kwargs)



.. hint:: **Naming Convention for Fixture Tags**

    Fixture tags should start with ``""@fixture.*""`` prefix to improve readability
    and understandibilty in feature files (Gherkin).

    Tags are used for different purposes. Therefore, it should be clear
    when a ``fixture-tag`` is used.



Fixture Cleanup Points
------------------------------------------------------------------------------

The point when a fixture-cleanup is performed depends on the scope where
:func:`~behave.use_fixture()` is called (and the fixture-setup is performed).


============= =========================== ==========================================================================================
Context Layer Fixture-Setup Point         Fixture-Cleanup Point
============= =========================== ==========================================================================================
test run      In ``before_all()`` hook    After ``after_all()``       at end of test-run.
feature       In ``before_feature()``     After ``after_feature()``,  at end of feature.
feature       In ``before_tag()``         After ``after_feature()``   for feature tag.
scenario      In ``before_scenario()``    After ``after_scenario()``, at end of scenario.
scenario      In ``before_tag()``         After ``after_scenario()``  for scenario tag.
scenario      In a step                   After ``after_scenario()``. Fixture is usable until end of scenario.
============= =========================== ==========================================================================================


Fixture Setup/Cleanup Semantics
------------------------------------------------------------------------------

If an error occurs during fixture-setup (meaning an exception is raised):

* Feature/scenario execution is aborted
* Any remaining fixture-setups are skipped
* After feature/scenario hooks are processed
* All fixture-cleanups and context cleanups are performed
* The feature/scenario is marked as failed

If an error occurs during fixture-cleanup (meaning an exception is raised):

* All remaining fixture-cleanups and context cleanups are performed
* First cleanup-error is reraised to pass failure to user (test runner)
* The feature/scenario is marked as failed



Ensure Fixture Cleanups with Fixture Setup Errors
------------------------------------------------------------------------------

Fixture-setup errors are special because a cleanup of a fixture is in many
cases not necessary (or rather difficult because the fixture object
is only partly created, etc.). Therefore, if an error occurs during fixture-setup
(meaning: an exception is raised), the fixture-cleanup part is normally not called.

If you need to ensure that the fixture-cleanup is performed, you need to
provide a slightly different fixture implementation:

.. code-block:: python

    # -- FILE: behave4my_project/fixtures.py  (or: features/environment.py)
    from behave import fixture
    from somewhere.browser.firefox import FirefoxBrowser

    def setup_fixture_part2_with_error(arg):
        raise RuntimeError(""OOPS-FIXTURE-SETUP-ERROR-HERE)

    # -- FIXTURE-VARIANT 1: Use generator-function with try/finally.
    @fixture
    def browser_firefox(context, timeout=30, **kwargs):
        try:
            browser = FirefoxBrowser(timeout, **kwargs)
            browser.part2 = setup_fixture_part2_with_error(""OOPS"")
            context.browser = browser   # NOT_REACHED
            yield browser
            # -- NORMAL FIXTURE-CLEANUP PART: NOT_REACHED due to setup-error.
         finally:
            browser.shutdown()  # -- CLEANUP: When generator-function is left.

.. code-block:: python

    # -- FIXTURE-VARIANT 2: Use normal function and register cleanup-task early.
    from somewhere.browser.chrome import ChromeBrowser

    @fixture
    def browser_chrome(context, timeout=30, **kwargs):
        browser = ChromeBrowser(timeout, **kwargs)
        context.browser = browser
        context.add_cleanup(browser.shutdown)   # -- ENSURE-CLEANUP EARLY
        browser.part2 = setup_fixture_part2_with_error(""OOPS"")
        return browser  # NOT_REACHED
        # -- CLEANUP: browser.shutdown() when context-layer is removed.

.. note::

    An fixture-setup-error that occurs when the browser object is created,
    is not covered by these solutions and not so easy to solve.



Composite Fixtures
------------------------------------------------------------------------------

The last section already describes some problems when you use
complex or *composite fixtures*. It must be ensured that cleanup of already
created fixture parts is performed even when errors occur late in the creation
of a *composite fixture*. This is basically a `scope guard`_ problem.

Solution 1:
~~~~~~~~~~~

.. code-block:: python

    # -- FILE: behave4my_project/fixtures.py
    # SOLUTION 1: Use ""use_fixture()"" to ensure cleanup even in case of errors.
    from behave import fixture, use_fixture

    @fixture
    def foo(context, *args, **kwargs):
        pass    # -- FIXTURE IMPLEMENTATION: Not of interest here.

    @fixture
    def bar(context, *args, **kwargs):
        pass    # -- FIXTURE IMPLEMENTATION: Not of interest here.

    # -- SOLUTION: With use_fixture()
    # ENSURES: foo-fixture is cleaned up even when setup-error occurs later.
    @fixture
    def composite1(context, *args, **kwargs):
        the_fixture1 = use_fixture(foo, context)
        the_fixture2 = use_fixture(bar, context)
        return [the_fixture1, the_fixture2]


Solution 2:
~~~~~~~~~~~

.. code-block:: python

    # -- ALTERNATIVE SOLUTION: With use_composite_fixture_with()
    from behave import fixture
    from behave.fixture import use_composite_fixture_with, fixture_call_params

    @fixture
    def composite2(context, *args, **kwargs):
        the_composite = use_composite_fixture_with(context, [
            fixture_call_params(foo, name=""foo""),
            fixture_call_params(bar, name=""bar""),
        ])
        return the_composite

}





",/content/Feature7.feature," Scenario: Use fixture with function (setup-only)
    Given a file named ""features/environment.py"" with:
      """"""
      from __future__ import print_function
      from behave.fixture import fixture, use_fixture

      @fixture(name=""browser.chrome"")
      def browser_chrome(context):
          # -- CASE: Setup-only
          print(""FIXTURE-SETUP: browser.chrome"")
          context.browser = ""chrome""

      def before_tag(context, tag):
          if tag == ""fixture.browser.chrome"":
              use_fixture(browser_chrome, context)
      """"""
    And a file named ""features/bob.feature"" with:
      """"""
      Feature: Fixture setup only
        @fixture.browser.chrome
        Scenario: Fixture with browser=chrome
          Given the browser is ""chrome""

        Scenario: Fixture Cleanup check
          Then no browser info exists
      """"""
    When I run ""behave -f plain features/bob.feature""
    Then it should pass with:
      """"""
      2 scenarios passed, 0 failed, 0 skipped
      2 steps passed, 0 failed, 0 skipped, 0 undefined
      """"""
    And the command output should contain:
      """"""
      FIXTURE-SETUP: browser.chrome
        Scenario: Fixture with browser=chrome
          Given the browser is ""chrome"" ... passed

        Scenario: Fixture Cleanup check
          Then no browser info exists ... passed
      """""""
/content/Record8.rst,"{
Scenario:.. _docid.fixtures:

Fixtures
==============================================================================

A common task during test execution is to:

* setup a functionality when a test-scope is entered
* cleanup (or teardown) the functionality at the end of the test-scope

**Fixtures** are provided as concept to simplify this setup/cleanup task
in `behave`_.

.. include:: _common_extlinks.rst

Providing a Fixture
-------------------

.. code-block:: python

    # -- FILE: behave4my_project/fixtures.py  (or in: features/environment.py)
    from behave import fixture
    from somewhere.browser.firefox import FirefoxBrowser

    # -- FIXTURE-VARIANT 1: Use generator-function
    @fixture
    def browser_firefox(context, timeout=30, **kwargs):
        # -- SETUP-FIXTURE PART:
        context.browser = FirefoxBrowser(timeout, **kwargs)
        yield context.browser
        # -- CLEANUP-FIXTURE PART:
        context.browser.shutdown()

.. code-block:: python

    # -- FIXTURE-VARIANT 2: Use normal function
    from somewhere.browser.chrome import ChromeBrowser

    @fixture
    def browser_chrome(context, timeout=30, **kwargs):
        # -- SETUP-FIXTURE PART: And register as context-cleanup task.
        browser = ChromeBrowser(timeout, **kwargs)
        context.browser = browser
        context.add_cleanup(browser.shutdown)
        return browser
        # -- CLEANUP-FIXTURE PART: browser.shutdown()
        # Fixture-cleanup is called when current context-layer is removed.

.. seealso::

    A *fixture* is similar to:

    * a :func:`contextlib.contextmanager`
    * a `pytest.fixture`_
    * the `scope guard`_ idiom

Given:Using a Fixture
---------------

In many cases, the usage of a fixture is triggered by the ``fixture-tag``
in a feature file. The ``fixture-tag`` marks that a fixture
should be used in this scenario/feature (as test-scope).

.. code-block:: gherkin

    # -- FILE: features/use_fixture1.feature
    Feature: Use Fixture on Scenario Level

        @fixture.browser.firefox
        Scenario: Use Web Browser Firefox
            Given I load web page ""https://somewhere.web""
            ...
        # -- AFTER-SCENARIO: Cleanup fixture.browser.firefox

.. code-block:: gherkin

    # -- FILE: features/use_fixture2.feature
    @fixture.browser.firefox
    Feature: Use Fixture on Feature Level

        Scenario: Use Web Browser Firefox
            Given I load web page ""https://somewhere.web""
            ...

        Scenario: Another Browser Test
            ...

    # -- AFTER-FEATURE: Cleanup fixture.browser.firefox


A **fixture** can be used by calling the :func:`~behave.use_fixture()` function.
The :func:`~behave.use_fixture()` call performs the ``SETUP-FIXTURE`` part and returns the
setup result. In addition, it ensures that ``CLEANUP-FIXTURE`` part is called
later-on when the current context-layer is removed.
Therefore, any manual cleanup handling in the ``after_tag()`` hook is not necessary.

.. code-block:: python

    # -- FILE: features/environment.py
    from behave import use_fixture
    from behave4my_project.fixtures import browser_firefox

    def before_tag(context, tag):
        if tag == ""fixture.browser.firefox"":
            use_fixture(browser_firefox, context, timeout=10)



Then:Realistic Example
~~~~~~~~~~~~~~~~~

A more realistic example by using a fixture registry is shown below:

.. code-block:: python

    # -- FILE: features/environment.py
    from behave.fixture import use_fixture_by_tag, fixture_call_params
    from behave4my_project.fixtures import browser_firefox, browser_chrome

    # -- REGISTRY DATA SCHEMA 1: fixture_func
    fixture_registry1 = {
        ""fixture.browser.firefox"": browser_firefox,
        ""fixture.browser.chrome"":  browser_chrome,
    }
    # -- REGISTRY DATA SCHEMA 2: (fixture_func, fixture_args, fixture_kwargs)
    fixture_registry2 = {
        ""fixture.browser.firefox"": fixture_call_params(browser_firefox),
        ""fixture.browser.chrome"":  fixture_call_params(browser_chrome, timeout=12),
    }

    def before_tag(context, tag):
        if tag.startswith(""fixture.""):
            return use_fixture_by_tag(tag, context, fixture_registry1):
        # -- MORE: Tag processing steps ...


.. code-block:: python

    # -- FILE: behave/fixture.py
    # ...
    def use_fixture_by_tag(tag, context, fixture_registry):
        fixture_data = fixture_registry.get(tag, None)
        if fixture_data is None:
            raise LookupError(""Unknown fixture-tag: %s"" % tag)

        # -- FOR DATA SCHEMA 1:
        fixture_func = fixture_data
        return use_fixture(fixture_func, context)

        # -- FOR DATA SCHEMA 2:
        fixture_func, fixture_args, fixture_kwargs = fixture_data
        return use_fixture(fixture_func, context, *fixture_args, **fixture_kwargs)



.. hint:: **Naming Convention for Fixture Tags**

    Fixture tags should start with ``""@fixture.*""`` prefix to improve readability
    and understandibilty in feature files (Gherkin).

    Tags are used for different purposes. Therefore, it should be clear
    when a ``fixture-tag`` is used.



Fixture Cleanup Points
------------------------------------------------------------------------------

The point when a fixture-cleanup is performed depends on the scope where
:func:`~behave.use_fixture()` is called (and the fixture-setup is performed).


============= =========================== ==========================================================================================
Context Layer Fixture-Setup Point         Fixture-Cleanup Point
============= =========================== ==========================================================================================
test run      In ``before_all()`` hook    After ``after_all()``       at end of test-run.
feature       In ``before_feature()``     After ``after_feature()``,  at end of feature.
feature       In ``before_tag()``         After ``after_feature()``   for feature tag.
scenario      In ``before_scenario()``    After ``after_scenario()``, at end of scenario.
scenario      In ``before_tag()``         After ``after_scenario()``  for scenario tag.
scenario      In a step                   After ``after_scenario()``. Fixture is usable until end of scenario.
============= =========================== ==========================================================================================


Fixture Setup/Cleanup Semantics
------------------------------------------------------------------------------

If an error occurs during fixture-setup (meaning an exception is raised):

* Feature/scenario execution is aborted
* Any remaining fixture-setups are skipped
* After feature/scenario hooks are processed
* All fixture-cleanups and context cleanups are performed
* The feature/scenario is marked as failed

If an error occurs during fixture-cleanup (meaning an exception is raised):

* All remaining fixture-cleanups and context cleanups are performed
* First cleanup-error is reraised to pass failure to user (test runner)
* The feature/scenario is marked as failed



Ensure Fixture Cleanups with Fixture Setup Errors
------------------------------------------------------------------------------

Fixture-setup errors are special because a cleanup of a fixture is in many
cases not necessary (or rather difficult because the fixture object
is only partly created, etc.). Therefore, if an error occurs during fixture-setup
(meaning: an exception is raised), the fixture-cleanup part is normally not called.

If you need to ensure that the fixture-cleanup is performed, you need to
provide a slightly different fixture implementation:

.. code-block:: python

    # -- FILE: behave4my_project/fixtures.py  (or: features/environment.py)
    from behave import fixture
    from somewhere.browser.firefox import FirefoxBrowser

    def setup_fixture_part2_with_error(arg):
        raise RuntimeError(""OOPS-FIXTURE-SETUP-ERROR-HERE)

    # -- FIXTURE-VARIANT 1: Use generator-function with try/finally.
    @fixture
    def browser_firefox(context, timeout=30, **kwargs):
        try:
            browser = FirefoxBrowser(timeout, **kwargs)
            browser.part2 = setup_fixture_part2_with_error(""OOPS"")
            context.browser = browser   # NOT_REACHED
            yield browser
            # -- NORMAL FIXTURE-CLEANUP PART: NOT_REACHED due to setup-error.
         finally:
            browser.shutdown()  # -- CLEANUP: When generator-function is left.

.. code-block:: python

    # -- FIXTURE-VARIANT 2: Use normal function and register cleanup-task early.
    from somewhere.browser.chrome import ChromeBrowser

    @fixture
    def browser_chrome(context, timeout=30, **kwargs):
        browser = ChromeBrowser(timeout, **kwargs)
        context.browser = browser
        context.add_cleanup(browser.shutdown)   # -- ENSURE-CLEANUP EARLY
        browser.part2 = setup_fixture_part2_with_error(""OOPS"")
        return browser  # NOT_REACHED
        # -- CLEANUP: browser.shutdown() when context-layer is removed.

.. note::

    An fixture-setup-error that occurs when the browser object is created,
    is not covered by these solutions and not so easy to solve.



Composite Fixtures
------------------------------------------------------------------------------

The last section already describes some problems when you use
complex or *composite fixtures*. It must be ensured that cleanup of already
created fixture parts is performed even when errors occur late in the creation
of a *composite fixture*. This is basically a `scope guard`_ problem.

Solution 1:
~~~~~~~~~~~

.. code-block:: python

    # -- FILE: behave4my_project/fixtures.py
    # SOLUTION 1: Use ""use_fixture()"" to ensure cleanup even in case of errors.
    from behave import fixture, use_fixture

    @fixture
    def foo(context, *args, **kwargs):
        pass    # -- FIXTURE IMPLEMENTATION: Not of interest here.

    @fixture
    def bar(context, *args, **kwargs):
        pass    # -- FIXTURE IMPLEMENTATION: Not of interest here.

    # -- SOLUTION: With use_fixture()
    # ENSURES: foo-fixture is cleaned up even when setup-error occurs later.
    @fixture
    def composite1(context, *args, **kwargs):
        the_fixture1 = use_fixture(foo, context)
        the_fixture2 = use_fixture(bar, context)
        return [the_fixture1, the_fixture2]


Solution 2:
~~~~~~~~~~~

.. code-block:: python

    # -- ALTERNATIVE SOLUTION: With use_composite_fixture_with()
    from behave import fixture
    from behave.fixture import use_composite_fixture_with, fixture_call_params

    @fixture
    def composite2(context, *args, **kwargs):
        the_composite = use_composite_fixture_with(context, [
            fixture_call_params(foo, name=""foo""),
            fixture_call_params(bar, name=""bar""),
        ])
        return the_composite

}





",/content/Feature8.feature," Scenario: Use fixture (case: feature)
    Given a file named ""features/environment.py"" with:
      """"""
      from __future__ import print_function
      from behave.fixture import fixture, use_fixture

      @fixture
      def foo(context):
          print(""FIXTURE-SETUP: foo"")
          yield
          print(""FIXTURE-CLEANUP: foo"")

      def before_tag(context, tag):
          if tag == ""fixture.foo"":
              use_fixture(foo, context)

      def after_feature(context, feature):
          print(""HOOK-CALLED: after_feature: %s"" % feature.name)
      """"""
    And a file named ""features/use2.feature"" with:
      """"""
      @fixture.foo
      Feature: Use Fixture for Feature

        Scenario:
          Given a step passes

        Scenario:
          Then another step passes
      """"""
    When I run ""behave -f plain features/use2.feature""
    Then it should pass with:
      """"""
      1 feature passed, 0 failed, 0 skipped
      2 scenarios passed, 0 failed, 0 skipped
      2 steps passed, 0 failed, 0 skipped, 0 undefined
      """"""
    And the command output should contain:
      """"""
      FIXTURE-SETUP: foo
      Feature: Use Fixture for Feature
        Scenario:
          Given a step passes ... passed

        Scenario:
          Then another step passes ... passed

      HOOK-CALLED: after_feature: Use Fixture for Feature
      FIXTURE-CLEANUP: foo
      """"""
    But note that ""the fixture-cleanup after the feature"""
/content/Record9.rst,"{
Scenario:.. _docid.fixtures:

Fixtures
==============================================================================

A common task during test execution is to:

* setup a functionality when a test-scope is entered
* cleanup (or teardown) the functionality at the end of the test-scope

**Fixtures** are provided as concept to simplify this setup/cleanup task
in `behave`_.

.. include:: _common_extlinks.rst

Providing a Fixture
-------------------

.. code-block:: python

    # -- FILE: behave4my_project/fixtures.py  (or in: features/environment.py)
    from behave import fixture
    from somewhere.browser.firefox import FirefoxBrowser

    # -- FIXTURE-VARIANT 1: Use generator-function
    @fixture
    def browser_firefox(context, timeout=30, **kwargs):
        # -- SETUP-FIXTURE PART:
        context.browser = FirefoxBrowser(timeout, **kwargs)
        yield context.browser
        # -- CLEANUP-FIXTURE PART:
        context.browser.shutdown()

.. code-block:: python

    # -- FIXTURE-VARIANT 2: Use normal function
    from somewhere.browser.chrome import ChromeBrowser

    @fixture
    def browser_chrome(context, timeout=30, **kwargs):
        # -- SETUP-FIXTURE PART: And register as context-cleanup task.
        browser = ChromeBrowser(timeout, **kwargs)
        context.browser = browser
        context.add_cleanup(browser.shutdown)
        return browser
        # -- CLEANUP-FIXTURE PART: browser.shutdown()
        # Fixture-cleanup is called when current context-layer is removed.

.. seealso::

    A *fixture* is similar to:

    * a :func:`contextlib.contextmanager`
    * a `pytest.fixture`_
    * the `scope guard`_ idiom

Given:Using a Fixture
---------------

In many cases, the usage of a fixture is triggered by the ``fixture-tag``
in a feature file. The ``fixture-tag`` marks that a fixture
should be used in this scenario/feature (as test-scope).

.. code-block:: gherkin

    # -- FILE: features/use_fixture1.feature
    Feature: Use Fixture on Scenario Level

        @fixture.browser.firefox
        Scenario: Use Web Browser Firefox
            Given I load web page ""https://somewhere.web""
            ...
        # -- AFTER-SCENARIO: Cleanup fixture.browser.firefox

.. code-block:: gherkin

    # -- FILE: features/use_fixture2.feature
    @fixture.browser.firefox
    Feature: Use Fixture on Feature Level

        Scenario: Use Web Browser Firefox
            Given I load web page ""https://somewhere.web""
            ...

        Scenario: Another Browser Test
            ...

    # -- AFTER-FEATURE: Cleanup fixture.browser.firefox


A **fixture** can be used by calling the :func:`~behave.use_fixture()` function.
The :func:`~behave.use_fixture()` call performs the ``SETUP-FIXTURE`` part and returns the
setup result. In addition, it ensures that ``CLEANUP-FIXTURE`` part is called
later-on when the current context-layer is removed.
Therefore, any manual cleanup handling in the ``after_tag()`` hook is not necessary.

.. code-block:: python

    # -- FILE: features/environment.py
    from behave import use_fixture
    from behave4my_project.fixtures import browser_firefox

    def before_tag(context, tag):
        if tag == ""fixture.browser.firefox"":
            use_fixture(browser_firefox, context, timeout=10)



Then:Realistic Example
~~~~~~~~~~~~~~~~~

A more realistic example by using a fixture registry is shown below:

.. code-block:: python

    # -- FILE: features/environment.py
    from behave.fixture import use_fixture_by_tag, fixture_call_params
    from behave4my_project.fixtures import browser_firefox, browser_chrome

    # -- REGISTRY DATA SCHEMA 1: fixture_func
    fixture_registry1 = {
        ""fixture.browser.firefox"": browser_firefox,
        ""fixture.browser.chrome"":  browser_chrome,
    }
    # -- REGISTRY DATA SCHEMA 2: (fixture_func, fixture_args, fixture_kwargs)
    fixture_registry2 = {
        ""fixture.browser.firefox"": fixture_call_params(browser_firefox),
        ""fixture.browser.chrome"":  fixture_call_params(browser_chrome, timeout=12),
    }

    def before_tag(context, tag):
        if tag.startswith(""fixture.""):
            return use_fixture_by_tag(tag, context, fixture_registry1):
        # -- MORE: Tag processing steps ...


.. code-block:: python

    # -- FILE: behave/fixture.py
    # ...
    def use_fixture_by_tag(tag, context, fixture_registry):
        fixture_data = fixture_registry.get(tag, None)
        if fixture_data is None:
            raise LookupError(""Unknown fixture-tag: %s"" % tag)

        # -- FOR DATA SCHEMA 1:
        fixture_func = fixture_data
        return use_fixture(fixture_func, context)

        # -- FOR DATA SCHEMA 2:
        fixture_func, fixture_args, fixture_kwargs = fixture_data
        return use_fixture(fixture_func, context, *fixture_args, **fixture_kwargs)



.. hint:: **Naming Convention for Fixture Tags**

    Fixture tags should start with ``""@fixture.*""`` prefix to improve readability
    and understandibilty in feature files (Gherkin).

    Tags are used for different purposes. Therefore, it should be clear
    when a ``fixture-tag`` is used.



Fixture Cleanup Points
------------------------------------------------------------------------------

The point when a fixture-cleanup is performed depends on the scope where
:func:`~behave.use_fixture()` is called (and the fixture-setup is performed).


============= =========================== ==========================================================================================
Context Layer Fixture-Setup Point         Fixture-Cleanup Point
============= =========================== ==========================================================================================
test run      In ``before_all()`` hook    After ``after_all()``       at end of test-run.
feature       In ``before_feature()``     After ``after_feature()``,  at end of feature.
feature       In ``before_tag()``         After ``after_feature()``   for feature tag.
scenario      In ``before_scenario()``    After ``after_scenario()``, at end of scenario.
scenario      In ``before_tag()``         After ``after_scenario()``  for scenario tag.
scenario      In a step                   After ``after_scenario()``. Fixture is usable until end of scenario.
============= =========================== ==========================================================================================


Fixture Setup/Cleanup Semantics
------------------------------------------------------------------------------

If an error occurs during fixture-setup (meaning an exception is raised):

* Feature/scenario execution is aborted
* Any remaining fixture-setups are skipped
* After feature/scenario hooks are processed
* All fixture-cleanups and context cleanups are performed
* The feature/scenario is marked as failed

If an error occurs during fixture-cleanup (meaning an exception is raised):

* All remaining fixture-cleanups and context cleanups are performed
* First cleanup-error is reraised to pass failure to user (test runner)
* The feature/scenario is marked as failed



Ensure Fixture Cleanups with Fixture Setup Errors
------------------------------------------------------------------------------

Fixture-setup errors are special because a cleanup of a fixture is in many
cases not necessary (or rather difficult because the fixture object
is only partly created, etc.). Therefore, if an error occurs during fixture-setup
(meaning: an exception is raised), the fixture-cleanup part is normally not called.

If you need to ensure that the fixture-cleanup is performed, you need to
provide a slightly different fixture implementation:

.. code-block:: python

    # -- FILE: behave4my_project/fixtures.py  (or: features/environment.py)
    from behave import fixture
    from somewhere.browser.firefox import FirefoxBrowser

    def setup_fixture_part2_with_error(arg):
        raise RuntimeError(""OOPS-FIXTURE-SETUP-ERROR-HERE)

    # -- FIXTURE-VARIANT 1: Use generator-function with try/finally.
    @fixture
    def browser_firefox(context, timeout=30, **kwargs):
        try:
            browser = FirefoxBrowser(timeout, **kwargs)
            browser.part2 = setup_fixture_part2_with_error(""OOPS"")
            context.browser = browser   # NOT_REACHED
            yield browser
            # -- NORMAL FIXTURE-CLEANUP PART: NOT_REACHED due to setup-error.
         finally:
            browser.shutdown()  # -- CLEANUP: When generator-function is left.

.. code-block:: python

    # -- FIXTURE-VARIANT 2: Use normal function and register cleanup-task early.
    from somewhere.browser.chrome import ChromeBrowser

    @fixture
    def browser_chrome(context, timeout=30, **kwargs):
        browser = ChromeBrowser(timeout, **kwargs)
        context.browser = browser
        context.add_cleanup(browser.shutdown)   # -- ENSURE-CLEANUP EARLY
        browser.part2 = setup_fixture_part2_with_error(""OOPS"")
        return browser  # NOT_REACHED
        # -- CLEANUP: browser.shutdown() when context-layer is removed.

.. note::

    An fixture-setup-error that occurs when the browser object is created,
    is not covered by these solutions and not so easy to solve.



Composite Fixtures
------------------------------------------------------------------------------

The last section already describes some problems when you use
complex or *composite fixtures*. It must be ensured that cleanup of already
created fixture parts is performed even when errors occur late in the creation
of a *composite fixture*. This is basically a `scope guard`_ problem.

Solution 1:
~~~~~~~~~~~

.. code-block:: python

    # -- FILE: behave4my_project/fixtures.py
    # SOLUTION 1: Use ""use_fixture()"" to ensure cleanup even in case of errors.
    from behave import fixture, use_fixture

    @fixture
    def foo(context, *args, **kwargs):
        pass    # -- FIXTURE IMPLEMENTATION: Not of interest here.

    @fixture
    def bar(context, *args, **kwargs):
        pass    # -- FIXTURE IMPLEMENTATION: Not of interest here.

    # -- SOLUTION: With use_fixture()
    # ENSURES: foo-fixture is cleaned up even when setup-error occurs later.
    @fixture
    def composite1(context, *args, **kwargs):
        the_fixture1 = use_fixture(foo, context)
        the_fixture2 = use_fixture(bar, context)
        return [the_fixture1, the_fixture2]


Solution 2:
~~~~~~~~~~~

.. code-block:: python

    # -- ALTERNATIVE SOLUTION: With use_composite_fixture_with()
    from behave import fixture
    from behave.fixture import use_composite_fixture_with, fixture_call_params

    @fixture
    def composite2(context, *args, **kwargs):
        the_composite = use_composite_fixture_with(context, [
            fixture_call_params(foo, name=""foo""),
            fixture_call_params(bar, name=""bar""),
        ])
        return the_composite

}





",/content/Feature9.feature," Scenario: Use fixture (case: step)
    Given a file named ""features/environment.py"" with:
      """"""
      from __future__ import print_function

      def after_scenario(context, scenario):
          print(""HOOK-CALLED: after_scenario: %s"" % scenario.name)
      """"""
    And an empty file named ""behave4me/__init__.py""
    And a file named ""behave4me/fixtures.py"" with:
      """"""
      from __future__ import print_function
      from behave import fixture

      @fixture
      def foo(context, name):
          print(""FIXTURE-SETUP: foo%s"" % name)
          yield 42
          print(""FIXTURE-CLEANUP: foo%s"" % name)
      """"""
    And a file named ""features/steps/fixture_steps2.py"" with:
      """"""
      from __future__ import print_function
      from behave import step, use_fixture
      from behave4me.fixtures import foo

      @step(u'I use fixture ""{fixture_name}""')
      def step_use_fixture(context, fixture_name):
          if fixture_name.startswith(""foo""):
              name = fixture_name.replace(""foo"", """")
              the_fixture = use_fixture(foo, context, name)
              setattr(context, fixture_name, the_fixture)
          else:
              raise LookupError(""Unknown fixture: %s"" % fixture_name)
      """"""
    And a file named ""features/use3.feature"" with:
      """"""
      @fixture.foo
      Feature:

        Scenario: Use Fixture
          Given I use fixture ""foo_1""
          Then a step passes

        Scenario:
          Then another step passes
      """"""
    When I run ""behave -f plain features/use3.feature""
    Then it should pass with:
      """"""
      2 scenarios passed, 0 failed, 0 skipped
      3 steps passed, 0 failed, 0 skipped, 0 undefined
      """"""
    And the command output should contain:
      """"""
      Feature:
        Scenario: Use Fixture
          Given I use fixture ""foo_1"" ... passed
          Then a step passes ... passed
        HOOK-CALLED: after_scenario: Use Fixture
        FIXTURE-CLEANUP: foo_1
      """"""
    But note that ""the fixture-cleanup occurs after the scenario"""
/content/Record10.rst,"{
Scenario:.. _docid.fixtures:

Fixtures
==============================================================================

A common task during test execution is to:

* setup a functionality when a test-scope is entered
* cleanup (or teardown) the functionality at the end of the test-scope

**Fixtures** are provided as concept to simplify this setup/cleanup task
in `behave`_.

.. include:: _common_extlinks.rst

Providing a Fixture
-------------------

.. code-block:: python

    # -- FILE: behave4my_project/fixtures.py  (or in: features/environment.py)
    from behave import fixture
    from somewhere.browser.firefox import FirefoxBrowser

    # -- FIXTURE-VARIANT 1: Use generator-function
    @fixture
    def browser_firefox(context, timeout=30, **kwargs):
        # -- SETUP-FIXTURE PART:
        context.browser = FirefoxBrowser(timeout, **kwargs)
        yield context.browser
        # -- CLEANUP-FIXTURE PART:
        context.browser.shutdown()

.. code-block:: python

    # -- FIXTURE-VARIANT 2: Use normal function
    from somewhere.browser.chrome import ChromeBrowser

    @fixture
    def browser_chrome(context, timeout=30, **kwargs):
        # -- SETUP-FIXTURE PART: And register as context-cleanup task.
        browser = ChromeBrowser(timeout, **kwargs)
        context.browser = browser
        context.add_cleanup(browser.shutdown)
        return browser
        # -- CLEANUP-FIXTURE PART: browser.shutdown()
        # Fixture-cleanup is called when current context-layer is removed.

.. seealso::

    A *fixture* is similar to:

    * a :func:`contextlib.contextmanager`
    * a `pytest.fixture`_
    * the `scope guard`_ idiom

Given:Using a Fixture
---------------

In many cases, the usage of a fixture is triggered by the ``fixture-tag``
in a feature file. The ``fixture-tag`` marks that a fixture
should be used in this scenario/feature (as test-scope).

.. code-block:: gherkin

    # -- FILE: features/use_fixture1.feature
    Feature: Use Fixture on Scenario Level

        @fixture.browser.firefox
        Scenario: Use Web Browser Firefox
            Given I load web page ""https://somewhere.web""
            ...
        # -- AFTER-SCENARIO: Cleanup fixture.browser.firefox

.. code-block:: gherkin

    # -- FILE: features/use_fixture2.feature
    @fixture.browser.firefox
    Feature: Use Fixture on Feature Level

        Scenario: Use Web Browser Firefox
            Given I load web page ""https://somewhere.web""
            ...

        Scenario: Another Browser Test
            ...

    # -- AFTER-FEATURE: Cleanup fixture.browser.firefox


A **fixture** can be used by calling the :func:`~behave.use_fixture()` function.
The :func:`~behave.use_fixture()` call performs the ``SETUP-FIXTURE`` part and returns the
setup result. In addition, it ensures that ``CLEANUP-FIXTURE`` part is called
later-on when the current context-layer is removed.
Therefore, any manual cleanup handling in the ``after_tag()`` hook is not necessary.

.. code-block:: python

    # -- FILE: features/environment.py
    from behave import use_fixture
    from behave4my_project.fixtures import browser_firefox

    def before_tag(context, tag):
        if tag == ""fixture.browser.firefox"":
            use_fixture(browser_firefox, context, timeout=10)



Then:Realistic Example
~~~~~~~~~~~~~~~~~

A more realistic example by using a fixture registry is shown below:

.. code-block:: python

    # -- FILE: features/environment.py
    from behave.fixture import use_fixture_by_tag, fixture_call_params
    from behave4my_project.fixtures import browser_firefox, browser_chrome

    # -- REGISTRY DATA SCHEMA 1: fixture_func
    fixture_registry1 = {
        ""fixture.browser.firefox"": browser_firefox,
        ""fixture.browser.chrome"":  browser_chrome,
    }
    # -- REGISTRY DATA SCHEMA 2: (fixture_func, fixture_args, fixture_kwargs)
    fixture_registry2 = {
        ""fixture.browser.firefox"": fixture_call_params(browser_firefox),
        ""fixture.browser.chrome"":  fixture_call_params(browser_chrome, timeout=12),
    }

    def before_tag(context, tag):
        if tag.startswith(""fixture.""):
            return use_fixture_by_tag(tag, context, fixture_registry1):
        # -- MORE: Tag processing steps ...


.. code-block:: python

    # -- FILE: behave/fixture.py
    # ...
    def use_fixture_by_tag(tag, context, fixture_registry):
        fixture_data = fixture_registry.get(tag, None)
        if fixture_data is None:
            raise LookupError(""Unknown fixture-tag: %s"" % tag)

        # -- FOR DATA SCHEMA 1:
        fixture_func = fixture_data
        return use_fixture(fixture_func, context)

        # -- FOR DATA SCHEMA 2:
        fixture_func, fixture_args, fixture_kwargs = fixture_data
        return use_fixture(fixture_func, context, *fixture_args, **fixture_kwargs)



.. hint:: **Naming Convention for Fixture Tags**

    Fixture tags should start with ``""@fixture.*""`` prefix to improve readability
    and understandibilty in feature files (Gherkin).

    Tags are used for different purposes. Therefore, it should be clear
    when a ``fixture-tag`` is used.



Fixture Cleanup Points
------------------------------------------------------------------------------

The point when a fixture-cleanup is performed depends on the scope where
:func:`~behave.use_fixture()` is called (and the fixture-setup is performed).


============= =========================== ==========================================================================================
Context Layer Fixture-Setup Point         Fixture-Cleanup Point
============= =========================== ==========================================================================================
test run      In ``before_all()`` hook    After ``after_all()``       at end of test-run.
feature       In ``before_feature()``     After ``after_feature()``,  at end of feature.
feature       In ``before_tag()``         After ``after_feature()``   for feature tag.
scenario      In ``before_scenario()``    After ``after_scenario()``, at end of scenario.
scenario      In ``before_tag()``         After ``after_scenario()``  for scenario tag.
scenario      In a step                   After ``after_scenario()``. Fixture is usable until end of scenario.
============= =========================== ==========================================================================================


Fixture Setup/Cleanup Semantics
------------------------------------------------------------------------------

If an error occurs during fixture-setup (meaning an exception is raised):

* Feature/scenario execution is aborted
* Any remaining fixture-setups are skipped
* After feature/scenario hooks are processed
* All fixture-cleanups and context cleanups are performed
* The feature/scenario is marked as failed

If an error occurs during fixture-cleanup (meaning an exception is raised):

* All remaining fixture-cleanups and context cleanups are performed
* First cleanup-error is reraised to pass failure to user (test runner)
* The feature/scenario is marked as failed



Ensure Fixture Cleanups with Fixture Setup Errors
------------------------------------------------------------------------------

Fixture-setup errors are special because a cleanup of a fixture is in many
cases not necessary (or rather difficult because the fixture object
is only partly created, etc.). Therefore, if an error occurs during fixture-setup
(meaning: an exception is raised), the fixture-cleanup part is normally not called.

If you need to ensure that the fixture-cleanup is performed, you need to
provide a slightly different fixture implementation:

.. code-block:: python

    # -- FILE: behave4my_project/fixtures.py  (or: features/environment.py)
    from behave import fixture
    from somewhere.browser.firefox import FirefoxBrowser

    def setup_fixture_part2_with_error(arg):
        raise RuntimeError(""OOPS-FIXTURE-SETUP-ERROR-HERE)

    # -- FIXTURE-VARIANT 1: Use generator-function with try/finally.
    @fixture
    def browser_firefox(context, timeout=30, **kwargs):
        try:
            browser = FirefoxBrowser(timeout, **kwargs)
            browser.part2 = setup_fixture_part2_with_error(""OOPS"")
            context.browser = browser   # NOT_REACHED
            yield browser
            # -- NORMAL FIXTURE-CLEANUP PART: NOT_REACHED due to setup-error.
         finally:
            browser.shutdown()  # -- CLEANUP: When generator-function is left.

.. code-block:: python

    # -- FIXTURE-VARIANT 2: Use normal function and register cleanup-task early.
    from somewhere.browser.chrome import ChromeBrowser

    @fixture
    def browser_chrome(context, timeout=30, **kwargs):
        browser = ChromeBrowser(timeout, **kwargs)
        context.browser = browser
        context.add_cleanup(browser.shutdown)   # -- ENSURE-CLEANUP EARLY
        browser.part2 = setup_fixture_part2_with_error(""OOPS"")
        return browser  # NOT_REACHED
        # -- CLEANUP: browser.shutdown() when context-layer is removed.

.. note::

    An fixture-setup-error that occurs when the browser object is created,
    is not covered by these solutions and not so easy to solve.



Composite Fixtures
------------------------------------------------------------------------------

The last section already describes some problems when you use
complex or *composite fixtures*. It must be ensured that cleanup of already
created fixture parts is performed even when errors occur late in the creation
of a *composite fixture*. This is basically a `scope guard`_ problem.

Solution 1:
~~~~~~~~~~~

.. code-block:: python

    # -- FILE: behave4my_project/fixtures.py
    # SOLUTION 1: Use ""use_fixture()"" to ensure cleanup even in case of errors.
    from behave import fixture, use_fixture

    @fixture
    def foo(context, *args, **kwargs):
        pass    # -- FIXTURE IMPLEMENTATION: Not of interest here.

    @fixture
    def bar(context, *args, **kwargs):
        pass    # -- FIXTURE IMPLEMENTATION: Not of interest here.

    # -- SOLUTION: With use_fixture()
    # ENSURES: foo-fixture is cleaned up even when setup-error occurs later.
    @fixture
    def composite1(context, *args, **kwargs):
        the_fixture1 = use_fixture(foo, context)
        the_fixture2 = use_fixture(bar, context)
        return [the_fixture1, the_fixture2]


Solution 2:
~~~~~~~~~~~

.. code-block:: python

    # -- ALTERNATIVE SOLUTION: With use_composite_fixture_with()
    from behave import fixture
    from behave.fixture import use_composite_fixture_with, fixture_call_params

    @fixture
    def composite2(context, *args, **kwargs):
        the_composite = use_composite_fixture_with(context, [
            fixture_call_params(foo, name=""foo""),
            fixture_call_params(bar, name=""bar""),
        ])
        return the_composite

}





",/content/Feature10.feature," Scenario: Use multiple fixtures (with setup/cleanup)
    Given a file named ""features/environment.py"" with:
      """"""
      from __future__ import print_function
      from behave.fixture import fixture, use_fixture

      @fixture
      def foo(context):
          print(""FIXTURE-SETUP: foo"")
          yield
          print(""FIXTURE-CLEANUP: foo"")

      @fixture
      def bar(context):
          def cleanup_bar():
              print(""FIXTURE-CLEANUP: bar"")

          print(""FIXTURE-SETUP: bar"")
          context.add_cleanup(cleanup_bar)

      def before_tag(context, tag):
          if tag == ""fixture.foo"":
              use_fixture(foo, context)
          elif tag == ""fixture.bar"":
              use_fixture(bar, context)
      """"""
    And a file named ""features/two.feature"" with:
      """"""
      Feature: Use multiple Fixtures
        @fixture.foo
        @fixture.bar
        Scenario: Two Fixtures
          Given a step passes

        Scenario:
          Then another step passes
      """"""
    When I run ""behave -f plain features/two.feature""
    Then it should pass with:
      """"""
      2 scenarios passed, 0 failed, 0 skipped
      2 steps passed, 0 failed, 0 skipped, 0 undefined
      """"""
    And the command output should contain:
      """"""
      FIXTURE-SETUP: foo
      FIXTURE-SETUP: bar
        Scenario: Two Fixtures
          Given a step passes ... passed
      FIXTURE-CLEANUP: bar
      FIXTURE-CLEANUP: foo
      """"""
    But note that ""the fixture-cleanup occurs in reverse order (LIFO)"""
/content/Record11.rst,"{
Scenario:.. _docid.fixtures:

Fixtures
==============================================================================

A common task during test execution is to:

* setup a functionality when a test-scope is entered
* cleanup (or teardown) the functionality at the end of the test-scope

**Fixtures** are provided as concept to simplify this setup/cleanup task
in `behave`_.

.. include:: _common_extlinks.rst

Providing a Fixture
-------------------

.. code-block:: python

    # -- FILE: behave4my_project/fixtures.py  (or in: features/environment.py)
    from behave import fixture
    from somewhere.browser.firefox import FirefoxBrowser

    # -- FIXTURE-VARIANT 1: Use generator-function
    @fixture
    def browser_firefox(context, timeout=30, **kwargs):
        # -- SETUP-FIXTURE PART:
        context.browser = FirefoxBrowser(timeout, **kwargs)
        yield context.browser
        # -- CLEANUP-FIXTURE PART:
        context.browser.shutdown()

.. code-block:: python

    # -- FIXTURE-VARIANT 2: Use normal function
    from somewhere.browser.chrome import ChromeBrowser

    @fixture
    def browser_chrome(context, timeout=30, **kwargs):
        # -- SETUP-FIXTURE PART: And register as context-cleanup task.
        browser = ChromeBrowser(timeout, **kwargs)
        context.browser = browser
        context.add_cleanup(browser.shutdown)
        return browser
        # -- CLEANUP-FIXTURE PART: browser.shutdown()
        # Fixture-cleanup is called when current context-layer is removed.

.. seealso::

    A *fixture* is similar to:

    * a :func:`contextlib.contextmanager`
    * a `pytest.fixture`_
    * the `scope guard`_ idiom

Given:Using a Fixture
---------------

In many cases, the usage of a fixture is triggered by the ``fixture-tag``
in a feature file. The ``fixture-tag`` marks that a fixture
should be used in this scenario/feature (as test-scope).

.. code-block:: gherkin

    # -- FILE: features/use_fixture1.feature
    Feature: Use Fixture on Scenario Level

        @fixture.browser.firefox
        Scenario: Use Web Browser Firefox
            Given I load web page ""https://somewhere.web""
            ...
        # -- AFTER-SCENARIO: Cleanup fixture.browser.firefox

.. code-block:: gherkin

    # -- FILE: features/use_fixture2.feature
    @fixture.browser.firefox
    Feature: Use Fixture on Feature Level

        Scenario: Use Web Browser Firefox
            Given I load web page ""https://somewhere.web""
            ...

        Scenario: Another Browser Test
            ...

    # -- AFTER-FEATURE: Cleanup fixture.browser.firefox


A **fixture** can be used by calling the :func:`~behave.use_fixture()` function.
The :func:`~behave.use_fixture()` call performs the ``SETUP-FIXTURE`` part and returns the
setup result. In addition, it ensures that ``CLEANUP-FIXTURE`` part is called
later-on when the current context-layer is removed.
Therefore, any manual cleanup handling in the ``after_tag()`` hook is not necessary.

.. code-block:: python

    # -- FILE: features/environment.py
    from behave import use_fixture
    from behave4my_project.fixtures import browser_firefox

    def before_tag(context, tag):
        if tag == ""fixture.browser.firefox"":
            use_fixture(browser_firefox, context, timeout=10)



Then:Realistic Example
~~~~~~~~~~~~~~~~~

A more realistic example by using a fixture registry is shown below:

.. code-block:: python

    # -- FILE: features/environment.py
    from behave.fixture import use_fixture_by_tag, fixture_call_params
    from behave4my_project.fixtures import browser_firefox, browser_chrome

    # -- REGISTRY DATA SCHEMA 1: fixture_func
    fixture_registry1 = {
        ""fixture.browser.firefox"": browser_firefox,
        ""fixture.browser.chrome"":  browser_chrome,
    }
    # -- REGISTRY DATA SCHEMA 2: (fixture_func, fixture_args, fixture_kwargs)
    fixture_registry2 = {
        ""fixture.browser.firefox"": fixture_call_params(browser_firefox),
        ""fixture.browser.chrome"":  fixture_call_params(browser_chrome, timeout=12),
    }

    def before_tag(context, tag):
        if tag.startswith(""fixture.""):
            return use_fixture_by_tag(tag, context, fixture_registry1):
        # -- MORE: Tag processing steps ...


.. code-block:: python

    # -- FILE: behave/fixture.py
    # ...
    def use_fixture_by_tag(tag, context, fixture_registry):
        fixture_data = fixture_registry.get(tag, None)
        if fixture_data is None:
            raise LookupError(""Unknown fixture-tag: %s"" % tag)

        # -- FOR DATA SCHEMA 1:
        fixture_func = fixture_data
        return use_fixture(fixture_func, context)

        # -- FOR DATA SCHEMA 2:
        fixture_func, fixture_args, fixture_kwargs = fixture_data
        return use_fixture(fixture_func, context, *fixture_args, **fixture_kwargs)



.. hint:: **Naming Convention for Fixture Tags**

    Fixture tags should start with ``""@fixture.*""`` prefix to improve readability
    and understandibilty in feature files (Gherkin).

    Tags are used for different purposes. Therefore, it should be clear
    when a ``fixture-tag`` is used.



Fixture Cleanup Points
------------------------------------------------------------------------------

The point when a fixture-cleanup is performed depends on the scope where
:func:`~behave.use_fixture()` is called (and the fixture-setup is performed).


============= =========================== ==========================================================================================
Context Layer Fixture-Setup Point         Fixture-Cleanup Point
============= =========================== ==========================================================================================
test run      In ``before_all()`` hook    After ``after_all()``       at end of test-run.
feature       In ``before_feature()``     After ``after_feature()``,  at end of feature.
feature       In ``before_tag()``         After ``after_feature()``   for feature tag.
scenario      In ``before_scenario()``    After ``after_scenario()``, at end of scenario.
scenario      In ``before_tag()``         After ``after_scenario()``  for scenario tag.
scenario      In a step                   After ``after_scenario()``. Fixture is usable until end of scenario.
============= =========================== ==========================================================================================


Fixture Setup/Cleanup Semantics
------------------------------------------------------------------------------

If an error occurs during fixture-setup (meaning an exception is raised):

* Feature/scenario execution is aborted
* Any remaining fixture-setups are skipped
* After feature/scenario hooks are processed
* All fixture-cleanups and context cleanups are performed
* The feature/scenario is marked as failed

If an error occurs during fixture-cleanup (meaning an exception is raised):

* All remaining fixture-cleanups and context cleanups are performed
* First cleanup-error is reraised to pass failure to user (test runner)
* The feature/scenario is marked as failed



Ensure Fixture Cleanups with Fixture Setup Errors
------------------------------------------------------------------------------

Fixture-setup errors are special because a cleanup of a fixture is in many
cases not necessary (or rather difficult because the fixture object
is only partly created, etc.). Therefore, if an error occurs during fixture-setup
(meaning: an exception is raised), the fixture-cleanup part is normally not called.

If you need to ensure that the fixture-cleanup is performed, you need to
provide a slightly different fixture implementation:

.. code-block:: python

    # -- FILE: behave4my_project/fixtures.py  (or: features/environment.py)
    from behave import fixture
    from somewhere.browser.firefox import FirefoxBrowser

    def setup_fixture_part2_with_error(arg):
        raise RuntimeError(""OOPS-FIXTURE-SETUP-ERROR-HERE)

    # -- FIXTURE-VARIANT 1: Use generator-function with try/finally.
    @fixture
    def browser_firefox(context, timeout=30, **kwargs):
        try:
            browser = FirefoxBrowser(timeout, **kwargs)
            browser.part2 = setup_fixture_part2_with_error(""OOPS"")
            context.browser = browser   # NOT_REACHED
            yield browser
            # -- NORMAL FIXTURE-CLEANUP PART: NOT_REACHED due to setup-error.
         finally:
            browser.shutdown()  # -- CLEANUP: When generator-function is left.

.. code-block:: python

    # -- FIXTURE-VARIANT 2: Use normal function and register cleanup-task early.
    from somewhere.browser.chrome import ChromeBrowser

    @fixture
    def browser_chrome(context, timeout=30, **kwargs):
        browser = ChromeBrowser(timeout, **kwargs)
        context.browser = browser
        context.add_cleanup(browser.shutdown)   # -- ENSURE-CLEANUP EARLY
        browser.part2 = setup_fixture_part2_with_error(""OOPS"")
        return browser  # NOT_REACHED
        # -- CLEANUP: browser.shutdown() when context-layer is removed.

.. note::

    An fixture-setup-error that occurs when the browser object is created,
    is not covered by these solutions and not so easy to solve.



Composite Fixtures
------------------------------------------------------------------------------

The last section already describes some problems when you use
complex or *composite fixtures*. It must be ensured that cleanup of already
created fixture parts is performed even when errors occur late in the creation
of a *composite fixture*. This is basically a `scope guard`_ problem.

Solution 1:
~~~~~~~~~~~

.. code-block:: python

    # -- FILE: behave4my_project/fixtures.py
    # SOLUTION 1: Use ""use_fixture()"" to ensure cleanup even in case of errors.
    from behave import fixture, use_fixture

    @fixture
    def foo(context, *args, **kwargs):
        pass    # -- FIXTURE IMPLEMENTATION: Not of interest here.

    @fixture
    def bar(context, *args, **kwargs):
        pass    # -- FIXTURE IMPLEMENTATION: Not of interest here.

    # -- SOLUTION: With use_fixture()
    # ENSURES: foo-fixture is cleaned up even when setup-error occurs later.
    @fixture
    def composite1(context, *args, **kwargs):
        the_fixture1 = use_fixture(foo, context)
        the_fixture2 = use_fixture(bar, context)
        return [the_fixture1, the_fixture2]


Solution 2:
~~~~~~~~~~~

.. code-block:: python

    # -- ALTERNATIVE SOLUTION: With use_composite_fixture_with()
    from behave import fixture
    from behave.fixture import use_composite_fixture_with, fixture_call_params

    @fixture
    def composite2(context, *args, **kwargs):
        the_composite = use_composite_fixture_with(context, [
            fixture_call_params(foo, name=""foo""),
            fixture_call_params(bar, name=""bar""),
        ])
        return the_composite

}





",/content/Feature11.feature,"Scenario: Use same fixture twice with different args
    Given a file named ""features/environment.py"" with:
      """"""
      from __future__ import print_function
      from behave.fixture import fixture, use_fixture

      @fixture
      def foo(context, name):
          name2 = ""foo%s"" % name
          print(""FIXTURE-SETUP: %s"" % name2)
          setattr(context, name2, 1)
          yield
          print(""FIXTURE-CLEANUP: %s"" % name2)

      def before_tag(context, tag):
          if tag.startswith(""fixture.foo""):
              # -- FIXTURE-TAG SCHEMA: fixture.foo*
              name = tag.replace(""fixture.foo"", """")
              use_fixture(foo, context, name)
      """"""
    And a file named ""features/two.feature"" with:
      """"""
      Feature: Use same Fixture twice
        @fixture.foo_1
        @fixture.foo_2
        Scenario: Use Fixtures
          Given a step passes

        Scenario:
          Then another step passes
      """"""
    When I run ""behave -f plain features/two.feature""
    Then it should pass with:
      """"""
      2 scenarios passed, 0 failed, 0 skipped
      2 steps passed, 0 failed, 0 skipped, 0 undefined
      """"""
    And the command output should contain:
      """"""
      FIXTURE-SETUP: foo_1
      FIXTURE-SETUP: foo_2
        Scenario: Use Fixtures
          Given a step passes ... passed
      FIXTURE-CLEANUP: foo_2
      FIXTURE-CLEANUP: foo_1
      """"""
    But note that ""the fixture-cleanup occurs in reverse order (LIFO)"""
/content/Record12.rst,"{
Scenario:.. _docid.fixtures:

Fixtures
==============================================================================

A common task during test execution is to:

* setup a functionality when a test-scope is entered
* cleanup (or teardown) the functionality at the end of the test-scope

**Fixtures** are provided as concept to simplify this setup/cleanup task
in `behave`_.

.. include:: _common_extlinks.rst

Providing a Fixture
-------------------

.. code-block:: python

    # -- FILE: behave4my_project/fixtures.py  (or in: features/environment.py)
    from behave import fixture
    from somewhere.browser.firefox import FirefoxBrowser

    # -- FIXTURE-VARIANT 1: Use generator-function
    @fixture
    def browser_firefox(context, timeout=30, **kwargs):
        # -- SETUP-FIXTURE PART:
        context.browser = FirefoxBrowser(timeout, **kwargs)
        yield context.browser
        # -- CLEANUP-FIXTURE PART:
        context.browser.shutdown()

.. code-block:: python

    # -- FIXTURE-VARIANT 2: Use normal function
    from somewhere.browser.chrome import ChromeBrowser

    @fixture
    def browser_chrome(context, timeout=30, **kwargs):
        # -- SETUP-FIXTURE PART: And register as context-cleanup task.
        browser = ChromeBrowser(timeout, **kwargs)
        context.browser = browser
        context.add_cleanup(browser.shutdown)
        return browser
        # -- CLEANUP-FIXTURE PART: browser.shutdown()
        # Fixture-cleanup is called when current context-layer is removed.

.. seealso::

    A *fixture* is similar to:

    * a :func:`contextlib.contextmanager`
    * a `pytest.fixture`_
    * the `scope guard`_ idiom

Given:Using a Fixture
---------------

In many cases, the usage of a fixture is triggered by the ``fixture-tag``
in a feature file. The ``fixture-tag`` marks that a fixture
should be used in this scenario/feature (as test-scope).

.. code-block:: gherkin

    # -- FILE: features/use_fixture1.feature
    Feature: Use Fixture on Scenario Level

        @fixture.browser.firefox
        Scenario: Use Web Browser Firefox
            Given I load web page ""https://somewhere.web""
            ...
        # -- AFTER-SCENARIO: Cleanup fixture.browser.firefox

.. code-block:: gherkin

    # -- FILE: features/use_fixture2.feature
    @fixture.browser.firefox
    Feature: Use Fixture on Feature Level

        Scenario: Use Web Browser Firefox
            Given I load web page ""https://somewhere.web""
            ...

        Scenario: Another Browser Test
            ...

    # -- AFTER-FEATURE: Cleanup fixture.browser.firefox


A **fixture** can be used by calling the :func:`~behave.use_fixture()` function.
The :func:`~behave.use_fixture()` call performs the ``SETUP-FIXTURE`` part and returns the
setup result. In addition, it ensures that ``CLEANUP-FIXTURE`` part is called
later-on when the current context-layer is removed.
Therefore, any manual cleanup handling in the ``after_tag()`` hook is not necessary.

.. code-block:: python

    # -- FILE: features/environment.py
    from behave import use_fixture
    from behave4my_project.fixtures import browser_firefox

    def before_tag(context, tag):
        if tag == ""fixture.browser.firefox"":
            use_fixture(browser_firefox, context, timeout=10)



Then:Realistic Example
~~~~~~~~~~~~~~~~~

A more realistic example by using a fixture registry is shown below:

.. code-block:: python

    # -- FILE: features/environment.py
    from behave.fixture import use_fixture_by_tag, fixture_call_params
    from behave4my_project.fixtures import browser_firefox, browser_chrome

    # -- REGISTRY DATA SCHEMA 1: fixture_func
    fixture_registry1 = {
        ""fixture.browser.firefox"": browser_firefox,
        ""fixture.browser.chrome"":  browser_chrome,
    }
    # -- REGISTRY DATA SCHEMA 2: (fixture_func, fixture_args, fixture_kwargs)
    fixture_registry2 = {
        ""fixture.browser.firefox"": fixture_call_params(browser_firefox),
        ""fixture.browser.chrome"":  fixture_call_params(browser_chrome, timeout=12),
    }

    def before_tag(context, tag):
        if tag.startswith(""fixture.""):
            return use_fixture_by_tag(tag, context, fixture_registry1):
        # -- MORE: Tag processing steps ...


.. code-block:: python

    # -- FILE: behave/fixture.py
    # ...
    def use_fixture_by_tag(tag, context, fixture_registry):
        fixture_data = fixture_registry.get(tag, None)
        if fixture_data is None:
            raise LookupError(""Unknown fixture-tag: %s"" % tag)

        # -- FOR DATA SCHEMA 1:
        fixture_func = fixture_data
        return use_fixture(fixture_func, context)

        # -- FOR DATA SCHEMA 2:
        fixture_func, fixture_args, fixture_kwargs = fixture_data
        return use_fixture(fixture_func, context, *fixture_args, **fixture_kwargs)



.. hint:: **Naming Convention for Fixture Tags**

    Fixture tags should start with ``""@fixture.*""`` prefix to improve readability
    and understandibilty in feature files (Gherkin).

    Tags are used for different purposes. Therefore, it should be clear
    when a ``fixture-tag`` is used.



Fixture Cleanup Points
------------------------------------------------------------------------------

The point when a fixture-cleanup is performed depends on the scope where
:func:`~behave.use_fixture()` is called (and the fixture-setup is performed).


============= =========================== ==========================================================================================
Context Layer Fixture-Setup Point         Fixture-Cleanup Point
============= =========================== ==========================================================================================
test run      In ``before_all()`` hook    After ``after_all()``       at end of test-run.
feature       In ``before_feature()``     After ``after_feature()``,  at end of feature.
feature       In ``before_tag()``         After ``after_feature()``   for feature tag.
scenario      In ``before_scenario()``    After ``after_scenario()``, at end of scenario.
scenario      In ``before_tag()``         After ``after_scenario()``  for scenario tag.
scenario      In a step                   After ``after_scenario()``. Fixture is usable until end of scenario.
============= =========================== ==========================================================================================


Fixture Setup/Cleanup Semantics
------------------------------------------------------------------------------

If an error occurs during fixture-setup (meaning an exception is raised):

* Feature/scenario execution is aborted
* Any remaining fixture-setups are skipped
* After feature/scenario hooks are processed
* All fixture-cleanups and context cleanups are performed
* The feature/scenario is marked as failed

If an error occurs during fixture-cleanup (meaning an exception is raised):

* All remaining fixture-cleanups and context cleanups are performed
* First cleanup-error is reraised to pass failure to user (test runner)
* The feature/scenario is marked as failed



Ensure Fixture Cleanups with Fixture Setup Errors
------------------------------------------------------------------------------

Fixture-setup errors are special because a cleanup of a fixture is in many
cases not necessary (or rather difficult because the fixture object
is only partly created, etc.). Therefore, if an error occurs during fixture-setup
(meaning: an exception is raised), the fixture-cleanup part is normally not called.

If you need to ensure that the fixture-cleanup is performed, you need to
provide a slightly different fixture implementation:

.. code-block:: python

    # -- FILE: behave4my_project/fixtures.py  (or: features/environment.py)
    from behave import fixture
    from somewhere.browser.firefox import FirefoxBrowser

    def setup_fixture_part2_with_error(arg):
        raise RuntimeError(""OOPS-FIXTURE-SETUP-ERROR-HERE)

    # -- FIXTURE-VARIANT 1: Use generator-function with try/finally.
    @fixture
    def browser_firefox(context, timeout=30, **kwargs):
        try:
            browser = FirefoxBrowser(timeout, **kwargs)
            browser.part2 = setup_fixture_part2_with_error(""OOPS"")
            context.browser = browser   # NOT_REACHED
            yield browser
            # -- NORMAL FIXTURE-CLEANUP PART: NOT_REACHED due to setup-error.
         finally:
            browser.shutdown()  # -- CLEANUP: When generator-function is left.

.. code-block:: python

    # -- FIXTURE-VARIANT 2: Use normal function and register cleanup-task early.
    from somewhere.browser.chrome import ChromeBrowser

    @fixture
    def browser_chrome(context, timeout=30, **kwargs):
        browser = ChromeBrowser(timeout, **kwargs)
        context.browser = browser
        context.add_cleanup(browser.shutdown)   # -- ENSURE-CLEANUP EARLY
        browser.part2 = setup_fixture_part2_with_error(""OOPS"")
        return browser  # NOT_REACHED
        # -- CLEANUP: browser.shutdown() when context-layer is removed.

.. note::

    An fixture-setup-error that occurs when the browser object is created,
    is not covered by these solutions and not so easy to solve.



Composite Fixtures
------------------------------------------------------------------------------

The last section already describes some problems when you use
complex or *composite fixtures*. It must be ensured that cleanup of already
created fixture parts is performed even when errors occur late in the creation
of a *composite fixture*. This is basically a `scope guard`_ problem.

Solution 1:
~~~~~~~~~~~

.. code-block:: python

    # -- FILE: behave4my_project/fixtures.py
    # SOLUTION 1: Use ""use_fixture()"" to ensure cleanup even in case of errors.
    from behave import fixture, use_fixture

    @fixture
    def foo(context, *args, **kwargs):
        pass    # -- FIXTURE IMPLEMENTATION: Not of interest here.

    @fixture
    def bar(context, *args, **kwargs):
        pass    # -- FIXTURE IMPLEMENTATION: Not of interest here.

    # -- SOLUTION: With use_fixture()
    # ENSURES: foo-fixture is cleaned up even when setup-error occurs later.
    @fixture
    def composite1(context, *args, **kwargs):
        the_fixture1 = use_fixture(foo, context)
        the_fixture2 = use_fixture(bar, context)
        return [the_fixture1, the_fixture2]


Solution 2:
~~~~~~~~~~~

.. code-block:: python

    # -- ALTERNATIVE SOLUTION: With use_composite_fixture_with()
    from behave import fixture
    from behave.fixture import use_composite_fixture_with, fixture_call_params

    @fixture
    def composite2(context, *args, **kwargs):
        the_composite = use_composite_fixture_with(context, [
            fixture_call_params(foo, name=""foo""),
            fixture_call_params(bar, name=""bar""),
        ])
        return the_composite

}





",/content/Feature12.feature," Scenario: Use invalid fixture (with two yields or more)
    Given a file named ""features/environment.py"" with:
      """"""
      from __future__ import print_function
      from behave.fixture import fixture, use_fixture

      @fixture
      def invalid_fixture(context):
          # -- CASE: Setup-only
          print(""FIXTURE-SETUP: invalid"")
          yield
          print(""FIXTURE-CLEANUP: invalid"")
          yield
          print(""OOPS: Too many yields used"")

      def before_tag(context, tag):
          if tag == ""fixture.invalid"":
              use_fixture(invalid_fixture, context)
      """"""
    And a file named ""features/invalid_fixture.feature"" with:
      """"""
      Feature: Fixture with more than one yield
        @fixture.invalid
        Scenario: Use invalid fixture
          Given a step passes

        Scenario:
          Then another step passes
      """"""
    When I run ""behave -f plain features/invalid_fixture.feature""
    Then it should fail with:
      """"""
      1 scenario passed, 1 failed, 0 skipped
      2 steps passed, 0 failed, 0 skipped, 0 undefined
      """"""
    And the command output should contain:
      """"""
      FIXTURE-SETUP: invalid
        Scenario: Use invalid fixture
          Given a step passes ... passed
      FIXTURE-CLEANUP: invalid
      CLEANUP-ERROR in cleanup_fixture: InvalidFixtureError: Has more than one yield: <function invalid_fixture at
      """"""
    But note that ""cleanup errors cause scenario to fail (by default)"""
/content/Record13.rst,"{
Scenario:.. _docid.fixtures:

Fixtures
==============================================================================

A common task during test execution is to:

* setup a functionality when a test-scope is entered
* cleanup (or teardown) the functionality at the end of the test-scope

**Fixtures** are provided as concept to simplify this setup/cleanup task
in `behave`_.

.. include:: _common_extlinks.rst

Providing a Fixture
-------------------

.. code-block:: python

    # -- FILE: behave4my_project/fixtures.py  (or in: features/environment.py)
    from behave import fixture
    from somewhere.browser.firefox import FirefoxBrowser

    # -- FIXTURE-VARIANT 1: Use generator-function
    @fixture
    def browser_firefox(context, timeout=30, **kwargs):
        # -- SETUP-FIXTURE PART:
        context.browser = FirefoxBrowser(timeout, **kwargs)
        yield context.browser
        # -- CLEANUP-FIXTURE PART:
        context.browser.shutdown()

.. code-block:: python

    # -- FIXTURE-VARIANT 2: Use normal function
    from somewhere.browser.chrome import ChromeBrowser

    @fixture
    def browser_chrome(context, timeout=30, **kwargs):
        # -- SETUP-FIXTURE PART: And register as context-cleanup task.
        browser = ChromeBrowser(timeout, **kwargs)
        context.browser = browser
        context.add_cleanup(browser.shutdown)
        return browser
        # -- CLEANUP-FIXTURE PART: browser.shutdown()
        # Fixture-cleanup is called when current context-layer is removed.

.. seealso::

    A *fixture* is similar to:

    * a :func:`contextlib.contextmanager`
    * a `pytest.fixture`_
    * the `scope guard`_ idiom

Given:Using a Fixture
---------------

In many cases, the usage of a fixture is triggered by the ``fixture-tag``
in a feature file. The ``fixture-tag`` marks that a fixture
should be used in this scenario/feature (as test-scope).

.. code-block:: gherkin

    # -- FILE: features/use_fixture1.feature
    Feature: Use Fixture on Scenario Level

        @fixture.browser.firefox
        Scenario: Use Web Browser Firefox
            Given I load web page ""https://somewhere.web""
            ...
        # -- AFTER-SCENARIO: Cleanup fixture.browser.firefox

.. code-block:: gherkin

    # -- FILE: features/use_fixture2.feature
    @fixture.browser.firefox
    Feature: Use Fixture on Feature Level

        Scenario: Use Web Browser Firefox
            Given I load web page ""https://somewhere.web""
            ...

        Scenario: Another Browser Test
            ...

    # -- AFTER-FEATURE: Cleanup fixture.browser.firefox


A **fixture** can be used by calling the :func:`~behave.use_fixture()` function.
The :func:`~behave.use_fixture()` call performs the ``SETUP-FIXTURE`` part and returns the
setup result. In addition, it ensures that ``CLEANUP-FIXTURE`` part is called
later-on when the current context-layer is removed.
Therefore, any manual cleanup handling in the ``after_tag()`` hook is not necessary.

.. code-block:: python

    # -- FILE: features/environment.py
    from behave import use_fixture
    from behave4my_project.fixtures import browser_firefox

    def before_tag(context, tag):
        if tag == ""fixture.browser.firefox"":
            use_fixture(browser_firefox, context, timeout=10)



Then:Realistic Example
~~~~~~~~~~~~~~~~~

A more realistic example by using a fixture registry is shown below:

.. code-block:: python

    # -- FILE: features/environment.py
    from behave.fixture import use_fixture_by_tag, fixture_call_params
    from behave4my_project.fixtures import browser_firefox, browser_chrome

    # -- REGISTRY DATA SCHEMA 1: fixture_func
    fixture_registry1 = {
        ""fixture.browser.firefox"": browser_firefox,
        ""fixture.browser.chrome"":  browser_chrome,
    }
    # -- REGISTRY DATA SCHEMA 2: (fixture_func, fixture_args, fixture_kwargs)
    fixture_registry2 = {
        ""fixture.browser.firefox"": fixture_call_params(browser_firefox),
        ""fixture.browser.chrome"":  fixture_call_params(browser_chrome, timeout=12),
    }

    def before_tag(context, tag):
        if tag.startswith(""fixture.""):
            return use_fixture_by_tag(tag, context, fixture_registry1):
        # -- MORE: Tag processing steps ...


.. code-block:: python

    # -- FILE: behave/fixture.py
    # ...
    def use_fixture_by_tag(tag, context, fixture_registry):
        fixture_data = fixture_registry.get(tag, None)
        if fixture_data is None:
            raise LookupError(""Unknown fixture-tag: %s"" % tag)

        # -- FOR DATA SCHEMA 1:
        fixture_func = fixture_data
        return use_fixture(fixture_func, context)

        # -- FOR DATA SCHEMA 2:
        fixture_func, fixture_args, fixture_kwargs = fixture_data
        return use_fixture(fixture_func, context, *fixture_args, **fixture_kwargs)



.. hint:: **Naming Convention for Fixture Tags**

    Fixture tags should start with ``""@fixture.*""`` prefix to improve readability
    and understandibilty in feature files (Gherkin).

    Tags are used for different purposes. Therefore, it should be clear
    when a ``fixture-tag`` is used.



Fixture Cleanup Points
------------------------------------------------------------------------------

The point when a fixture-cleanup is performed depends on the scope where
:func:`~behave.use_fixture()` is called (and the fixture-setup is performed).


============= =========================== ==========================================================================================
Context Layer Fixture-Setup Point         Fixture-Cleanup Point
============= =========================== ==========================================================================================
test run      In ``before_all()`` hook    After ``after_all()``       at end of test-run.
feature       In ``before_feature()``     After ``after_feature()``,  at end of feature.
feature       In ``before_tag()``         After ``after_feature()``   for feature tag.
scenario      In ``before_scenario()``    After ``after_scenario()``, at end of scenario.
scenario      In ``before_tag()``         After ``after_scenario()``  for scenario tag.
scenario      In a step                   After ``after_scenario()``. Fixture is usable until end of scenario.
============= =========================== ==========================================================================================


Fixture Setup/Cleanup Semantics
------------------------------------------------------------------------------

If an error occurs during fixture-setup (meaning an exception is raised):

* Feature/scenario execution is aborted
* Any remaining fixture-setups are skipped
* After feature/scenario hooks are processed
* All fixture-cleanups and context cleanups are performed
* The feature/scenario is marked as failed

If an error occurs during fixture-cleanup (meaning an exception is raised):

* All remaining fixture-cleanups and context cleanups are performed
* First cleanup-error is reraised to pass failure to user (test runner)
* The feature/scenario is marked as failed



Ensure Fixture Cleanups with Fixture Setup Errors
------------------------------------------------------------------------------

Fixture-setup errors are special because a cleanup of a fixture is in many
cases not necessary (or rather difficult because the fixture object
is only partly created, etc.). Therefore, if an error occurs during fixture-setup
(meaning: an exception is raised), the fixture-cleanup part is normally not called.

If you need to ensure that the fixture-cleanup is performed, you need to
provide a slightly different fixture implementation:

.. code-block:: python

    # -- FILE: behave4my_project/fixtures.py  (or: features/environment.py)
    from behave import fixture
    from somewhere.browser.firefox import FirefoxBrowser

    def setup_fixture_part2_with_error(arg):
        raise RuntimeError(""OOPS-FIXTURE-SETUP-ERROR-HERE)

    # -- FIXTURE-VARIANT 1: Use generator-function with try/finally.
    @fixture
    def browser_firefox(context, timeout=30, **kwargs):
        try:
            browser = FirefoxBrowser(timeout, **kwargs)
            browser.part2 = setup_fixture_part2_with_error(""OOPS"")
            context.browser = browser   # NOT_REACHED
            yield browser
            # -- NORMAL FIXTURE-CLEANUP PART: NOT_REACHED due to setup-error.
         finally:
            browser.shutdown()  # -- CLEANUP: When generator-function is left.

.. code-block:: python

    # -- FIXTURE-VARIANT 2: Use normal function and register cleanup-task early.
    from somewhere.browser.chrome import ChromeBrowser

    @fixture
    def browser_chrome(context, timeout=30, **kwargs):
        browser = ChromeBrowser(timeout, **kwargs)
        context.browser = browser
        context.add_cleanup(browser.shutdown)   # -- ENSURE-CLEANUP EARLY
        browser.part2 = setup_fixture_part2_with_error(""OOPS"")
        return browser  # NOT_REACHED
        # -- CLEANUP: browser.shutdown() when context-layer is removed.

.. note::

    An fixture-setup-error that occurs when the browser object is created,
    is not covered by these solutions and not so easy to solve.



Composite Fixtures
------------------------------------------------------------------------------

The last section already describes some problems when you use
complex or *composite fixtures*. It must be ensured that cleanup of already
created fixture parts is performed even when errors occur late in the creation
of a *composite fixture*. This is basically a `scope guard`_ problem.

Solution 1:
~~~~~~~~~~~

.. code-block:: python

    # -- FILE: behave4my_project/fixtures.py
    # SOLUTION 1: Use ""use_fixture()"" to ensure cleanup even in case of errors.
    from behave import fixture, use_fixture

    @fixture
    def foo(context, *args, **kwargs):
        pass    # -- FIXTURE IMPLEMENTATION: Not of interest here.

    @fixture
    def bar(context, *args, **kwargs):
        pass    # -- FIXTURE IMPLEMENTATION: Not of interest here.

    # -- SOLUTION: With use_fixture()
    # ENSURES: foo-fixture is cleaned up even when setup-error occurs later.
    @fixture
    def composite1(context, *args, **kwargs):
        the_fixture1 = use_fixture(foo, context)
        the_fixture2 = use_fixture(bar, context)
        return [the_fixture1, the_fixture2]


Solution 2:
~~~~~~~~~~~

.. code-block:: python

    # -- ALTERNATIVE SOLUTION: With use_composite_fixture_with()
    from behave import fixture
    from behave.fixture import use_composite_fixture_with, fixture_call_params

    @fixture
    def composite2(context, *args, **kwargs):
        the_composite = use_composite_fixture_with(context, [
            fixture_call_params(foo, name=""foo""),
            fixture_call_params(bar, name=""bar""),
        ])
        return the_composite

}





",/content/Feature13.feature,"Scenario: Fixture with cleanup-error causes failed (case: scenario)
    Given a file named ""features/environment.py"" with:
      """"""
      from __future__ import print_function
      from behave.fixture import fixture, use_fixture

      @fixture
      def bad_with_cleanup_error(context):
          print(""FIXTURE-SETUP: bad_with_cleanup_error"")
          yield
          print(""FIXTURE-CLEANUP: bad_with_cleanup_error"")
          raise RuntimeError(""BAD_FIXTURE_CLEANUP_ERROR"")

      def before_tag(context, tag):
          if tag == ""fixture.bad_with_cleanup_error"":
              use_fixture(bad_with_cleanup_error, context)
      """"""
    And a file named ""features/bad_fixture.feature"" with:
      """"""
      Feature: Fixture with cleanup error
        @fixture.bad_with_cleanup_error
        Scenario: Use fixture
          Given a step passes

        Scenario:
          Then another step passes
      """"""
    When I run ""behave -f plain features/bad_fixture.feature""
    Then it should fail with:
      """"""
      1 scenario passed, 1 failed, 0 skipped
      2 steps passed, 0 failed, 0 skipped, 0 undefined
      """"""
    And the command output should contain:
      """"""
      FIXTURE-SETUP: bad_with_cleanup_error
        Scenario: Use fixture
          Given a step passes ... passed
      FIXTURE-CLEANUP: bad_with_cleanup_error
      CLEANUP-ERROR in cleanup_fixture: RuntimeError: BAD_FIXTURE_CLEANUP_ERROR
      """"""
    And the command output should contain:
      """"""
      File ""features/environment.py"", line 9, in bad_with_cleanup_error
        raise RuntimeError(""BAD_FIXTURE_CLEANUP_ERROR"")
      RuntimeError: BAD_FIXTURE_CLEANUP_ERROR
      """"""
    But note that ""traceback shows cleanup-fixture failure location""
    And note that ""cleanup error causes scenario to fail (by default)"""
/content/Record14.rst,"{
Scenario:.. _docid.fixtures:

Fixtures
==============================================================================

A common task during test execution is to:

* setup a functionality when a test-scope is entered
* cleanup (or teardown) the functionality at the end of the test-scope

**Fixtures** are provided as concept to simplify this setup/cleanup task
in `behave`_.

.. include:: _common_extlinks.rst

Providing a Fixture
-------------------

.. code-block:: python

    # -- FILE: behave4my_project/fixtures.py  (or in: features/environment.py)
    from behave import fixture
    from somewhere.browser.firefox import FirefoxBrowser

    # -- FIXTURE-VARIANT 1: Use generator-function
    @fixture
    def browser_firefox(context, timeout=30, **kwargs):
        # -- SETUP-FIXTURE PART:
        context.browser = FirefoxBrowser(timeout, **kwargs)
        yield context.browser
        # -- CLEANUP-FIXTURE PART:
        context.browser.shutdown()

.. code-block:: python

    # -- FIXTURE-VARIANT 2: Use normal function
    from somewhere.browser.chrome import ChromeBrowser

    @fixture
    def browser_chrome(context, timeout=30, **kwargs):
        # -- SETUP-FIXTURE PART: And register as context-cleanup task.
        browser = ChromeBrowser(timeout, **kwargs)
        context.browser = browser
        context.add_cleanup(browser.shutdown)
        return browser
        # -- CLEANUP-FIXTURE PART: browser.shutdown()
        # Fixture-cleanup is called when current context-layer is removed.

.. seealso::

    A *fixture* is similar to:

    * a :func:`contextlib.contextmanager`
    * a `pytest.fixture`_
    * the `scope guard`_ idiom

Given:Using a Fixture
---------------

In many cases, the usage of a fixture is triggered by the ``fixture-tag``
in a feature file. The ``fixture-tag`` marks that a fixture
should be used in this scenario/feature (as test-scope).

.. code-block:: gherkin

    # -- FILE: features/use_fixture1.feature
    Feature: Use Fixture on Scenario Level

        @fixture.browser.firefox
        Scenario: Use Web Browser Firefox
            Given I load web page ""https://somewhere.web""
            ...
        # -- AFTER-SCENARIO: Cleanup fixture.browser.firefox

.. code-block:: gherkin

    # -- FILE: features/use_fixture2.feature
    @fixture.browser.firefox
    Feature: Use Fixture on Feature Level

        Scenario: Use Web Browser Firefox
            Given I load web page ""https://somewhere.web""
            ...

        Scenario: Another Browser Test
            ...

    # -- AFTER-FEATURE: Cleanup fixture.browser.firefox


A **fixture** can be used by calling the :func:`~behave.use_fixture()` function.
The :func:`~behave.use_fixture()` call performs the ``SETUP-FIXTURE`` part and returns the
setup result. In addition, it ensures that ``CLEANUP-FIXTURE`` part is called
later-on when the current context-layer is removed.
Therefore, any manual cleanup handling in the ``after_tag()`` hook is not necessary.

.. code-block:: python

    # -- FILE: features/environment.py
    from behave import use_fixture
    from behave4my_project.fixtures import browser_firefox

    def before_tag(context, tag):
        if tag == ""fixture.browser.firefox"":
            use_fixture(browser_firefox, context, timeout=10)



Then:Realistic Example
~~~~~~~~~~~~~~~~~

A more realistic example by using a fixture registry is shown below:

.. code-block:: python

    # -- FILE: features/environment.py
    from behave.fixture import use_fixture_by_tag, fixture_call_params
    from behave4my_project.fixtures import browser_firefox, browser_chrome

    # -- REGISTRY DATA SCHEMA 1: fixture_func
    fixture_registry1 = {
        ""fixture.browser.firefox"": browser_firefox,
        ""fixture.browser.chrome"":  browser_chrome,
    }
    # -- REGISTRY DATA SCHEMA 2: (fixture_func, fixture_args, fixture_kwargs)
    fixture_registry2 = {
        ""fixture.browser.firefox"": fixture_call_params(browser_firefox),
        ""fixture.browser.chrome"":  fixture_call_params(browser_chrome, timeout=12),
    }

    def before_tag(context, tag):
        if tag.startswith(""fixture.""):
            return use_fixture_by_tag(tag, context, fixture_registry1):
        # -- MORE: Tag processing steps ...


.. code-block:: python

    # -- FILE: behave/fixture.py
    # ...
    def use_fixture_by_tag(tag, context, fixture_registry):
        fixture_data = fixture_registry.get(tag, None)
        if fixture_data is None:
            raise LookupError(""Unknown fixture-tag: %s"" % tag)

        # -- FOR DATA SCHEMA 1:
        fixture_func = fixture_data
        return use_fixture(fixture_func, context)

        # -- FOR DATA SCHEMA 2:
        fixture_func, fixture_args, fixture_kwargs = fixture_data
        return use_fixture(fixture_func, context, *fixture_args, **fixture_kwargs)



.. hint:: **Naming Convention for Fixture Tags**

    Fixture tags should start with ``""@fixture.*""`` prefix to improve readability
    and understandibilty in feature files (Gherkin).

    Tags are used for different purposes. Therefore, it should be clear
    when a ``fixture-tag`` is used.



Fixture Cleanup Points
------------------------------------------------------------------------------

The point when a fixture-cleanup is performed depends on the scope where
:func:`~behave.use_fixture()` is called (and the fixture-setup is performed).


============= =========================== ==========================================================================================
Context Layer Fixture-Setup Point         Fixture-Cleanup Point
============= =========================== ==========================================================================================
test run      In ``before_all()`` hook    After ``after_all()``       at end of test-run.
feature       In ``before_feature()``     After ``after_feature()``,  at end of feature.
feature       In ``before_tag()``         After ``after_feature()``   for feature tag.
scenario      In ``before_scenario()``    After ``after_scenario()``, at end of scenario.
scenario      In ``before_tag()``         After ``after_scenario()``  for scenario tag.
scenario      In a step                   After ``after_scenario()``. Fixture is usable until end of scenario.
============= =========================== ==========================================================================================


Fixture Setup/Cleanup Semantics
------------------------------------------------------------------------------

If an error occurs during fixture-setup (meaning an exception is raised):

* Feature/scenario execution is aborted
* Any remaining fixture-setups are skipped
* After feature/scenario hooks are processed
* All fixture-cleanups and context cleanups are performed
* The feature/scenario is marked as failed

If an error occurs during fixture-cleanup (meaning an exception is raised):

* All remaining fixture-cleanups and context cleanups are performed
* First cleanup-error is reraised to pass failure to user (test runner)
* The feature/scenario is marked as failed



Ensure Fixture Cleanups with Fixture Setup Errors
------------------------------------------------------------------------------

Fixture-setup errors are special because a cleanup of a fixture is in many
cases not necessary (or rather difficult because the fixture object
is only partly created, etc.). Therefore, if an error occurs during fixture-setup
(meaning: an exception is raised), the fixture-cleanup part is normally not called.

If you need to ensure that the fixture-cleanup is performed, you need to
provide a slightly different fixture implementation:

.. code-block:: python

    # -- FILE: behave4my_project/fixtures.py  (or: features/environment.py)
    from behave import fixture
    from somewhere.browser.firefox import FirefoxBrowser

    def setup_fixture_part2_with_error(arg):
        raise RuntimeError(""OOPS-FIXTURE-SETUP-ERROR-HERE)

    # -- FIXTURE-VARIANT 1: Use generator-function with try/finally.
    @fixture
    def browser_firefox(context, timeout=30, **kwargs):
        try:
            browser = FirefoxBrowser(timeout, **kwargs)
            browser.part2 = setup_fixture_part2_with_error(""OOPS"")
            context.browser = browser   # NOT_REACHED
            yield browser
            # -- NORMAL FIXTURE-CLEANUP PART: NOT_REACHED due to setup-error.
         finally:
            browser.shutdown()  # -- CLEANUP: When generator-function is left.

.. code-block:: python

    # -- FIXTURE-VARIANT 2: Use normal function and register cleanup-task early.
    from somewhere.browser.chrome import ChromeBrowser

    @fixture
    def browser_chrome(context, timeout=30, **kwargs):
        browser = ChromeBrowser(timeout, **kwargs)
        context.browser = browser
        context.add_cleanup(browser.shutdown)   # -- ENSURE-CLEANUP EARLY
        browser.part2 = setup_fixture_part2_with_error(""OOPS"")
        return browser  # NOT_REACHED
        # -- CLEANUP: browser.shutdown() when context-layer is removed.

.. note::

    An fixture-setup-error that occurs when the browser object is created,
    is not covered by these solutions and not so easy to solve.



Composite Fixtures
------------------------------------------------------------------------------

The last section already describes some problems when you use
complex or *composite fixtures*. It must be ensured that cleanup of already
created fixture parts is performed even when errors occur late in the creation
of a *composite fixture*. This is basically a `scope guard`_ problem.

Solution 1:
~~~~~~~~~~~

.. code-block:: python

    # -- FILE: behave4my_project/fixtures.py
    # SOLUTION 1: Use ""use_fixture()"" to ensure cleanup even in case of errors.
    from behave import fixture, use_fixture

    @fixture
    def foo(context, *args, **kwargs):
        pass    # -- FIXTURE IMPLEMENTATION: Not of interest here.

    @fixture
    def bar(context, *args, **kwargs):
        pass    # -- FIXTURE IMPLEMENTATION: Not of interest here.

    # -- SOLUTION: With use_fixture()
    # ENSURES: foo-fixture is cleaned up even when setup-error occurs later.
    @fixture
    def composite1(context, *args, **kwargs):
        the_fixture1 = use_fixture(foo, context)
        the_fixture2 = use_fixture(bar, context)
        return [the_fixture1, the_fixture2]


Solution 2:
~~~~~~~~~~~

.. code-block:: python

    # -- ALTERNATIVE SOLUTION: With use_composite_fixture_with()
    from behave import fixture
    from behave.fixture import use_composite_fixture_with, fixture_call_params

    @fixture
    def composite2(context, *args, **kwargs):
        the_composite = use_composite_fixture_with(context, [
            fixture_call_params(foo, name=""foo""),
            fixture_call_params(bar, name=""bar""),
        ])
        return the_composite

}





",/content/Feature14.feature," Scenario: Multiple fixture cleanup-errors cause no abort after first error (case: scenario)
    Given a file named ""features/environment.py"" with:
      """"""
      from __future__ import print_function
      from behave.fixture import fixture, use_fixture

      @fixture
      def foo(context):
          print(""FIXTURE-SETUP: foo"")
          yield
          print(""FIXTURE-CLEANUP: foo"")

      @fixture
      def bad_with_cleanup_error(context, name):
          print(""FIXTURE-SETUP: bad_with_cleanup_error%s"" % name)
          yield
          print(""FIXTURE-CLEANUP: bad_with_cleanup_error%s"" % name)
          raise RuntimeError(""BAD_FIXTURE_CLEANUP_ERROR%s"" % name)

      def before_tag(context, tag):
          if tag.startswith(""fixture.bad_with_cleanup_error""):
              name = tag.replace(""fixture.bad_with_cleanup_error"", """")
              use_fixture(bad_with_cleanup_error, context, name)
          elif tag.startswith(""fixture.foo""):
              use_fixture(foo, context)
      """"""
    And a file named ""features/bad_fixture2.feature"" with:
      """"""
      Feature: Fixture with cleanup error
        @fixture.bad_with_cleanup_error_1
        @fixture.foo
        @fixture.bad_with_cleanup_error_2
        Scenario: Use fixture
          Given a step passes

        Scenario:
          Then another step passes
      """"""
    When I run ""behave -f plain features/bad_fixture2.feature""
    Then it should fail with:
      """"""
      1 scenario passed, 1 failed, 0 skipped
      2 steps passed, 0 failed, 0 skipped, 0 undefined
      """"""
    And the command output should contain:
      """"""
      FIXTURE-SETUP: bad_with_cleanup_error_1
      FIXTURE-SETUP: foo
      FIXTURE-SETUP: bad_with_cleanup_error_2
        Scenario: Use fixture
          Given a step passes ... passed
      FIXTURE-CLEANUP: bad_with_cleanup_error_2
      CLEANUP-ERROR in cleanup_fixture: RuntimeError: BAD_FIXTURE_CLEANUP_ERROR_2
      """"""
    And the command output should contain:
      """"""
      FIXTURE-CLEANUP: foo
      FIXTURE-CLEANUP: bad_with_cleanup_error_1
      CLEANUP-ERROR in cleanup_fixture: RuntimeError: BAD_FIXTURE_CLEANUP_ERROR_1
      """"""
    But note that ""all fixture-cleanups are executed (even when errors occur)""
    And note that ""fixture-cleanups are executed in reverse order (LIFO)"""
/content/Record15.rst,"{
Scenario:EXAMPLE: Use Environment Variables in Steps
=============================================================================

:RELATED TO: `issue #497`_

This directory provides a simple example how you can use environment variables
in step implementations.

::


Given: # -- USE: -f plain --no-capture  (via ""behave.ini"" defaults)
    $ behave
    Feature: Test Environment variable concept

      Scenario: 
    USE ENVIRONMENT-VAR: LOGNAME = xxx  (variant 1)
        When I click on $LOGNAME ... passed
    USE ENVIRONMENT-VAR: LOGNAME = xxx  (variant 2)



Then:   When I use the environment variable $LOGNAME ... passed

    1 feature passed, 0 failed, 0 skipped
    1 scenario passed, 0 failed, 0 skipped
    2 steps passed, 0 failed, 0 skipped, 0 undefined
    Took 0m0.000s

.. _`issue #497`: https://github.com/behave/behave/issues/497


}
",/content/Feature15.feature,"Feature: Test Environment variable concept

    Scenario:
        When I click on $LOGNAME
        When I use the environment variable $LOGNAME

"
/content/Record16.rst,"{
Scenario:EXAMPLE: Disable Background Inheritance Mechanism for Scenario
===============================================================================

:RELATED-TO: #756

This example shows how the Background inheritance mechanism in Gherkin
can be disabled in ``behave``.

Parts of the recipe:

* features/example.feature (Feature file as example)
* features/environment.py (glue code and hooks for fixture-tag / fixture)
* behave_fixture_lib/no_background.py (fixture implementation, workhorse)


.. warning:: BEWARE: This shows you how can do it, not that you should do it

    BETTER:

    * Use Rules to group Scenarios, each with its own Background (in Gherkin v6)
    * Split Feature aspects into multiple feature files (if needed)
    * ... (see issue #756 above)

Given:Explanation
------------------------------------------------------------------------

Example code how to provide a behave fixture to disable the
background inheritance mechanism by using a fixture / fixture-tag.
The fixture-tag ""@fixture.behave.no_background"" marks the
location in Gherkin (which Scenario) where the fixture should be used

.. code-block:: gherkin

    # -- FILE: features/example.feature
    Feature: Show how @fixture.behave.no_background is used

        Background:
          Given a background step

        Scenario: Alice
          When a step passes
          And note that ""Background steps are executed here""

        @fixture.behave.no_background
        Scenario: Bob
          Given I need another scenario setup
          When another step passes
          And note that ""NO-BACKGROUND STEPS are executed here""

When the feature is executed, you see that:

* First Scenario ""Alice"": Background steps are inherited and executed first.
* Second Scenario ""Bob"": No Background step is executed.

.. code-block:: sh

    $ ../../bin/behave -f plain features/example.feature
    Feature: Override the Background Inheritance Mechanism in some Scenarios
      Background:

      Scenario: Alice
        Given a background step passes ... passed
        When a step passes ... passed
        And note that ""Background steps are executed here"" ... passed
    FIXTURE-HINT: DISABLE-BACKGROUND FOR: Bob

      Scenario: Bob
        Given I need another scenario setup ... passed
        When another step passes ... passed
        And note that ""NO-BACKGROUND STEPS are executed here"" ... passed

    1 feature passed, 0 failed, 0 skipped
    2 scenarios passed, 0 failed, 0 skipped
    6 steps passed, 0 failed, 0 skipped, 0 undefined



Then:The environment file provides the glue code that the fixture is called:

.. code-block:: python

    # -- FILE: features/environment.py
    from behave_fixture_lib.no_background import behave_no_background
    from behave.fixture import use_fixture_by_tag

    # -- FIXTURE REGISTRY:
    fixture_registry = {
        ""fixture.behave.no_background"": behave_no_background,
    }

    # -----------------------------------------------------------------------------
    # HOOKS:
    # -----------------------------------------------------------------------------
    def before_tag(context, tag):
        if tag.startswith(""fixture.""):
            return use_fixture_by_tag(tag, context, fixture_registry)


.. code-block:: python

    # -- FILE: behave_fixture_lib/no_background.py (fixture implementation)
    from behave import fixture

    @fixture(name=""fixture.behave.no_background"")
    def behave_no_background(ctx):
        # -- SETUP-PART-ONLY: Disable background inheritance (for scenarios only).
        current_scenario = ctx.scenario
        if current_scenario:
            print(""FIXTURE-HINT: DISABLE-BACKGROUND FOR: %s"" % current_scenario.name)
            current_scenario.use_background = False
}
",/content/Feature16.feature,"Scenario: Alice
    When a step passes
    And note that ""BACKGROUND STEPS are executed here"""
/content/Record17.rst,"{
Scenario:EXAMPLE: Disable Background Inheritance Mechanism for Scenario
===============================================================================

:RELATED-TO: #756

This example shows how the Background inheritance mechanism in Gherkin
can be disabled in ``behave``.

Parts of the recipe:

* features/example.feature (Feature file as example)
* features/environment.py (glue code and hooks for fixture-tag / fixture)
* behave_fixture_lib/no_background.py (fixture implementation, workhorse)


.. warning:: BEWARE: This shows you how can do it, not that you should do it

    BETTER:

    * Use Rules to group Scenarios, each with its own Background (in Gherkin v6)
    * Split Feature aspects into multiple feature files (if needed)
    * ... (see issue #756 above)

Given:Explanation
------------------------------------------------------------------------

Example code how to provide a behave fixture to disable the
background inheritance mechanism by using a fixture / fixture-tag.
The fixture-tag ""@fixture.behave.no_background"" marks the
location in Gherkin (which Scenario) where the fixture should be used

.. code-block:: gherkin

    # -- FILE: features/example.feature
    Feature: Show how @fixture.behave.no_background is used

        Background:
          Given a background step

        Scenario: Alice
          When a step passes
          And note that ""Background steps are executed here""

        @fixture.behave.no_background
        Scenario: Bob
          Given I need another scenario setup
          When another step passes
          And note that ""NO-BACKGROUND STEPS are executed here""

When the feature is executed, you see that:

* First Scenario ""Alice"": Background steps are inherited and executed first.
* Second Scenario ""Bob"": No Background step is executed.

.. code-block:: sh

    $ ../../bin/behave -f plain features/example.feature
    Feature: Override the Background Inheritance Mechanism in some Scenarios
      Background:

      Scenario: Alice
        Given a background step passes ... passed
        When a step passes ... passed
        And note that ""Background steps are executed here"" ... passed
    FIXTURE-HINT: DISABLE-BACKGROUND FOR: Bob

      Scenario: Bob
        Given I need another scenario setup ... passed
        When another step passes ... passed
        And note that ""NO-BACKGROUND STEPS are executed here"" ... passed

    1 feature passed, 0 failed, 0 skipped
    2 scenarios passed, 0 failed, 0 skipped
    6 steps passed, 0 failed, 0 skipped, 0 undefined



Then:The environment file provides the glue code that the fixture is called:

.. code-block:: python

    # -- FILE: features/environment.py
    from behave_fixture_lib.no_background import behave_no_background
    from behave.fixture import use_fixture_by_tag

    # -- FIXTURE REGISTRY:
    fixture_registry = {
        ""fixture.behave.no_background"": behave_no_background,
    }

    # -----------------------------------------------------------------------------
    # HOOKS:
    # -----------------------------------------------------------------------------
    def before_tag(context, tag):
        if tag.startswith(""fixture.""):
            return use_fixture_by_tag(tag, context, fixture_registry)


.. code-block:: python

    # -- FILE: behave_fixture_lib/no_background.py (fixture implementation)
    from behave import fixture

    @fixture(name=""fixture.behave.no_background"")
    def behave_no_background(ctx):
        # -- SETUP-PART-ONLY: Disable background inheritance (for scenarios only).
        current_scenario = ctx.scenario
        if current_scenario:
            print(""FIXTURE-HINT: DISABLE-BACKGROUND FOR: %s"" % current_scenario.name)
            current_scenario.use_background = False
}
",/content/Feature17.feature,"Scenario: Bob
      Given I need another scenario setup
      When another step passes
      And note that ""NO-BACKGROUND STEPS are executed here"""
/content/Record18.rst,"{
Scenario:EXAMPLE: Use Soft Assertions in behave
=============================================================================

:RELATED TO: `discussion #1094`_

This directory provides a simple example how soft-assertions can be used
in ``behave`` by using the ``assertpy`` package.


HINT:

* Python2.7: ""@soft_assertions()"" decorator does not seem to work.
  Use ContextManager solution instead, like: ``with soft_assertions(): ...``



Given:Bootstrap
-----------------------------------------------------------------------------

ASSUMPTIONS:

* Python3 is installed (or: Python2.7)
* virtualenv is installed (otherwise use: pip install virtualenv)

Create a virtual-environment with ""virtualenv"" and activate it::


    $ python3 -mvirtualenv .venv

    # -- STEP 2: Activate the virtualenv
    # CASE 1: BASH-LIKE SHELL (on UNIX-like platform: Linux, macOS, WSL, ...)
    $ source .venv/bin/activate

    # CASE 2: CMD SHELL (on Windows)
    cmd> .venv/Scripts/activate

Install the required Python packages in the virtualenv::

    $ pip install -r py.requirements.txt


Then:Run the Example
-----------------------------------------------------------------------------

::

    # -- USE: -f plain --no-capture  (via ""behave.ini"" defaults)
    $ ../../bin/behave -f pretty features
    Feature: Use Soft Assertions in behave # features/soft_asserts.feature:1
      RELATED TO: https://github.com/behave/behave/discussions/1094
      Scenario: Failing with Soft Assertions -- CASE 1             # features/soft_asserts.feature:5
        Given a minimum number value of ""5""                        # features/steps/number_steps.py:16
        Then the numbers ""2"" and ""12"" are in the valid range       # features/steps/number_steps.py:27
          Assertion Failed: soft assertion failures:
          1. Expected <2> to be greater than or equal to <5>, but was not.

        But note that ""the step-2 (then step) is expected to fail"" # None

      @behave.continue_after_failed_step
      Scenario: Failing with Soft Assertions -- CASE 2             # features/soft_asserts.feature:17
        Given a minimum number value of ""5""                        # features/steps/number_steps.py:16
        Then the number ""4"" is in the valid range                  # features/steps/number_steps.py:21
          Assertion Failed: Expected <4> to be greater than or equal to <5>, but was not.

        And the number ""8"" is in the valid range                   # features/steps/number_steps.py:21
        But note that ""the step-2 and step-3 are expected to fail"" # ../../behave4cmd0/note_steps.py:15
        But note that ""the step-4 should pass""                     # ../../behave4cmd0/note_steps.py:15

      @behave.continue_after_failed_step
      Scenario: Failing with Soft Assertions -- CASE 1 and CASE 2  # features/soft_asserts.feature:28
        Given a minimum number value of ""5""                        # features/steps/number_steps.py:16
        Then the number ""2"" is in the valid range                  # features/steps/number_steps.py:21
          Assertion Failed: Expected <2> to be greater than or equal to <5>, but was not.

        And the numbers ""3"" and ""4"" are in the valid range         # features/steps/number_steps.py:27
          Assertion Failed: soft assertion failures:
          1. Expected <3> to be greater than or equal to <5>, but was not.
          2. Expected <4> to be greater than or equal to <5>, but was not.

        And the number ""8"" is in the valid range                   # features/steps/number_steps.py:21
        But note that ""the step-2 and step-3 are expected to fail"" # ../../behave4cmd0/note_steps.py:15
        But note that ""the step-4 should pass""                     # ../../behave4cmd0/note_steps.py:15

      Scenario: Passing                                                  # features/soft_asserts.feature:37
        Given a step passes                                              # ../../behave4cmd0/passing_steps.py:23
        And note that ""this scenario should be executed and should pass"" # ../../behave4cmd0/note_steps.py:15


    Failing scenarios:
      features/soft_asserts.feature:5  Failing with Soft Assertions -- CASE 1
      features/soft_asserts.feature:17  Failing with Soft Assertions -- CASE 2
      features/soft_asserts.feature:28  Failing with Soft Assertions -- CASE 1 and CASE 2

    0 features passed, 1 failed, 0 skipped
    1 scenario passed, 3 failed, 0 skipped
    11 steps passed, 4 failed, 1 skipped, 0 undefined

.. _`discussion #1094`: https://github.com/behave/behave/discussions/1094
}

",/content/Feature18.feature,"Scenario: Failing with Soft Assertions -- CASE 1

    HINT:
    Multiple assert statements in a step are executed even if a assert fails.
    After a failed step in the Scenario,
    the remaining steps are skipped and the next Scenario is executed.

    Given a minimum number value of ""5""
    Then  the numbers ""2"" and ""12"" are in the valid range
    But note that ""the step-2 (then step) is expected to fail"""
/content/Record19.rst,"{
Scenario:EXAMPLE: Use Soft Assertions in behave
=============================================================================

:RELATED TO: `discussion #1094`_

This directory provides a simple example how soft-assertions can be used
in ``behave`` by using the ``assertpy`` package.


HINT:

* Python2.7: ""@soft_assertions()"" decorator does not seem to work.
  Use ContextManager solution instead, like: ``with soft_assertions(): ...``



Given:Bootstrap
-----------------------------------------------------------------------------

ASSUMPTIONS:

* Python3 is installed (or: Python2.7)
* virtualenv is installed (otherwise use: pip install virtualenv)

Create a virtual-environment with ""virtualenv"" and activate it::


    $ python3 -mvirtualenv .venv

    # -- STEP 2: Activate the virtualenv
    # CASE 1: BASH-LIKE SHELL (on UNIX-like platform: Linux, macOS, WSL, ...)
    $ source .venv/bin/activate

    # CASE 2: CMD SHELL (on Windows)
    cmd> .venv/Scripts/activate

Install the required Python packages in the virtualenv::

    $ pip install -r py.requirements.txt


Then:Run the Example
-----------------------------------------------------------------------------

::

    # -- USE: -f plain --no-capture  (via ""behave.ini"" defaults)
    $ ../../bin/behave -f pretty features
    Feature: Use Soft Assertions in behave # features/soft_asserts.feature:1
      RELATED TO: https://github.com/behave/behave/discussions/1094
      Scenario: Failing with Soft Assertions -- CASE 1             # features/soft_asserts.feature:5
        Given a minimum number value of ""5""                        # features/steps/number_steps.py:16
        Then the numbers ""2"" and ""12"" are in the valid range       # features/steps/number_steps.py:27
          Assertion Failed: soft assertion failures:
          1. Expected <2> to be greater than or equal to <5>, but was not.

        But note that ""the step-2 (then step) is expected to fail"" # None

      @behave.continue_after_failed_step
      Scenario: Failing with Soft Assertions -- CASE 2             # features/soft_asserts.feature:17
        Given a minimum number value of ""5""                        # features/steps/number_steps.py:16
        Then the number ""4"" is in the valid range                  # features/steps/number_steps.py:21
          Assertion Failed: Expected <4> to be greater than or equal to <5>, but was not.

        And the number ""8"" is in the valid range                   # features/steps/number_steps.py:21
        But note that ""the step-2 and step-3 are expected to fail"" # ../../behave4cmd0/note_steps.py:15
        But note that ""the step-4 should pass""                     # ../../behave4cmd0/note_steps.py:15

      @behave.continue_after_failed_step
      Scenario: Failing with Soft Assertions -- CASE 1 and CASE 2  # features/soft_asserts.feature:28
        Given a minimum number value of ""5""                        # features/steps/number_steps.py:16
        Then the number ""2"" is in the valid range                  # features/steps/number_steps.py:21
          Assertion Failed: Expected <2> to be greater than or equal to <5>, but was not.

        And the numbers ""3"" and ""4"" are in the valid range         # features/steps/number_steps.py:27
          Assertion Failed: soft assertion failures:
          1. Expected <3> to be greater than or equal to <5>, but was not.
          2. Expected <4> to be greater than or equal to <5>, but was not.

        And the number ""8"" is in the valid range                   # features/steps/number_steps.py:21
        But note that ""the step-2 and step-3 are expected to fail"" # ../../behave4cmd0/note_steps.py:15
        But note that ""the step-4 should pass""                     # ../../behave4cmd0/note_steps.py:15

      Scenario: Passing                                                  # features/soft_asserts.feature:37
        Given a step passes                                              # ../../behave4cmd0/passing_steps.py:23
        And note that ""this scenario should be executed and should pass"" # ../../behave4cmd0/note_steps.py:15


    Failing scenarios:
      features/soft_asserts.feature:5  Failing with Soft Assertions -- CASE 1
      features/soft_asserts.feature:17  Failing with Soft Assertions -- CASE 2
      features/soft_asserts.feature:28  Failing with Soft Assertions -- CASE 1 and CASE 2

    0 features passed, 1 failed, 0 skipped
    1 scenario passed, 3 failed, 0 skipped
    11 steps passed, 4 failed, 1 skipped, 0 undefined

.. _`discussion #1094`: https://github.com/behave/behave/discussions/1094
}

",/content/Feature19.feature,"Scenario: Failing with Soft Assertions -- CASE 2

    HINT: If a step in the Scenario fails, execution is continued.

    Given a minimum number value of ""5""
    Then  the number ""4"" is in the valid range
    And   the number ""8"" is in the valid range
    But note that ""the step-2 is expected to fail""
    But note that ""the step-3 should be executed and should pass"""
/content/Record20.rst,"{
Scenario:EXAMPLE: Use Soft Assertions in behave
=============================================================================

:RELATED TO: `discussion #1094`_

This directory provides a simple example how soft-assertions can be used
in ``behave`` by using the ``assertpy`` package.


HINT:

* Python2.7: ""@soft_assertions()"" decorator does not seem to work.
  Use ContextManager solution instead, like: ``with soft_assertions(): ...``



Given:Bootstrap
-----------------------------------------------------------------------------

ASSUMPTIONS:

* Python3 is installed (or: Python2.7)
* virtualenv is installed (otherwise use: pip install virtualenv)

Create a virtual-environment with ""virtualenv"" and activate it::


    $ python3 -mvirtualenv .venv

    # -- STEP 2: Activate the virtualenv
    # CASE 1: BASH-LIKE SHELL (on UNIX-like platform: Linux, macOS, WSL, ...)
    $ source .venv/bin/activate

    # CASE 2: CMD SHELL (on Windows)
    cmd> .venv/Scripts/activate

Install the required Python packages in the virtualenv::

    $ pip install -r py.requirements.txt


Then:Run the Example
-----------------------------------------------------------------------------

::

    # -- USE: -f plain --no-capture  (via ""behave.ini"" defaults)
    $ ../../bin/behave -f pretty features
    Feature: Use Soft Assertions in behave # features/soft_asserts.feature:1
      RELATED TO: https://github.com/behave/behave/discussions/1094
      Scenario: Failing with Soft Assertions -- CASE 1             # features/soft_asserts.feature:5
        Given a minimum number value of ""5""                        # features/steps/number_steps.py:16
        Then the numbers ""2"" and ""12"" are in the valid range       # features/steps/number_steps.py:27
          Assertion Failed: soft assertion failures:
          1. Expected <2> to be greater than or equal to <5>, but was not.

        But note that ""the step-2 (then step) is expected to fail"" # None

      @behave.continue_after_failed_step
      Scenario: Failing with Soft Assertions -- CASE 2             # features/soft_asserts.feature:17
        Given a minimum number value of ""5""                        # features/steps/number_steps.py:16
        Then the number ""4"" is in the valid range                  # features/steps/number_steps.py:21
          Assertion Failed: Expected <4> to be greater than or equal to <5>, but was not.

        And the number ""8"" is in the valid range                   # features/steps/number_steps.py:21
        But note that ""the step-2 and step-3 are expected to fail"" # ../../behave4cmd0/note_steps.py:15
        But note that ""the step-4 should pass""                     # ../../behave4cmd0/note_steps.py:15

      @behave.continue_after_failed_step
      Scenario: Failing with Soft Assertions -- CASE 1 and CASE 2  # features/soft_asserts.feature:28
        Given a minimum number value of ""5""                        # features/steps/number_steps.py:16
        Then the number ""2"" is in the valid range                  # features/steps/number_steps.py:21
          Assertion Failed: Expected <2> to be greater than or equal to <5>, but was not.

        And the numbers ""3"" and ""4"" are in the valid range         # features/steps/number_steps.py:27
          Assertion Failed: soft assertion failures:
          1. Expected <3> to be greater than or equal to <5>, but was not.
          2. Expected <4> to be greater than or equal to <5>, but was not.

        And the number ""8"" is in the valid range                   # features/steps/number_steps.py:21
        But note that ""the step-2 and step-3 are expected to fail"" # ../../behave4cmd0/note_steps.py:15
        But note that ""the step-4 should pass""                     # ../../behave4cmd0/note_steps.py:15

      Scenario: Passing                                                  # features/soft_asserts.feature:37
        Given a step passes                                              # ../../behave4cmd0/passing_steps.py:23
        And note that ""this scenario should be executed and should pass"" # ../../behave4cmd0/note_steps.py:15


    Failing scenarios:
      features/soft_asserts.feature:5  Failing with Soft Assertions -- CASE 1
      features/soft_asserts.feature:17  Failing with Soft Assertions -- CASE 2
      features/soft_asserts.feature:28  Failing with Soft Assertions -- CASE 1 and CASE 2

    0 features passed, 1 failed, 0 skipped
    1 scenario passed, 3 failed, 0 skipped
    11 steps passed, 4 failed, 1 skipped, 0 undefined

.. _`discussion #1094`: https://github.com/behave/behave/discussions/1094
}

",/content/Feature20.feature," Scenario: Failing with Soft Assertions -- CASE 1 and CASE 2

    Given a minimum number value of ""5""
    Then  the number ""2"" is in the valid range
    And   the numbers ""3"" and ""4"" are in the valid range
    And   the number ""8"" is in the valid range
    But note that ""the step-2 and step-3 are expected to fail""
    But note that ""the step-4 should be executed and should pass"""
/content/Record21.rst,"{
Scenario:EXAMPLE: Use Soft Assertions in behave
=============================================================================

:RELATED TO: `discussion #1094`_

This directory provides a simple example how soft-assertions can be used
in ``behave`` by using the ``assertpy`` package.


HINT:

* Python2.7: ""@soft_assertions()"" decorator does not seem to work.
  Use ContextManager solution instead, like: ``with soft_assertions(): ...``



Given:Bootstrap
-----------------------------------------------------------------------------

ASSUMPTIONS:

* Python3 is installed (or: Python2.7)
* virtualenv is installed (otherwise use: pip install virtualenv)

Create a virtual-environment with ""virtualenv"" and activate it::


    $ python3 -mvirtualenv .venv

    # -- STEP 2: Activate the virtualenv
    # CASE 1: BASH-LIKE SHELL (on UNIX-like platform: Linux, macOS, WSL, ...)
    $ source .venv/bin/activate

    # CASE 2: CMD SHELL (on Windows)
    cmd> .venv/Scripts/activate

Install the required Python packages in the virtualenv::

    $ pip install -r py.requirements.txt


Then:Run the Example
-----------------------------------------------------------------------------

::

    # -- USE: -f plain --no-capture  (via ""behave.ini"" defaults)
    $ ../../bin/behave -f pretty features
    Feature: Use Soft Assertions in behave # features/soft_asserts.feature:1
      RELATED TO: https://github.com/behave/behave/discussions/1094
      Scenario: Failing with Soft Assertions -- CASE 1             # features/soft_asserts.feature:5
        Given a minimum number value of ""5""                        # features/steps/number_steps.py:16
        Then the numbers ""2"" and ""12"" are in the valid range       # features/steps/number_steps.py:27
          Assertion Failed: soft assertion failures:
          1. Expected <2> to be greater than or equal to <5>, but was not.

        But note that ""the step-2 (then step) is expected to fail"" # None

      @behave.continue_after_failed_step
      Scenario: Failing with Soft Assertions -- CASE 2             # features/soft_asserts.feature:17
        Given a minimum number value of ""5""                        # features/steps/number_steps.py:16
        Then the number ""4"" is in the valid range                  # features/steps/number_steps.py:21
          Assertion Failed: Expected <4> to be greater than or equal to <5>, but was not.

        And the number ""8"" is in the valid range                   # features/steps/number_steps.py:21
        But note that ""the step-2 and step-3 are expected to fail"" # ../../behave4cmd0/note_steps.py:15
        But note that ""the step-4 should pass""                     # ../../behave4cmd0/note_steps.py:15

      @behave.continue_after_failed_step
      Scenario: Failing with Soft Assertions -- CASE 1 and CASE 2  # features/soft_asserts.feature:28
        Given a minimum number value of ""5""                        # features/steps/number_steps.py:16
        Then the number ""2"" is in the valid range                  # features/steps/number_steps.py:21
          Assertion Failed: Expected <2> to be greater than or equal to <5>, but was not.

        And the numbers ""3"" and ""4"" are in the valid range         # features/steps/number_steps.py:27
          Assertion Failed: soft assertion failures:
          1. Expected <3> to be greater than or equal to <5>, but was not.
          2. Expected <4> to be greater than or equal to <5>, but was not.

        And the number ""8"" is in the valid range                   # features/steps/number_steps.py:21
        But note that ""the step-2 and step-3 are expected to fail"" # ../../behave4cmd0/note_steps.py:15
        But note that ""the step-4 should pass""                     # ../../behave4cmd0/note_steps.py:15

      Scenario: Passing                                                  # features/soft_asserts.feature:37
        Given a step passes                                              # ../../behave4cmd0/passing_steps.py:23
        And note that ""this scenario should be executed and should pass"" # ../../behave4cmd0/note_steps.py:15


    Failing scenarios:
      features/soft_asserts.feature:5  Failing with Soft Assertions -- CASE 1
      features/soft_asserts.feature:17  Failing with Soft Assertions -- CASE 2
      features/soft_asserts.feature:28  Failing with Soft Assertions -- CASE 1 and CASE 2

    0 features passed, 1 failed, 0 skipped
    1 scenario passed, 3 failed, 0 skipped
    11 steps passed, 4 failed, 1 skipped, 0 undefined

.. _`discussion #1094`: https://github.com/behave/behave/discussions/1094
}

",/content/Feature21.feature,"Scenario: Passing
    Given a step passes
    And note that ""this scenario should be executed and should pass"""
/content/Record22.rst,"
{
Scenario:.. _tutorial:

========
Tutorial
========

First, :doc:`install behave <install>`.

Now make a directory called ""features"". In that directory create a file
called ""tutorial.feature"" containing:

.. code-block:: gherkin

 Feature: showing off behave

   Scenario: run a simple test
      Given we have behave installed
       When we implement a test
       Then behave will test it for us!

Make a new directory called ""features/steps"". In that directory create a
file called ""tutorial.py"" containing:

.. code-block:: python

    from behave import *

    @given('we have behave installed')
    def step_impl(context):
        pass

    @when('we implement a test')
    def step_impl(context):
        assert True is not False

    @then('behave will test it for us!')
    def step_impl(context):
        assert context.failed is False


  Given:Run behave::

    % behave
    Feature: showing off behave # features/tutorial.feature:1

      Scenario: run a simple test        # features/tutorial.feature:3
        Given we have behave installed   # features/steps/tutorial.py:3
        When we implement a test         # features/steps/tutorial.py:7
        Then behave will test it for us! # features/steps/tutorial.py:11

    1 feature passed, 0 failed, 0 skipped
    1 scenario passed, 0 failed, 0 skipped
    3 steps passed, 0 failed, 0 skipped, 0 undefined

Now, continue reading to learn how to make the most of *behave*.


Features
========

*behave* operates on directories containing:

1. `feature files`_ written by your Business Analyst / Sponsor / whoever
   with your behaviour scenarios in it, and
2. a ""steps"" directory with `Python step implementations`_ for the
   scenarios.

You may optionally include some `environmental controls`_ (code to run
before and after steps, scenarios, features or the whole shooting
match).

The minimum requirement for a features directory is::

  features/
  features/everything.feature
  features/steps/
  features/steps/steps.py

A more complex directory might look like::

  features/
  features/signup.feature
  features/login.feature
  features/account_details.feature
  features/environment.py
  features/steps/
  features/steps/website.py
  features/steps/utils.py

If you're having trouble setting things up and want to see what *behave* is
doing in attempting to find your features use the ""-v"" (verbose)
command-line switch.


Feature Files
=============

A feature file has a :ref:`natural language format <chapter.gherkin>`
describing a feature or part of a feature with representative examples of
expected outcomes.
They're plain-text (encoded in UTF-8) and look something like:

.. code-block:: gherkin

  Feature: Fight or flight
    In order to increase the ninja survival rate,
    As a ninja commander
    I want my ninjas to decide whether to take on an
    opponent based on their skill levels

    Scenario: Weaker opponent
      Given the ninja has a third level black-belt
       When attacked by a samurai
       Then the ninja should engage the opponent

    Scenario: Stronger opponent
      Given the ninja has a third level black-belt
       When attacked by Chuck Norris
       Then the ninja should run for his life

The ""Given"", ""When"" and ""Then"" parts of this prose form the actual steps
that will be taken by *behave* in testing your system. These map to `Python
step implementations`_. As a general guide:

**Given** we *put the system in a known state* before the
user (or external system) starts interacting with the system (in the When
steps). Avoid talking about user interaction in givens.

**When** we *take key actions* the user (or external system) performs. This
is the interaction with your system which should (or perhaps should not)
cause some state to change.

**Then** we *observe outcomes*.

You may also include ""And"" or ""But"" as a step - these are renamed by *behave*
to take the name of their preceding step, so:

.. code-block:: gherkin

    Scenario: Stronger opponent
      Given the ninja has a third level black-belt
       When attacked by Chuck Norris
       Then the ninja should run for his life
        And fall off a cliff

In this case *behave* will look for a step definition for
``""Then fall off a cliff""``.

Then:Scenario Outlines
-----------------

Sometimes a scenario should be run with a number of variables giving a set
of known states, actions to take and expected outcomes, all using the same
basic actions. You may use a Scenario Outline to achieve this:

.. code-block:: gherkin

  Scenario Outline: Blenders
     Given I put <thing> in a blender,
      When I switch the blender on
      Then it should transform into <other thing>

   Examples: Amphibians
     | thing         | other thing |
     | Red Tree Frog | mush        |

   Examples: Consumer Electronics
     | thing         | other thing |
     | iPhone        | toxic waste |
     | Galaxy Nexus  | toxic waste |

*behave* will run the scenario once for each (non-heading) line appearing
in the example data tables.


Step Data
---------

Sometimes it's useful to associate a table of data with your step.

Any text block following a step wrapped in ``""""""`` lines will be associated
with the step. For example:

.. code-block:: gherkin

   Scenario: some scenario
     Given a sample text loaded into the frobulator
        """"""
        Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do
        eiusmod tempor incididunt ut labore et dolore magna aliqua.
        """"""
    When we activate the frobulator
    Then we will find it similar to English

The text is available to the Python step code as the "".text"" attribute
in the :class:`~behave.runner.Context` variable passed into each step
function.

You may also associate a table of data with a step by simply entering it,
indented, following the step. This can be useful for loading specific
required data into a model.

.. code-block:: gherkin

   Scenario: some scenario
     Given a set of specific users
        | name      | department  |
        | Barry     | Beer Cans   |
        | Pudey     | Silly Walks |
        | Two-Lumps | Silly Walks |

    When we count the number of people in each department
    Then we will find two people in ""Silly Walks""
     But we will find one person in ""Beer Cans""

The table is available to the Python step code as the "".table"" attribute
in the :class:`~behave.runner.Context` variable passed into each step
function. The table for the example above could be accessed like so:

.. code-block:: python

    @given('a set of specific users')
    def step_impl(context):
        for row in context.table:
            model.add_user(name=row['name'], department=row['department'])

There's a variety of ways to access the table data - see the
:class:`~behave.model.Table` API documentation for the full details.


.. _docid.tutorial.python-step-implementations:

Python Step Implementations
===========================

Steps used in the scenarios are implemented in Python files in the ""steps""
directory. You can call these whatever you like as long as they use
the python ``*.py`` file extension. You don't need to tell *behave* which
ones to use - it'll use all of them.

The full detail of the Python side of *behave* is in the
:doc:`API documentation <api>`.

Steps are identified using decorators which match the predicate from the
feature file: **given**, **when**, **then** and **step** (variants with Title case are also
available if that's your preference.) The decorator accepts a string
containing the rest of the phrase used in the scenario step it belongs to.

Given a Scenario:

.. code-block:: gherkin

  Scenario: Search for an account
     Given I search for a valid account
      Then I will see the account details

Step code implementing the two steps here might look like
(using selenium webdriver and some other helpers):

.. code-block:: python

    @given('I search for a valid account')
    def step_impl(context):
        context.browser.get('http://localhost:8000/index')
        form = get_element(context.browser, tag='form')
        get_element(form, name=""msisdn"").send_keys('61415551234')
        form.submit()

    @then('I will see the account details')
    def step_impl(context):
        elements = find_elements(context.browser, id='no-account')
        eq_(elements, [], 'account not found')
        h = get_element(context.browser, id='account-head')
        ok_(h.text.startswith(""Account 61415551234""),
            'Heading %r has wrong text' % h.text)

The ``step`` decorator matches the step to *any* step type, ""given"", ""when""
or ""then"". The ""and"" and ""but"" step types are renamed internally to take
the preceding step's keyword (so an ""and"" following a ""given"" will become a
""given"" internally and use a **given** decorated step).

.. note::

      Step function names do not need to have a unique symbol name, because the
      text matching selects the step function from the step registry before it is
      called as anonymous function.  Hence, when *behave* prints out the missing
      step implementations in a test run, it uses ""step_impl"" for all functions
      by default.

If you find you'd like your step implementation to invoke another step you
may do so with the :class:`~behave.runner.Context` method
:func:`~behave.runner.Context.execute_steps`.

This function allows you to, for example:

.. code-block:: python

    @when('I do the same thing as before')
    def step_impl(context):
        context.execute_steps('''
            when I press the big red button
             and I duck
        ''')

This will cause the ""when I do the same thing as before"" step to execute
the other two steps as though they had also appeared in the scenario file.


.. _docid.tutorial.step-parameters:
.. _`step parameters`:

Step Parameters
---------------

Steps sometimes include very common phrases with only one variation
(one word is different or some words are different).
For example:

.. code-block:: gherkin

    # -- FILE: features/example_step_parameters.feature
    Scenario: look up a book
      Given I search for a valid book
       Then the result page will include ""success""

    Scenario: look up an invalid book
      Given I search for a invalid book
       Then the result page will include ""failure""

You can define one Python step-definition that handles both cases by using `step parameters`_ .
In this case, the *Then* step verifies the ``context.response`` parameter
that was stored in the ``context`` by the *Given* step:

.. code-block:: python

    # -- FILE: features/steps/example_steps_with_step_parameters.py
    # HINT: Step-matcher ""parse"" is the DEFAULT step-matcher class.
    from behave import then

    @then('the result page will include ""{text}""')
    def step_impl(context, text):
        if text not in context.response:
            fail('%r not in %r' % (text, context.response))

There are several step-matcher classes available in **behave**
that can be used for `step parameters`_.
You can select another step-matcher class by using
the :func:`behave.use_step_matcher()` function:

.. code-block:: python

    # -- FILE: features/steps/example_use_step_matcher_in_steps.py
    # HINTS:
    #   * ""parse"" in the DEFAULT step-matcher
    #   * Use ""use_step_matcher(...)"" in ""features/environment.py"" file
    #     to define your own own default step-matcher.
    from behave import given, when, use_step_matcher

    use_step_matcher(""cfparse"")

    @given('some event named ""{event_name}"" happens')
    def step_given_some_event_named_happens(context, event_name):
        pass    # ... DETAILS LEFT OUT HERE.

    use_step_matcher(""re"")

    @when('a person named ""(?P<name>...)"" enters the room')
    def step_when_person_enters_room(context, name):
        pass    # ... DETAILS LEFT OUT HERE.


Step-matchers
--------------

There are several step-matcher classes available in **behave**
that can be used for parsing `step parameters`_:

* **parse** (default step-matcher class, based on: :pypi:`parse`):
* **cfparse** (extends: :pypi:`parse`, requires: :pypi:`parse_type`):
* **re** (step-matcher class is based on regular expressions):


Step-matcher: parse
~~~~~~~~~~~~~~~~~~~

This step-matcher class provides a parser based on: :pypi:`parse` module.

It provides a simple parser that replaces regular expressions
for step parameters with a readable syntax like ``{param:Type}``.

The syntax is inspired by the Python builtin ``string.format()`` function.
Step parameters must use the named fields syntax of :pypi:`parse`
in step definitions. The named fields are extracted,
optionally type converted and then used as step function arguments.

FEATURES:

* Supports named step parameters (and unnamed step parameters)
* Supports **type conversions** by using type converters
  (see :func:`~behave.register_type()`).


Step-matcher: cfparse
~~~~~~~~~~~~~~~~~~~~~

This step-matcher class extends the ``parse`` step-matcher
and provides an extended parser with ""Cardinality Field"" (CF) support.

It automatically creates missing type converters for other cardinalities
as long as a type converter for cardinality=1 is provided.

It supports parse expressions like:

* ``{values:Type+}`` (cardinality=1..N, many)
* ``{values:Type*}`` (cardinality=0..N, many0)
* ``{value:Type?}``  (cardinality=0..1, optional).

FEATURES:

* Supports named step parameters (and unnamed step parameters)
* Supports **type conversions** by using type converters
  (see :func:`~behave.register_type()`).



Step-matcher: re
~~~~~~~~~~~~~~~~~~~~~

This step-matcher provides step-matcher class is based on regular expressions.
It uses full regular expressions to parse the clause text.
You will need to use named groups ""(?P<name>...)"" to define the variables pulled
from the text and passed to your ``step()`` function.

.. hint:: Type conversion is **not supported**.

    A step function writer may implement type conversion
    inside the step function (implementation).


To specify which parser to use,
call the :func:`~behave.use_step_matcher()` function with the name
of the step-matcher class to use.

You can change the step-matcher class at any time to suit your needs.
The following step-definitions use the current step-matcher class.

FEATURES:

* Supports named step parameters (and unnamed step parameters)
* Supports no type conversions

VARIANTS:

* ``""re0""``: Provides a regex matcher that is compatible with ``cucumber``
  (regex based step-matcher).


Context
-------

You'll have noticed the ""context"" variable that's passed around. It's a
clever place where you and *behave* can store information to share around.
It runs at three levels, automatically managed by *behave*.

When *behave* launches into a new feature or scenario it adds a new layer
to the context, allowing the new activity level to add new values, or
overwrite ones previously defined, for the duration of that activity. These
can be thought of as scopes.

You can define values in your `environmental controls`_ file which may be
set at the feature level and then overridden for some scenarios. Changes
made at the scenario level won't permanently affect the value set at the
feature level.

You may also use it to share values between steps. For example, in some
steps you define you might have:

.. code-block:: python

    @given('I request a new widget for an account via SOAP')
    def step_impl(context):
        client = Client(""http://127.0.0.1:8000/soap/"")
        context.response = client.Allocate(customer_first='Firstname',
            customer_last='Lastname', colour='red')

    @then('I should receive an OK SOAP response')
    def step_impl(context):
        eq_(context.response['ok'], 1)

There's also some values added to the context by *behave* itself:

**table**
  This holds any table data associated with a step.

**text**
  This holds any multi-line text associated with a step.

**failed**
  This is set at the root of the context when any step fails. It is
  sometimes useful to use this combined with the ``--stop`` command-line
  option to prevent some mis-behaving resource from being cleaned up in an
  ``after_feature()`` or similar (for example, a web browser being driven
  by Selenium.)

The *context* variable in all cases is an instance of
:class:`behave.runner.Context`.


.. _docid.tutorial.environmental-controls:

Environmental Controls
======================

The environment.py module may define code to run before and after certain
events during your testing:

**before_step(context, step), after_step(context, step)**
  These run before and after every step.
**before_scenario(context, scenario), after_scenario(context, scenario)**
  These run before and after each scenario is run.
**before_feature(context, feature), after_feature(context, feature)**
  These run before and after each feature file is exercised.
**before_tag(context, tag), after_tag(context, tag)**
  These run before and after a section tagged with the given name. They are
  invoked for each tag encountered in the order they're found in the
  feature file. See  `controlling things with tags`_.
**before_all(context), after_all(context)**
  These run before and after the whole shooting match.

The feature, scenario and step objects represent the information parsed
from the feature file. They have a number of attributes:

**keyword**
  ""Feature"", ""Scenario"", ""Given"", etc.
**name**
  The name of the step (the text after the keyword.)
**tags**
  A list of the tags attached to the section or step.
  See `controlling things with tags`_.
**filename** and **line**
  The file name (or ""<string>"") and line number of the statement.

A common use-case for environmental controls might be to set up a web
server and browser to run all your tests in. For example:

.. code-block:: python

    # -- FILE: features/environment.py
    from behave import fixture, use_fixture
    from behave4my_project.fixtures import wsgi_server
    from selenium import webdriver

    @fixture
    def selenium_browser_chrome(context):
        # -- HINT: @behave.fixture is similar to @contextlib.contextmanager
        context.browser = webdriver.Chrome()
        yield context.browser
        # -- CLEANUP-FIXTURE PART:
        context.browser.quit()

    def before_all(context):
        use_fixture(wsgi_server, context, port=8000)
        use_fixture(selenium_browser_chrome, context)
        # -- HINT: CLEANUP-FIXTURE is performed after after_all() hook is called.

    def before_feature(context, feature):
        model.init(environment='test')


.. code-block:: python

    # -- FILE: behave4my_project/fixtures.py
    # ALTERNATIVE: Place fixture in ""features/environment.py"" (but reuse is harder)
    from behave import fixture
    import threading
    from wsgiref import simple_server
    from my_application import model
    from my_application import web_app

    @fixture
    def wsgi_server(context, port=8000):
        context.server = simple_server.WSGIServer(('', port))
        context.server.set_app(web_app.main(environment='test'))
        context.thread = threading.Thread(target=context.server.serve_forever)
        context.thread.start()
        yield context.server
        # -- CLEANUP-FIXTURE PART:
        context.server.shutdown()
        context.thread.join()


Of course, if you wish, you could have a new browser for each feature, or to
retain the database state between features or even initialise the database
for each scenario.


.. _`controlling things with tags`:

Controlling Things With Tags
============================

You may also ""tag"" parts of your feature file. At the simplest level this
allows *behave* to selectively check parts of your feature set.

Given a feature file with:

.. code-block:: gherkin

  Feature: Fight or flight
    In order to increase the ninja survival rate,
    As a ninja commander
    I want my ninjas to decide whether to take on an
    opponent based on their skill levels

    @slow
    Scenario: Weaker opponent
      Given the ninja has a third level black-belt
      When attacked by a samurai
      Then the ninja should engage the opponent

    Scenario: Stronger opponent
      Given the ninja has a third level black-belt
      When attacked by Chuck Norris
      Then the ninja should run for his life

then running ``behave --tags=slow`` will run just the scenarios tagged
``@slow``. If you wish to check everything *except* the slow ones then you
may run ``behave --tags=""not @slow""``.

Another common use-case is to tag a scenario you're working on with
``@wip`` and then ``behave --tags=wip`` to just test that one case.

Tag selection on the command-line may be combined:

* ``--tags=""@wip or @slow""``
   This will select all the cases tagged *either* ""wip"" or ""slow"".

* ``--tags=""@wip and @slow""``
   This will select all the cases tagged *both* ""wip"" and ""slow"".

If a feature or scenario is tagged and then skipped because of a
command-line control then the *before_* and *after_* environment functions
will not be called for that feature or scenario. Note that *behave* has
additional support specifically for testing `works in progress`_.

The tags attached to a feature and scenario are available in
the environment functions via the ""feature"" or ""scenario"" object passed to
them. On those objects there is an attribute called ""tags"" which is a list
of the tag names attached, in the order they're found in the features file.

There are also `environmental controls`_ specific to tags, so in the above
example *behave* will attempt to invoke an ``environment.py`` function
``before_tag`` and ``after_tag`` before and after the Scenario tagged
``@slow``, passing in the name ""slow"". If multiple tags are present then
the functions will be called multiple times with each tag in the order
they're defined in the feature file.

Re-visiting the example from above; if only some of the features required a
browser and web server then you could tag them ``@fixture.browser``:

.. code-block:: python

    # -- FILE: features/environment.py
    # HINT: Reusing some code parts from above.
    ...

    def before_feature(context, feature):
        model.init(environment='test')
        if ""fixture.browser"" in feature.tags:
            use_fixture(wsgi_server, context)
            use_fixture(selenium_browser_chrome, context)


Works In Progress
=================

*behave* supports the concept of a highly-unstable ""work in progress""
scenario that you're actively developing. This scenario may produce strange
logging, or odd output to stdout or just plain interact in unexpected ways
with *behave*'s scenario runner.

To make testing such scenarios simpler we've implemented a ""-w""
command-line flag. This flag:

1. turns off stdout capture
2. turns off logging capture; you will still need to configure your own
   logging handlers - we recommend a ``before_all()`` with:

   .. code-block:: python

    if not context.config.log_capture:
        logging.basicConfig(level=logging.DEBUG)

3. turns off pretty output - no ANSI escape sequences to confuse your
   scenario's output
4. only runs scenarios tagged with ""@wip""
5. stops at the first error


Fixtures
===================================

Fixtures simplify the setup/cleanup tasks that are often needed during test execution.

.. code-block:: python

    # -- FILE: behave4my_project/fixtures.py  (or in: features/environment.py)
    from behave import fixture
    from somewhere.browser.firefox import FirefoxBrowser

    # -- FIXTURE: Use generator-function
    @fixture
    def browser_firefox(context, timeout=30, **kwargs):
        # -- SETUP-FIXTURE PART:
        context.browser = FirefoxBrowser(timeout, **kwargs)
        yield context.browser
        # -- CLEANUP-FIXTURE PART:
        context.browser.shutdown()

See :ref:`docid.fixtures` for more information.


.. index::
    single: debug-on-error

.. _debug-on-error:

Debug-on-Error (in Case of Step Failures)
=========================================

A ""debug on error/failure"" functionality can easily be provided,
by using the ``after_step()`` hook.
The debugger is started when a step fails.

It is in general a good idea to enable this functionality only when needed
(in interactive mode). The functionality is enabled (in this example)
by using the user-specific configuration data. A user can:

  * provide a userdata define on command-line
  * store a value in the ""behave.userdata"" section of behave's configuration file

.. code-block:: python

    # -- FILE: features/environment.py
    # USE: behave -D BEHAVE_DEBUG_ON_ERROR         (to enable  debug-on-error)
    # USE: behave -D BEHAVE_DEBUG_ON_ERROR=yes     (to enable  debug-on-error)
    # USE: behave -D BEHAVE_DEBUG_ON_ERROR=no      (to disable debug-on-error)

    BEHAVE_DEBUG_ON_ERROR = False

    def setup_debug_on_error(userdata):
        global BEHAVE_DEBUG_ON_ERROR
        BEHAVE_DEBUG_ON_ERROR = userdata.getbool(""BEHAVE_DEBUG_ON_ERROR"")

    def before_all(context):
        setup_debug_on_error(context.config.userdata)

    def after_step(context, step):
        if BEHAVE_DEBUG_ON_ERROR and step.status == ""failed"":
            # -- ENTER DEBUGGER: Zoom in on failure location.
            # NOTE: Use IPython debugger, same for pdb (basic python debugger).
            import ipdb
            ipdb.post_mortem(step.exc_traceback)


}


",/content/Feature22.feature,"Scenario: run a simple test
    Given we have behave installed
    When we implement a test
    Then behave will test it for us!
"
/content/Record23.rst,"{
Scenario:.. _id.appendix.formatters:

========================
Formatters and Reporters
========================

:pypi:`behave` provides 2 different concepts for reporting results of a test run:

* formatters
* reporters

A slightly different interface is provided for each ""formatter"" concept.
The ``Formatter`` is informed about each step that is taken.
The ``Reporter`` has a more coarse-grained API.


Reporters
---------

The following reporters are currently supported:

============== ================================================================
Name            Description
============== ================================================================
junit           Provides JUnit XML-like output.
summary         Provides a summary of the test run.
============== ================================================================


Formatters
----------

The following formatters are currently supported:

============== ======== ================================================================
Name           Mode     Description
============== ======== ================================================================
help           normal   Shows all registered formatters.
bad_steps      dry-run  Shows BAD STEP-DEFINITIONS (if any exist).
json           normal   JSON dump of test run
json.pretty    normal   JSON dump of test run (human readable)
plain          normal   Very basic formatter with maximum compatibility
pretty         normal   Standard colourised pretty formatter
progress       normal   Shows dotted progress for each executed scenario.
progress2      normal   Shows dotted progress for each executed step.
progress3      normal   Shows detailed progress for each step of a scenario.
rerun          normal   Emits scenario file locations of failing scenarios
sphinx.steps   dry-run  Generate sphinx-based documentation for step definitions.
steps          dry-run  Shows step definitions (step implementations).
steps.catalog  dry-run  Shows non-technical documentation for step definitions.
steps.code     dry-run  Shows executed steps combined with their code.
steps.doc      dry-run  Shows documentation for step definitions.
steps.usage    dry-run  Shows how step definitions are used by steps (in feature files).
tags           dry-run  Shows tags (and how often they are used).
tags.location  dry-run  Shows tags and the location where they are used.
============== ======== ================================================================

.. note::

    You can use more than one formatter during a test run.
    But in general you have only one formatter that writes to ``stdout``.

    The ""Mode"" column indicates if a formatter is intended to be used in
    dry-run (``--dry-run`` command-line option) or normal mode.


Given:User-Defined Formatters
-----------------------

Behave allows you to provide your own formatter (class)::

    # -- USE: Formatter class ""Json2Formatter"" in python module ""foo.bar""
    # NOTE: Formatter must be importable from python search path.
    behave -f foo.bar:Json2Formatter ...

The usage of a user-defined formatter can be simplified by providing an
alias name for it in the configuration file:

.. code-block:: ini

    # -- FILE: behave.ini
    # ALIAS SUPPORTS: behave -f json2 ...
    # NOTE: Formatter aliases may override builtin formatters.
    [behave.formatters]
    json2 = foo.bar:Json2Formatter

If your formatter can be configured, you should use the userdata concept
to provide them. The formatter should use the attribute schema:

.. code-block:: ini

    # -- FILE: behave.ini
    # SCHEMA: behave.formatter.<FORMATTER_NAME>.<ATTRIBUTE_NAME>
    [behave.userdata]
    behave.formatter.json2.use_pretty = true

    # -- SUPPORTS ALSO:
    #    behave -f json2 -D behave.formatter.json2.use_pretty ...


More Formatters
---------------

The following contributed formatters are currently known:

============== =========================================================================
Name           Description
============== =========================================================================
allure         :pypi:`allure-behave`, an Allure formatter for behave.
html           :pypi:`behave-html-formatter`, a simple HTML formatter for behave.
teamcity       :pypi:`behave-teamcity`, a formatter for JetBrains TeamCity CI testruns
               with behave.
============== =========================================================================

The usage of a custom formatter can be simplified if a formatter alias is defined for.


Then:EXAMPLE:

.. code-block:: ini

    # -- FILE: behave.ini
    # FORMATTER ALIASES: ""behave -f allure"" and others...
    [behave.formatters]
    allure = allure_behave.formatter:AllureFormatter
    html = behave_html_formatter:HTMLFormatter
    teamcity = behave_teamcity:TeamcityFormatter


Embedding Screenshots / Data in Reports
------------------------------------------------------------------------------

:Hint 1: Only supported by JSON formatter
:Hint 2: Binary attachments may require base64 encoding.

You can embed data in reports with the :class:`~behave.runner.Context` method
:func:`~behave.runner.Context.attach()`, if you have configured a formatter that
supports it. Currently only the JSON formatter supports embedding data.

For example:

.. code-block:: python

    # -- FILE: features/steps/screenshot_example_steps.py
    from behave import given, when
    from behave4example.web_browser.util import take_screenshot_and_attach_to_scenario

    @given(u'I open the Google webpage')
    @when(u'I open the Google webpage')
    def step_open_google_webpage(ctx):
        ctx.browser.get(""https://www.google.com"")
        take_screenshot_and_attach_to_scenario(ctx)

.. code-block:: python

    # -- FILE: behave4example/web_browser/util.py
    # HINTS:
    #   * EXAMPLE CODE ONLY
    #   * BROWSER-SPECIFIC: Implementation may depend on browser driver.
    def take_screenshot_and_attach_to_scenario(ctx):
        # -- HINT: SELENIUM WITH CHROME: ctx.browser.get_screenshot_as_base64()
        screenshot_image = ctx.browser.get_full_page_screenshot_as_png()
        ctx.attach(""image/png"", screenshot_image)

.. code-block:: python

    # -- FILE: features/environment.py
    # EXAMPLE REQUIRES: This browser driver setup code (or something similar).
    from selenium import webdriver

    def before_all(ctx):
        ctx.browser = webdriver.Firefox()

.. seealso::

    * Selenium Python SDK: https://www.selenium.dev/selenium/docs/api/py/
    * Playwright Python SDK: https://playwright.dev/python/docs/intro


    **RELATED:** Selenium webdriver details:

    * Selenium webdriver (for Firefox): `selenium.webdriver.firefox.webdriver.WebDriver.get_full_page_screenshot_as_png`_
    * Selenium webdriver (for Chrome):  `selenium.webdriver.remote.webdriver.WebDriver.get_screenshot_as_base64`_


    **RELATED:** Playwright details:

    * https://playwright.dev/python/docs/api/class-locator#locator-screenshot
    * https://playwright.dev/python/docs/api/class-page#page-screenshot

.. _`selenium.webdriver.firefox.webdriver.WebDriver.get_full_page_screenshot_as_png`: https://www.selenium.dev/selenium/docs/api/py/webdriver_firefox/selenium.webdriver.firefox.webdriver.html?highlight=screenshot#selenium.webdriver.firefox.webdriver.WebDriver.get_full_page_screenshot_as_png
.. _`selenium.webdriver.remote.webdriver.WebDriver.get_screenshot_as_base64`: https://www.selenium.dev/selenium/docs/api/py/webdriver_remote/selenium.webdriver.remote.webdriver.html?highlight=get_screenshot_as_base64#selenium.webdriver.remote.webdriver.WebDriver.get_screenshot_as_base64


}





",/content/Feature23.feature,"Scenario: Validate JSON output from features/ test run
    Given I use the current directory as working directory
    When I run ""behave -f json -o testrun1.json features/""
    When I run ""bin/jsonschema_validate.py testrun1.json""
    Then it should pass with:
        """"""
        validate: testrun1.json ... OK
        """""""
/content/Record24.rst,"{
Scenario:.. _id.appendix.formatters:

========================
Formatters and Reporters
========================

:pypi:`behave` provides 2 different concepts for reporting results of a test run:

* formatters
* reporters

A slightly different interface is provided for each ""formatter"" concept.
The ``Formatter`` is informed about each step that is taken.
The ``Reporter`` has a more coarse-grained API.


Reporters
---------

The following reporters are currently supported:

============== ================================================================
Name            Description
============== ================================================================
junit           Provides JUnit XML-like output.
summary         Provides a summary of the test run.
============== ================================================================


Formatters
----------

The following formatters are currently supported:

============== ======== ================================================================
Name           Mode     Description
============== ======== ================================================================
help           normal   Shows all registered formatters.
bad_steps      dry-run  Shows BAD STEP-DEFINITIONS (if any exist).
json           normal   JSON dump of test run
json.pretty    normal   JSON dump of test run (human readable)
plain          normal   Very basic formatter with maximum compatibility
pretty         normal   Standard colourised pretty formatter
progress       normal   Shows dotted progress for each executed scenario.
progress2      normal   Shows dotted progress for each executed step.
progress3      normal   Shows detailed progress for each step of a scenario.
rerun          normal   Emits scenario file locations of failing scenarios
sphinx.steps   dry-run  Generate sphinx-based documentation for step definitions.
steps          dry-run  Shows step definitions (step implementations).
steps.catalog  dry-run  Shows non-technical documentation for step definitions.
steps.code     dry-run  Shows executed steps combined with their code.
steps.doc      dry-run  Shows documentation for step definitions.
steps.usage    dry-run  Shows how step definitions are used by steps (in feature files).
tags           dry-run  Shows tags (and how often they are used).
tags.location  dry-run  Shows tags and the location where they are used.
============== ======== ================================================================

.. note::

    You can use more than one formatter during a test run.
    But in general you have only one formatter that writes to ``stdout``.

    The ""Mode"" column indicates if a formatter is intended to be used in
    dry-run (``--dry-run`` command-line option) or normal mode.


Given:User-Defined Formatters
-----------------------

Behave allows you to provide your own formatter (class)::

    # -- USE: Formatter class ""Json2Formatter"" in python module ""foo.bar""
    # NOTE: Formatter must be importable from python search path.
    behave -f foo.bar:Json2Formatter ...

The usage of a user-defined formatter can be simplified by providing an
alias name for it in the configuration file:

.. code-block:: ini

    # -- FILE: behave.ini
    # ALIAS SUPPORTS: behave -f json2 ...
    # NOTE: Formatter aliases may override builtin formatters.
    [behave.formatters]
    json2 = foo.bar:Json2Formatter

If your formatter can be configured, you should use the userdata concept
to provide them. The formatter should use the attribute schema:

.. code-block:: ini

    # -- FILE: behave.ini
    # SCHEMA: behave.formatter.<FORMATTER_NAME>.<ATTRIBUTE_NAME>
    [behave.userdata]
    behave.formatter.json2.use_pretty = true

    # -- SUPPORTS ALSO:
    #    behave -f json2 -D behave.formatter.json2.use_pretty ...


More Formatters
---------------

The following contributed formatters are currently known:

============== =========================================================================
Name           Description
============== =========================================================================
allure         :pypi:`allure-behave`, an Allure formatter for behave.
html           :pypi:`behave-html-formatter`, a simple HTML formatter for behave.
teamcity       :pypi:`behave-teamcity`, a formatter for JetBrains TeamCity CI testruns
               with behave.
============== =========================================================================

The usage of a custom formatter can be simplified if a formatter alias is defined for.


Then:EXAMPLE:

.. code-block:: ini

    # -- FILE: behave.ini
    # FORMATTER ALIASES: ""behave -f allure"" and others...
    [behave.formatters]
    allure = allure_behave.formatter:AllureFormatter
    html = behave_html_formatter:HTMLFormatter
    teamcity = behave_teamcity:TeamcityFormatter


Embedding Screenshots / Data in Reports
------------------------------------------------------------------------------

:Hint 1: Only supported by JSON formatter
:Hint 2: Binary attachments may require base64 encoding.

You can embed data in reports with the :class:`~behave.runner.Context` method
:func:`~behave.runner.Context.attach()`, if you have configured a formatter that
supports it. Currently only the JSON formatter supports embedding data.

For example:

.. code-block:: python

    # -- FILE: features/steps/screenshot_example_steps.py
    from behave import given, when
    from behave4example.web_browser.util import take_screenshot_and_attach_to_scenario

    @given(u'I open the Google webpage')
    @when(u'I open the Google webpage')
    def step_open_google_webpage(ctx):
        ctx.browser.get(""https://www.google.com"")
        take_screenshot_and_attach_to_scenario(ctx)

.. code-block:: python

    # -- FILE: behave4example/web_browser/util.py
    # HINTS:
    #   * EXAMPLE CODE ONLY
    #   * BROWSER-SPECIFIC: Implementation may depend on browser driver.
    def take_screenshot_and_attach_to_scenario(ctx):
        # -- HINT: SELENIUM WITH CHROME: ctx.browser.get_screenshot_as_base64()
        screenshot_image = ctx.browser.get_full_page_screenshot_as_png()
        ctx.attach(""image/png"", screenshot_image)

.. code-block:: python

    # -- FILE: features/environment.py
    # EXAMPLE REQUIRES: This browser driver setup code (or something similar).
    from selenium import webdriver

    def before_all(ctx):
        ctx.browser = webdriver.Firefox()

.. seealso::

    * Selenium Python SDK: https://www.selenium.dev/selenium/docs/api/py/
    * Playwright Python SDK: https://playwright.dev/python/docs/intro


    **RELATED:** Selenium webdriver details:

    * Selenium webdriver (for Firefox): `selenium.webdriver.firefox.webdriver.WebDriver.get_full_page_screenshot_as_png`_
    * Selenium webdriver (for Chrome):  `selenium.webdriver.remote.webdriver.WebDriver.get_screenshot_as_base64`_


    **RELATED:** Playwright details:

    * https://playwright.dev/python/docs/api/class-locator#locator-screenshot
    * https://playwright.dev/python/docs/api/class-page#page-screenshot

.. _`selenium.webdriver.firefox.webdriver.WebDriver.get_full_page_screenshot_as_png`: https://www.selenium.dev/selenium/docs/api/py/webdriver_firefox/selenium.webdriver.firefox.webdriver.html?highlight=screenshot#selenium.webdriver.firefox.webdriver.WebDriver.get_full_page_screenshot_as_png
.. _`selenium.webdriver.remote.webdriver.WebDriver.get_screenshot_as_base64`: https://www.selenium.dev/selenium/docs/api/py/webdriver_remote/selenium.webdriver.remote.webdriver.html?highlight=get_screenshot_as_base64#selenium.webdriver.remote.webdriver.WebDriver.get_screenshot_as_base64


}





",/content/Feature24.feature,"Scenario: Validate JSON output from issue.features/ test run
    Given I use the current directory as working directory
    When I run ""behave -f json -o testrun2.json issue.features/""
    When  I run ""bin/jsonschema_validate.py testrun2.json""
    Then it should pass with:
        """"""
        validate: testrun2.json ... OK
        """"""
"
/content/Record25.rst,"{
Scenario:.. _id.appendix.formatters:

========================
Formatters and Reporters
========================

:pypi:`behave` provides 2 different concepts for reporting results of a test run:

* formatters
* reporters

A slightly different interface is provided for each ""formatter"" concept.
The ``Formatter`` is informed about each step that is taken.
The ``Reporter`` has a more coarse-grained API.


Reporters
---------

The following reporters are currently supported:

============== ================================================================
Name            Description
============== ================================================================
junit           Provides JUnit XML-like output.
summary         Provides a summary of the test run.
============== ================================================================


Formatters
----------

The following formatters are currently supported:

============== ======== ================================================================
Name           Mode     Description
============== ======== ================================================================
help           normal   Shows all registered formatters.
bad_steps      dry-run  Shows BAD STEP-DEFINITIONS (if any exist).
json           normal   JSON dump of test run
json.pretty    normal   JSON dump of test run (human readable)
plain          normal   Very basic formatter with maximum compatibility
pretty         normal   Standard colourised pretty formatter
progress       normal   Shows dotted progress for each executed scenario.
progress2      normal   Shows dotted progress for each executed step.
progress3      normal   Shows detailed progress for each step of a scenario.
rerun          normal   Emits scenario file locations of failing scenarios
sphinx.steps   dry-run  Generate sphinx-based documentation for step definitions.
steps          dry-run  Shows step definitions (step implementations).
steps.catalog  dry-run  Shows non-technical documentation for step definitions.
steps.code     dry-run  Shows executed steps combined with their code.
steps.doc      dry-run  Shows documentation for step definitions.
steps.usage    dry-run  Shows how step definitions are used by steps (in feature files).
tags           dry-run  Shows tags (and how often they are used).
tags.location  dry-run  Shows tags and the location where they are used.
============== ======== ================================================================

.. note::

    You can use more than one formatter during a test run.
    But in general you have only one formatter that writes to ``stdout``.

    The ""Mode"" column indicates if a formatter is intended to be used in
    dry-run (``--dry-run`` command-line option) or normal mode.


Given:User-Defined Formatters
-----------------------

Behave allows you to provide your own formatter (class)::

    # -- USE: Formatter class ""Json2Formatter"" in python module ""foo.bar""
    # NOTE: Formatter must be importable from python search path.
    behave -f foo.bar:Json2Formatter ...

The usage of a user-defined formatter can be simplified by providing an
alias name for it in the configuration file:

.. code-block:: ini

    # -- FILE: behave.ini
    # ALIAS SUPPORTS: behave -f json2 ...
    # NOTE: Formatter aliases may override builtin formatters.
    [behave.formatters]
    json2 = foo.bar:Json2Formatter

If your formatter can be configured, you should use the userdata concept
to provide them. The formatter should use the attribute schema:

.. code-block:: ini

    # -- FILE: behave.ini
    # SCHEMA: behave.formatter.<FORMATTER_NAME>.<ATTRIBUTE_NAME>
    [behave.userdata]
    behave.formatter.json2.use_pretty = true

    # -- SUPPORTS ALSO:
    #    behave -f json2 -D behave.formatter.json2.use_pretty ...


More Formatters
---------------

The following contributed formatters are currently known:

============== =========================================================================
Name           Description
============== =========================================================================
allure         :pypi:`allure-behave`, an Allure formatter for behave.
html           :pypi:`behave-html-formatter`, a simple HTML formatter for behave.
teamcity       :pypi:`behave-teamcity`, a formatter for JetBrains TeamCity CI testruns
               with behave.
============== =========================================================================

The usage of a custom formatter can be simplified if a formatter alias is defined for.


Then:EXAMPLE:

.. code-block:: ini

    # -- FILE: behave.ini
    # FORMATTER ALIASES: ""behave -f allure"" and others...
    [behave.formatters]
    allure = allure_behave.formatter:AllureFormatter
    html = behave_html_formatter:HTMLFormatter
    teamcity = behave_teamcity:TeamcityFormatter


Embedding Screenshots / Data in Reports
------------------------------------------------------------------------------

:Hint 1: Only supported by JSON formatter
:Hint 2: Binary attachments may require base64 encoding.

You can embed data in reports with the :class:`~behave.runner.Context` method
:func:`~behave.runner.Context.attach()`, if you have configured a formatter that
supports it. Currently only the JSON formatter supports embedding data.

For example:

.. code-block:: python

    # -- FILE: features/steps/screenshot_example_steps.py
    from behave import given, when
    from behave4example.web_browser.util import take_screenshot_and_attach_to_scenario

    @given(u'I open the Google webpage')
    @when(u'I open the Google webpage')
    def step_open_google_webpage(ctx):
        ctx.browser.get(""https://www.google.com"")
        take_screenshot_and_attach_to_scenario(ctx)

.. code-block:: python

    # -- FILE: behave4example/web_browser/util.py
    # HINTS:
    #   * EXAMPLE CODE ONLY
    #   * BROWSER-SPECIFIC: Implementation may depend on browser driver.
    def take_screenshot_and_attach_to_scenario(ctx):
        # -- HINT: SELENIUM WITH CHROME: ctx.browser.get_screenshot_as_base64()
        screenshot_image = ctx.browser.get_full_page_screenshot_as_png()
        ctx.attach(""image/png"", screenshot_image)

.. code-block:: python

    # -- FILE: features/environment.py
    # EXAMPLE REQUIRES: This browser driver setup code (or something similar).
    from selenium import webdriver

    def before_all(ctx):
        ctx.browser = webdriver.Firefox()

.. seealso::

    * Selenium Python SDK: https://www.selenium.dev/selenium/docs/api/py/
    * Playwright Python SDK: https://playwright.dev/python/docs/intro


    **RELATED:** Selenium webdriver details:

    * Selenium webdriver (for Firefox): `selenium.webdriver.firefox.webdriver.WebDriver.get_full_page_screenshot_as_png`_
    * Selenium webdriver (for Chrome):  `selenium.webdriver.remote.webdriver.WebDriver.get_screenshot_as_base64`_


    **RELATED:** Playwright details:

    * https://playwright.dev/python/docs/api/class-locator#locator-screenshot
    * https://playwright.dev/python/docs/api/class-page#page-screenshot

.. _`selenium.webdriver.firefox.webdriver.WebDriver.get_full_page_screenshot_as_png`: https://www.selenium.dev/selenium/docs/api/py/webdriver_firefox/selenium.webdriver.firefox.webdriver.html?highlight=screenshot#selenium.webdriver.firefox.webdriver.WebDriver.get_full_page_screenshot_as_png
.. _`selenium.webdriver.remote.webdriver.WebDriver.get_screenshot_as_base64`: https://www.selenium.dev/selenium/docs/api/py/webdriver_remote/selenium.webdriver.remote.webdriver.html?highlight=get_screenshot_as_base64#selenium.webdriver.remote.webdriver.WebDriver.get_screenshot_as_base64


}





",/content/Feature25.feature,"Scenario: Validate JSON output from tools/test-features/ test run
    Given I use the current directory as working directory
    When I run ""behave -f json -o testrun3.json tools/test-features/""
    When I run ""bin/jsonschema_validate.py testrun3.json""
    Then it should pass with:
        """"""
        validate: testrun3.json ... OK
        """""""
/content/Record26.rst,"{
Scenario:.. _id.appendix.formatters:

========================
Formatters and Reporters
========================

:pypi:`behave` provides 2 different concepts for reporting results of a test run:

* formatters
* reporters

A slightly different interface is provided for each ""formatter"" concept.
The ``Formatter`` is informed about each step that is taken.
The ``Reporter`` has a more coarse-grained API.


Reporters
---------

The following reporters are currently supported:

============== ================================================================
Name            Description
============== ================================================================
junit           Provides JUnit XML-like output.
summary         Provides a summary of the test run.
============== ================================================================


Formatters
----------

The following formatters are currently supported:

============== ======== ================================================================
Name           Mode     Description
============== ======== ================================================================
help           normal   Shows all registered formatters.
bad_steps      dry-run  Shows BAD STEP-DEFINITIONS (if any exist).
json           normal   JSON dump of test run
json.pretty    normal   JSON dump of test run (human readable)
plain          normal   Very basic formatter with maximum compatibility
pretty         normal   Standard colourised pretty formatter
progress       normal   Shows dotted progress for each executed scenario.
progress2      normal   Shows dotted progress for each executed step.
progress3      normal   Shows detailed progress for each step of a scenario.
rerun          normal   Emits scenario file locations of failing scenarios
sphinx.steps   dry-run  Generate sphinx-based documentation for step definitions.
steps          dry-run  Shows step definitions (step implementations).
steps.catalog  dry-run  Shows non-technical documentation for step definitions.
steps.code     dry-run  Shows executed steps combined with their code.
steps.doc      dry-run  Shows documentation for step definitions.
steps.usage    dry-run  Shows how step definitions are used by steps (in feature files).
tags           dry-run  Shows tags (and how often they are used).
tags.location  dry-run  Shows tags and the location where they are used.
============== ======== ================================================================

.. note::

    You can use more than one formatter during a test run.
    But in general you have only one formatter that writes to ``stdout``.

    The ""Mode"" column indicates if a formatter is intended to be used in
    dry-run (``--dry-run`` command-line option) or normal mode.


Given:User-Defined Formatters
-----------------------

Behave allows you to provide your own formatter (class)::

    # -- USE: Formatter class ""Json2Formatter"" in python module ""foo.bar""
    # NOTE: Formatter must be importable from python search path.
    behave -f foo.bar:Json2Formatter ...

The usage of a user-defined formatter can be simplified by providing an
alias name for it in the configuration file:

.. code-block:: ini

    # -- FILE: behave.ini
    # ALIAS SUPPORTS: behave -f json2 ...
    # NOTE: Formatter aliases may override builtin formatters.
    [behave.formatters]
    json2 = foo.bar:Json2Formatter

If your formatter can be configured, you should use the userdata concept
to provide them. The formatter should use the attribute schema:

.. code-block:: ini

    # -- FILE: behave.ini
    # SCHEMA: behave.formatter.<FORMATTER_NAME>.<ATTRIBUTE_NAME>
    [behave.userdata]
    behave.formatter.json2.use_pretty = true

    # -- SUPPORTS ALSO:
    #    behave -f json2 -D behave.formatter.json2.use_pretty ...


More Formatters
---------------

The following contributed formatters are currently known:

============== =========================================================================
Name           Description
============== =========================================================================
allure         :pypi:`allure-behave`, an Allure formatter for behave.
html           :pypi:`behave-html-formatter`, a simple HTML formatter for behave.
teamcity       :pypi:`behave-teamcity`, a formatter for JetBrains TeamCity CI testruns
               with behave.
============== =========================================================================

The usage of a custom formatter can be simplified if a formatter alias is defined for.


Then:EXAMPLE:

.. code-block:: ini

    # -- FILE: behave.ini
    # FORMATTER ALIASES: ""behave -f allure"" and others...
    [behave.formatters]
    allure = allure_behave.formatter:AllureFormatter
    html = behave_html_formatter:HTMLFormatter
    teamcity = behave_teamcity:TeamcityFormatter


Embedding Screenshots / Data in Reports
------------------------------------------------------------------------------

:Hint 1: Only supported by JSON formatter
:Hint 2: Binary attachments may require base64 encoding.

You can embed data in reports with the :class:`~behave.runner.Context` method
:func:`~behave.runner.Context.attach()`, if you have configured a formatter that
supports it. Currently only the JSON formatter supports embedding data.

For example:

.. code-block:: python

    # -- FILE: features/steps/screenshot_example_steps.py
    from behave import given, when
    from behave4example.web_browser.util import take_screenshot_and_attach_to_scenario

    @given(u'I open the Google webpage')
    @when(u'I open the Google webpage')
    def step_open_google_webpage(ctx):
        ctx.browser.get(""https://www.google.com"")
        take_screenshot_and_attach_to_scenario(ctx)

.. code-block:: python

    # -- FILE: behave4example/web_browser/util.py
    # HINTS:
    #   * EXAMPLE CODE ONLY
    #   * BROWSER-SPECIFIC: Implementation may depend on browser driver.
    def take_screenshot_and_attach_to_scenario(ctx):
        # -- HINT: SELENIUM WITH CHROME: ctx.browser.get_screenshot_as_base64()
        screenshot_image = ctx.browser.get_full_page_screenshot_as_png()
        ctx.attach(""image/png"", screenshot_image)

.. code-block:: python

    # -- FILE: features/environment.py
    # EXAMPLE REQUIRES: This browser driver setup code (or something similar).
    from selenium import webdriver

    def before_all(ctx):
        ctx.browser = webdriver.Firefox()

.. seealso::

    * Selenium Python SDK: https://www.selenium.dev/selenium/docs/api/py/
    * Playwright Python SDK: https://playwright.dev/python/docs/intro


    **RELATED:** Selenium webdriver details:

    * Selenium webdriver (for Firefox): `selenium.webdriver.firefox.webdriver.WebDriver.get_full_page_screenshot_as_png`_
    * Selenium webdriver (for Chrome):  `selenium.webdriver.remote.webdriver.WebDriver.get_screenshot_as_base64`_


    **RELATED:** Playwright details:

    * https://playwright.dev/python/docs/api/class-locator#locator-screenshot
    * https://playwright.dev/python/docs/api/class-page#page-screenshot

.. _`selenium.webdriver.firefox.webdriver.WebDriver.get_full_page_screenshot_as_png`: https://www.selenium.dev/selenium/docs/api/py/webdriver_firefox/selenium.webdriver.firefox.webdriver.html?highlight=screenshot#selenium.webdriver.firefox.webdriver.WebDriver.get_full_page_screenshot_as_png
.. _`selenium.webdriver.remote.webdriver.WebDriver.get_screenshot_as_base64`: https://www.selenium.dev/selenium/docs/api/py/webdriver_remote/selenium.webdriver.remote.webdriver.html?highlight=get_screenshot_as_base64#selenium.webdriver.remote.webdriver.WebDriver.get_screenshot_as_base64


}





",/content/Feature26.feature,"Scenario: Validate JSON output from example/gherkin_v6/ test run
    Given I use the directory ""examples/gherkin_v6"" as working directory
    When I run ""behave -f json -o testrun_gherkin6_1.json features/""
    When I run ""../../bin/jsonschema_validate.py testrun_gherkin6_1.json""
    Then it should pass with:
        """"""
        validate: testrun_gherkin6_1.json ... OK
        """"""
"
/content/Record27.rst,"{
Scenario:.. _id.appendix.formatters:

========================
Formatters and Reporters
========================

:pypi:`behave` provides 2 different concepts for reporting results of a test run:

* formatters
* reporters

A slightly different interface is provided for each ""formatter"" concept.
The ``Formatter`` is informed about each step that is taken.
The ``Reporter`` has a more coarse-grained API.


Reporters
---------

The following reporters are currently supported:

============== ================================================================
Name            Description
============== ================================================================
junit           Provides JUnit XML-like output.
summary         Provides a summary of the test run.
============== ================================================================


Formatters
----------

The following formatters are currently supported:

============== ======== ================================================================
Name           Mode     Description
============== ======== ================================================================
help           normal   Shows all registered formatters.
bad_steps      dry-run  Shows BAD STEP-DEFINITIONS (if any exist).
json           normal   JSON dump of test run
json.pretty    normal   JSON dump of test run (human readable)
plain          normal   Very basic formatter with maximum compatibility
pretty         normal   Standard colourised pretty formatter
progress       normal   Shows dotted progress for each executed scenario.
progress2      normal   Shows dotted progress for each executed step.
progress3      normal   Shows detailed progress for each step of a scenario.
rerun          normal   Emits scenario file locations of failing scenarios
sphinx.steps   dry-run  Generate sphinx-based documentation for step definitions.
steps          dry-run  Shows step definitions (step implementations).
steps.catalog  dry-run  Shows non-technical documentation for step definitions.
steps.code     dry-run  Shows executed steps combined with their code.
steps.doc      dry-run  Shows documentation for step definitions.
steps.usage    dry-run  Shows how step definitions are used by steps (in feature files).
tags           dry-run  Shows tags (and how often they are used).
tags.location  dry-run  Shows tags and the location where they are used.
============== ======== ================================================================

.. note::

    You can use more than one formatter during a test run.
    But in general you have only one formatter that writes to ``stdout``.

    The ""Mode"" column indicates if a formatter is intended to be used in
    dry-run (``--dry-run`` command-line option) or normal mode.


Given:User-Defined Formatters
-----------------------

Behave allows you to provide your own formatter (class)::

    # -- USE: Formatter class ""Json2Formatter"" in python module ""foo.bar""
    # NOTE: Formatter must be importable from python search path.
    behave -f foo.bar:Json2Formatter ...

The usage of a user-defined formatter can be simplified by providing an
alias name for it in the configuration file:

.. code-block:: ini

    # -- FILE: behave.ini
    # ALIAS SUPPORTS: behave -f json2 ...
    # NOTE: Formatter aliases may override builtin formatters.
    [behave.formatters]
    json2 = foo.bar:Json2Formatter

If your formatter can be configured, you should use the userdata concept
to provide them. The formatter should use the attribute schema:

.. code-block:: ini

    # -- FILE: behave.ini
    # SCHEMA: behave.formatter.<FORMATTER_NAME>.<ATTRIBUTE_NAME>
    [behave.userdata]
    behave.formatter.json2.use_pretty = true

    # -- SUPPORTS ALSO:
    #    behave -f json2 -D behave.formatter.json2.use_pretty ...


More Formatters
---------------

The following contributed formatters are currently known:

============== =========================================================================
Name           Description
============== =========================================================================
allure         :pypi:`allure-behave`, an Allure formatter for behave.
html           :pypi:`behave-html-formatter`, a simple HTML formatter for behave.
teamcity       :pypi:`behave-teamcity`, a formatter for JetBrains TeamCity CI testruns
               with behave.
============== =========================================================================

The usage of a custom formatter can be simplified if a formatter alias is defined for.


Then:EXAMPLE:

.. code-block:: ini

    # -- FILE: behave.ini
    # FORMATTER ALIASES: ""behave -f allure"" and others...
    [behave.formatters]
    allure = allure_behave.formatter:AllureFormatter
    html = behave_html_formatter:HTMLFormatter
    teamcity = behave_teamcity:TeamcityFormatter


Embedding Screenshots / Data in Reports
------------------------------------------------------------------------------

:Hint 1: Only supported by JSON formatter
:Hint 2: Binary attachments may require base64 encoding.

You can embed data in reports with the :class:`~behave.runner.Context` method
:func:`~behave.runner.Context.attach()`, if you have configured a formatter that
supports it. Currently only the JSON formatter supports embedding data.

For example:

.. code-block:: python

    # -- FILE: features/steps/screenshot_example_steps.py
    from behave import given, when
    from behave4example.web_browser.util import take_screenshot_and_attach_to_scenario

    @given(u'I open the Google webpage')
    @when(u'I open the Google webpage')
    def step_open_google_webpage(ctx):
        ctx.browser.get(""https://www.google.com"")
        take_screenshot_and_attach_to_scenario(ctx)

.. code-block:: python

    # -- FILE: behave4example/web_browser/util.py
    # HINTS:
    #   * EXAMPLE CODE ONLY
    #   * BROWSER-SPECIFIC: Implementation may depend on browser driver.
    def take_screenshot_and_attach_to_scenario(ctx):
        # -- HINT: SELENIUM WITH CHROME: ctx.browser.get_screenshot_as_base64()
        screenshot_image = ctx.browser.get_full_page_screenshot_as_png()
        ctx.attach(""image/png"", screenshot_image)

.. code-block:: python

    # -- FILE: features/environment.py
    # EXAMPLE REQUIRES: This browser driver setup code (or something similar).
    from selenium import webdriver

    def before_all(ctx):
        ctx.browser = webdriver.Firefox()

.. seealso::

    * Selenium Python SDK: https://www.selenium.dev/selenium/docs/api/py/
    * Playwright Python SDK: https://playwright.dev/python/docs/intro


    **RELATED:** Selenium webdriver details:

    * Selenium webdriver (for Firefox): `selenium.webdriver.firefox.webdriver.WebDriver.get_full_page_screenshot_as_png`_
    * Selenium webdriver (for Chrome):  `selenium.webdriver.remote.webdriver.WebDriver.get_screenshot_as_base64`_


    **RELATED:** Playwright details:

    * https://playwright.dev/python/docs/api/class-locator#locator-screenshot
    * https://playwright.dev/python/docs/api/class-page#page-screenshot

.. _`selenium.webdriver.firefox.webdriver.WebDriver.get_full_page_screenshot_as_png`: https://www.selenium.dev/selenium/docs/api/py/webdriver_firefox/selenium.webdriver.firefox.webdriver.html?highlight=screenshot#selenium.webdriver.firefox.webdriver.WebDriver.get_full_page_screenshot_as_png
.. _`selenium.webdriver.remote.webdriver.WebDriver.get_screenshot_as_base64`: https://www.selenium.dev/selenium/docs/api/py/webdriver_remote/selenium.webdriver.remote.webdriver.html?highlight=get_screenshot_as_base64#selenium.webdriver.remote.webdriver.WebDriver.get_screenshot_as_base64


}





",/content/Feature27.feature," @gherkin_v6
  Scenario: Validate JSON output from example/gherkin_v6/ test run (case: partly failing)
    Given I use the directory ""examples/gherkin_v6"" as working directory
    When I run ""behave --tags=fail -f json -o testrun_gherkin6_2.json features/""
    When I run ""../../bin/jsonschema_validate.py testrun_gherkin6_2.json""
    Then it should pass with:
        """"""
        validate: testrun_gherkin6_2.json ... OK
        """""""
/content/Record28.rst,"{
Scenario:Tag Expressions
Given:.. include:: _content.tag_expressions_v2.rst
Then:.. include:: _content.tag_expressions_v2.rst

}",/content/Feature28.feature,"Scenario: Select @foo
    Given the tag expression ""@foo""
    Then the tag expression selects elements with tags:
        | tags         | selected? |
        |              |   no      |
        | @foo         |   yes     |
        | @other       |   no      |
        | @foo @other  |   yes     |"
/content/Record29.rst,"{
Scenario:Tag Expressions
Given:.. include:: _content.tag_expressions_v2.rst
Then:.. include:: _content.tag_expressions_v2.rst

}",/content/Feature29.feature,"Scenario: Tag expression with 0..1 tags
    Given the model elements with name and tags:
        | name | tags         | Comment |
        | S0   |              | Untagged    |
        | S1   | @foo         | With 1 tag  |
        | S2   | @other       |             |
        | S3   | @foo @other  | With 2 tags |
    And note that ""are all combinations of 0..2 tags""
    Then the tag expression selects model elements with:
        | tag expression | selected?      | Case comment |
        |                | S0, S1, S2, S3 | Select all (empty tag expression) |
        |  @foo          | S1, S3         | Select @foo                       |
        | -@foo          | S0, S2         | not @foo, selects untagged elements |
    But note that ""tag expression variants are also supported""
    And the tag expression selects model elements with:
        | tag expression | selected?      | Case comment |
        |  foo           | S1, S3         |     @foo: '@' is optional     |
        | -foo           | S0, S2         | not @foo: '@' is optional     |
        | ~foo           | S0, S2         | not @foo: tilde as minus      |
        | ~@foo          | S0, S2         | not @foo: '~@' is supported   |"
/content/Record30.rst,"{
Scenario:Tag Expressions
Given:.. include:: _content.tag_expressions_v2.rst
Then:.. include:: _content.tag_expressions_v2.rst

}",/content/Feature30.feature,"Scenario: Tag expression with two tags (@foo, @bar)
    Given the model elements with name and tags:
        | name | tags             | Comment |
        | S0   |                  | Untagged    |
        | S1   | @foo             | With 1 tag  |
        | S2   | @bar             |             |
        | S3   | @other           |             |
        | S4   | @foo @bar        | With 2 tags |
        | S5   | @foo @other      |             |
        | S6   | @bar @other      |             |
        | S7   | @foo @bar @other | With 3 tags |
    And note that ""are all combinations of 0..3 tags""
    Then the tag expression selects model elements with:
        | tag expression | selected?                      | Case |
        |                | S0, S1, S2, S3, S4, S5, S6, S7 | Select all            |
        |  @foo,@bar     | S1, S2, S4, S5, S6, S7         | @foo or @bar          |
        |  @foo,-@bar    | S0, S1, S3, S4, S5, S7         | @foo or not @bar      |
        | -@foo,-@bar    | S0, S1, S2, S3, S5, S6         | not @foo or @not @bar |
        |  @foo  @bar    | S4, S7                         | @foo and @bar         |
        |  @foo -@bar    | S1, S5                         | @foo and not @bar     |
        | -@foo -@bar    | S0, S3                         | not @foo and not @bar |"
/content/Record31.rst,"{
Scenario:Tag Expressions
Given:.. include:: _content.tag_expressions_v2.rst
Then:.. include:: _content.tag_expressions_v2.rst

}",/content/Feature31.feature," Scenario: Tag expression with three tags (@foo, @bar, @zap)
    Given the model elements with name and tags:
        | name | tags                   | Comment |
        | S0   |                        | Untagged    |
        | S1   | @foo                   | With 1 tag  |
        | S2   | @bar                   |             |
        | S3   | @zap                   |             |
        | S4   | @other                 |             |
        | S5   | @foo @bar              | With 2 tags |
        | S6   | @foo @zap              |             |
        | S7   | @foo @other            |             |
        | S8   | @bar @zap              |             |
        | S9   | @bar @other            |             |
        | S10  | @zap @other            |             |
        | S11  | @foo @bar @zap         | With 3 tags |
        | S12  | @foo @bar @other       |             |
        | S13  | @foo @zap @other       |             |
        | S14  | @bar @zap @other       |             |
        | S15  | @foo @bar @zap @other  | With 4 tags |
    And note that ""are all combinations of 0..4 tags""
    Then the tag expression selects model elements with:
        | tag expression   | selected?                   | Case |
        |  @foo,@bar  @zap | S6, S8, S11, S13, S14, S15  | (@foo or @bar) and @zap |
        |  @foo,@bar -@zap | S1, S2, S5, S7, S9, S12     | (@foo or @bar) and not @zap |
        |  @foo,-@bar @zap | S3, S6, S10, S11, S13, S15  | (@foo or not @bar) and @zap |"
/content/Record32.rst,"{
Scenario:Link:https://behave.readthedocs.io/en/latest/api/#logging-capture
Link2:https://behave.readthedocs.io/en/latest/api/#behave.runner.Context.captured
Logging Capture
The logging capture behave uses by default is implemented by the class LoggingCapture. It has methods

class behave.log_capture.LoggingCapture(config, level=None)
Capture logging events in a memory buffer for later display or query.

Captured logging events are stored on the attribute buffer:

buffer
This is a list of captured logging events as logging.LogRecords.

Given:By default the format of the messages will be:

'%(levelname)s:%(name)s:%(message)s'
This may be overridden using standard logging formatter names in the configuration variable logging_format.

The level of logging captured is set to logging.NOTSET by default. You may override this using the configuration setting logging_level (which is set to a level name.)

Finally there may be filtering of logging events specified by the configuration variable logging_filter.

abandon()
Turn off logging capture.

If other handlers were removed by inveigle() then they are reinstated.

any_errors()
Search through the buffer for any ERROR or CRITICAL events.

Returns boolean indicating whether a match was found.

find_event(pattern)
Search through the buffer for a message that matches the given regular expression.

Returns boolean indicating whether a match was found.

flush()
Override to implement custom flushing behaviour.

This version just zaps the buffer to empty.

inveigle()
Turn on logging capture by replacing all existing handlers configured in the logging module.

If the config var logging_clear_handlers is set then we also remove all existing handlers.

We also set the level of the root logger.

The opposite of this is abandon().

The log_capture module also defines a handy logging capture decorator that’s intended to be used on your environment file functions.

behave.log_capture.capture(*args, **kw)
Decorator to wrap an environment file function in log file capture.

It configures the logging capture using the behave context, the first argument to the function being decorated (so don’t use this to decorate something that doesn’t have context as the first argument).


Then:The basic usage is:

The function prints any captured logging (at the level determined by the log_level configuration setting) directly to stdout, regardless of error conditions.

It is mostly useful for debugging in situations where you are seeing a message like:

No handlers could be found for logger ""name""
The decorator takes an optional “level” keyword argument which limits the level of logging captured, overriding the level in the run’s configuration:

This would limit the logging captured to just ERROR and above, and thus only display logged events if they are interesting.
If any output capture is enabled, provides access to a Captured object that contains a snapshot of all captured data (stdout/stderr/log).
}",/content/Feature32.feature,"Scenario: Test Setup
        Given a new working directory
        And   a file named ""features/steps/stderr_steps.py"" with:
            """"""
            from behave import step
            import sys

            @step('a step writes ""{text}"" to stderr and passes')
            def step_writes_to_stderr_and_passes(context, text):
                sys.stderr.write(""stderr:%s;\n"" % text)

            @step('a step writes ""{text}"" to stderr and fails')
            def step_writes_to_stderr_and_fails(context, text):
                sys.stderr.write(""stderr:%s;\n"" % text)
                assert False, ""XFAIL, step with: %s;"" % text
            """"""
        And a file named ""features/capture_stderr.example1.feature"" with:
            """"""
            Feature:
                Scenario:
                    Given a step writes ""Alice"" to stderr and passes
                    When a step writes ""Bob"" to stderr and passes
                    Then a step writes ""Charly"" to stderr and passes
            """"""
        And a file named ""features/capture_stderr.example2.feature"" with:
            """"""
            Feature:
                Scenario:
                    Given a step writes ""Alice"" to stderr and passes
                    When a step writes ""Bob"" to stderr and fails
                    Then a step writes ""Charly"" to stderr and fails
            """"""

"
/content/Record33.rst,"{
Scenario:Link:https://behave.readthedocs.io/en/latest/api/#logging-capture
Link2:https://behave.readthedocs.io/en/latest/api/#behave.runner.Context.captured
Logging Capture
The logging capture behave uses by default is implemented by the class LoggingCapture. It has methods

class behave.log_capture.LoggingCapture(config, level=None)
Capture logging events in a memory buffer for later display or query.

Captured logging events are stored on the attribute buffer:

buffer
This is a list of captured logging events as logging.LogRecords.

Given:By default the format of the messages will be:

'%(levelname)s:%(name)s:%(message)s'
This may be overridden using standard logging formatter names in the configuration variable logging_format.

The level of logging captured is set to logging.NOTSET by default. You may override this using the configuration setting logging_level (which is set to a level name.)

Finally there may be filtering of logging events specified by the configuration variable logging_filter.

abandon()
Turn off logging capture.

If other handlers were removed by inveigle() then they are reinstated.

any_errors()
Search through the buffer for any ERROR or CRITICAL events.

Returns boolean indicating whether a match was found.

find_event(pattern)
Search through the buffer for a message that matches the given regular expression.

Returns boolean indicating whether a match was found.

flush()
Override to implement custom flushing behaviour.

This version just zaps the buffer to empty.

inveigle()
Turn on logging capture by replacing all existing handlers configured in the logging module.

If the config var logging_clear_handlers is set then we also remove all existing handlers.

We also set the level of the root logger.

The opposite of this is abandon().

The log_capture module also defines a handy logging capture decorator that’s intended to be used on your environment file functions.

behave.log_capture.capture(*args, **kw)
Decorator to wrap an environment file function in log file capture.

It configures the logging capture using the behave context, the first argument to the function being decorated (so don’t use this to decorate something that doesn’t have context as the first argument).


Then:The basic usage is:

The function prints any captured logging (at the level determined by the log_level configuration setting) directly to stdout, regardless of error conditions.

It is mostly useful for debugging in situations where you are seeing a message like:

No handlers could be found for logger ""name""
The decorator takes an optional “level” keyword argument which limits the level of logging captured, overriding the level in the run’s configuration:

This would limit the logging captured to just ERROR and above, and thus only display logged events if they are interesting.
If any output capture is enabled, provides access to a Captured object that contains a snapshot of all captured data (stdout/stderr/log).
}",/content/Feature33.feature,"Scenario: Captured output is suppressed if scenario passes (CASE 1: --capture)
        When I run ""behave -f plain -T --capture features/capture_stderr.example1.feature""
        Then it should pass
        And the command output should contain:
            """"""
            Feature:
                Scenario:
                    Given a step writes ""Alice"" to stderr and passes ... passed
                    When a step writes ""Bob"" to stderr and passes ... passed
                    Then a step writes ""Charly"" to stderr and passes ... passed
            """"""
        But the command output should not contain:
            """"""
            stderr:Alice;
            """""""
/content/Record34.rst,"{
Scenario:Link:https://behave.readthedocs.io/en/latest/api/#logging-capture
Link2:https://behave.readthedocs.io/en/latest/api/#behave.runner.Context.captured
Logging Capture
The logging capture behave uses by default is implemented by the class LoggingCapture. It has methods

class behave.log_capture.LoggingCapture(config, level=None)
Capture logging events in a memory buffer for later display or query.

Captured logging events are stored on the attribute buffer:

buffer
This is a list of captured logging events as logging.LogRecords.

Given:By default the format of the messages will be:

'%(levelname)s:%(name)s:%(message)s'
This may be overridden using standard logging formatter names in the configuration variable logging_format.

The level of logging captured is set to logging.NOTSET by default. You may override this using the configuration setting logging_level (which is set to a level name.)

Finally there may be filtering of logging events specified by the configuration variable logging_filter.

abandon()
Turn off logging capture.

If other handlers were removed by inveigle() then they are reinstated.

any_errors()
Search through the buffer for any ERROR or CRITICAL events.

Returns boolean indicating whether a match was found.

find_event(pattern)
Search through the buffer for a message that matches the given regular expression.

Returns boolean indicating whether a match was found.

flush()
Override to implement custom flushing behaviour.

This version just zaps the buffer to empty.

inveigle()
Turn on logging capture by replacing all existing handlers configured in the logging module.

If the config var logging_clear_handlers is set then we also remove all existing handlers.

We also set the level of the root logger.

The opposite of this is abandon().

The log_capture module also defines a handy logging capture decorator that’s intended to be used on your environment file functions.

behave.log_capture.capture(*args, **kw)
Decorator to wrap an environment file function in log file capture.

It configures the logging capture using the behave context, the first argument to the function being decorated (so don’t use this to decorate something that doesn’t have context as the first argument).


Then:The basic usage is:

The function prints any captured logging (at the level determined by the log_level configuration setting) directly to stdout, regardless of error conditions.

It is mostly useful for debugging in situations where you are seeing a message like:

No handlers could be found for logger ""name""
The decorator takes an optional “level” keyword argument which limits the level of logging captured, overriding the level in the run’s configuration:

This would limit the logging captured to just ERROR and above, and thus only display logged events if they are interesting.
If any output capture is enabled, provides access to a Captured object that contains a snapshot of all captured data (stdout/stderr/log).
}",/content/Feature34.feature," Scenario: Captured output is suppressed if scenario passes (CASE 2: --capture-stderr)
        When I run ""behave -f plain -T --capture-stderr features/capture_stderr.example1.feature""
        Then it should pass
        And the command output should contain:
            """"""
            Feature:
                Scenario:
                    Given a step writes ""Alice"" to stderr and passes ... passed
                    When a step writes ""Bob"" to stderr and passes ... passed
                    Then a step writes ""Charly"" to stderr and passes ... passed
            """"""
        But the command output should not contain:
            """"""
            stderr:Alice;
            """""""
/content/Record35.rst,"{
Scenario:Link:https://behave.readthedocs.io/en/latest/api/#logging-capture
Link2:https://behave.readthedocs.io/en/latest/api/#behave.runner.Context.captured
Logging Capture
The logging capture behave uses by default is implemented by the class LoggingCapture. It has methods

class behave.log_capture.LoggingCapture(config, level=None)
Capture logging events in a memory buffer for later display or query.

Captured logging events are stored on the attribute buffer:

buffer
This is a list of captured logging events as logging.LogRecords.

Given:By default the format of the messages will be:

'%(levelname)s:%(name)s:%(message)s'
This may be overridden using standard logging formatter names in the configuration variable logging_format.

The level of logging captured is set to logging.NOTSET by default. You may override this using the configuration setting logging_level (which is set to a level name.)

Finally there may be filtering of logging events specified by the configuration variable logging_filter.

abandon()
Turn off logging capture.

If other handlers were removed by inveigle() then they are reinstated.

any_errors()
Search through the buffer for any ERROR or CRITICAL events.

Returns boolean indicating whether a match was found.

find_event(pattern)
Search through the buffer for a message that matches the given regular expression.

Returns boolean indicating whether a match was found.

flush()
Override to implement custom flushing behaviour.

This version just zaps the buffer to empty.

inveigle()
Turn on logging capture by replacing all existing handlers configured in the logging module.

If the config var logging_clear_handlers is set then we also remove all existing handlers.

We also set the level of the root logger.

The opposite of this is abandon().

The log_capture module also defines a handy logging capture decorator that’s intended to be used on your environment file functions.

behave.log_capture.capture(*args, **kw)
Decorator to wrap an environment file function in log file capture.

It configures the logging capture using the behave context, the first argument to the function being decorated (so don’t use this to decorate something that doesn’t have context as the first argument).


Then:The basic usage is:

The function prints any captured logging (at the level determined by the log_level configuration setting) directly to stdout, regardless of error conditions.

It is mostly useful for debugging in situations where you are seeing a message like:

No handlers could be found for logger ""name""
The decorator takes an optional “level” keyword argument which limits the level of logging captured, overriding the level in the run’s configuration:

This would limit the logging captured to just ERROR and above, and thus only display logged events if they are interesting.
If any output capture is enabled, provides access to a Captured object that contains a snapshot of all captured data (stdout/stderr/log).
}",/content/Feature35.feature," Scenario: Captured output is shown up to first failure if scenario fails (CASE 1: --capture)
        When I run ""behave -f plain -T --capture features/capture_stderr.example2.feature""
        Then it should fail with:
            """"""
            0 scenarios passed, 1 failed, 0 skipped
            1 step passed, 1 failed, 1 skipped, 0 undefined
            """"""
        And the command output should contain:
            """"""
            Feature:
                Scenario:
                    Given a step writes ""Alice"" to stderr and passes ... passed
                    When a step writes ""Bob"" to stderr and fails ... failed
            Assertion Failed: XFAIL, step with: Bob;
            Captured stderr:
            stderr:Alice;
            stderr:Bob;
            """"""
        But the command output should not contain:
            """"""
            stderr:Charly;
            """""""
/content/Record36.rst,"{
Scenario:Link:https://behave.readthedocs.io/en/latest/api/#logging-capture
Link2:https://behave.readthedocs.io/en/latest/api/#behave.runner.Context.captured
Logging Capture
The logging capture behave uses by default is implemented by the class LoggingCapture. It has methods

class behave.log_capture.LoggingCapture(config, level=None)
Capture logging events in a memory buffer for later display or query.

Captured logging events are stored on the attribute buffer:

buffer
This is a list of captured logging events as logging.LogRecords.

Given:By default the format of the messages will be:

'%(levelname)s:%(name)s:%(message)s'
This may be overridden using standard logging formatter names in the configuration variable logging_format.

The level of logging captured is set to logging.NOTSET by default. You may override this using the configuration setting logging_level (which is set to a level name.)

Finally there may be filtering of logging events specified by the configuration variable logging_filter.

abandon()
Turn off logging capture.

If other handlers were removed by inveigle() then they are reinstated.

any_errors()
Search through the buffer for any ERROR or CRITICAL events.

Returns boolean indicating whether a match was found.

find_event(pattern)
Search through the buffer for a message that matches the given regular expression.

Returns boolean indicating whether a match was found.

flush()
Override to implement custom flushing behaviour.

This version just zaps the buffer to empty.

inveigle()
Turn on logging capture by replacing all existing handlers configured in the logging module.

If the config var logging_clear_handlers is set then we also remove all existing handlers.

We also set the level of the root logger.

The opposite of this is abandon().

The log_capture module also defines a handy logging capture decorator that’s intended to be used on your environment file functions.

behave.log_capture.capture(*args, **kw)
Decorator to wrap an environment file function in log file capture.

It configures the logging capture using the behave context, the first argument to the function being decorated (so don’t use this to decorate something that doesn’t have context as the first argument).


Then:The basic usage is:

The function prints any captured logging (at the level determined by the log_level configuration setting) directly to stdout, regardless of error conditions.

It is mostly useful for debugging in situations where you are seeing a message like:

No handlers could be found for logger ""name""
The decorator takes an optional “level” keyword argument which limits the level of logging captured, overriding the level in the run’s configuration:

This would limit the logging captured to just ERROR and above, and thus only display logged events if they are interesting.
If any output capture is enabled, provides access to a Captured object that contains a snapshot of all captured data (stdout/stderr/log).
}",/content/Feature36.feature," Scenario: Captured output is shown if scenario fails up to first failure (CASE 2: --capture-stderr)
        When I run ""behave -f plain -T --capture-stderr features/capture_stderr.example2.feature""
        Then it should fail with:
            """"""
            0 scenarios passed, 1 failed, 0 skipped
            1 step passed, 1 failed, 1 skipped, 0 undefined
            """"""
        And the command output should contain:
            """"""
            Feature:
                Scenario:
                    Given a step writes ""Alice"" to stderr and passes ... passed
                    When a step writes ""Bob"" to stderr and fails ... failed
            Assertion Failed: XFAIL, step with: Bob;
            Captured stderr:
            stderr:Alice;
            stderr:Bob;
            """"""
        But the command output should not contain:
            """"""
            stderr:Charly;
            """""""
/content/Record37.rst,"{
Scenario:Link:https://behave.readthedocs.io/en/latest/api/#logging-capture
Link2:https://behave.readthedocs.io/en/latest/api/#behave.runner.Context.captured
Logging Capture
The logging capture behave uses by default is implemented by the class LoggingCapture. It has methods

class behave.log_capture.LoggingCapture(config, level=None)
Capture logging events in a memory buffer for later display or query.

Captured logging events are stored on the attribute buffer:

buffer
This is a list of captured logging events as logging.LogRecords.

Given:By default the format of the messages will be:

'%(levelname)s:%(name)s:%(message)s'
This may be overridden using standard logging formatter names in the configuration variable logging_format.

The level of logging captured is set to logging.NOTSET by default. You may override this using the configuration setting logging_level (which is set to a level name.)

Finally there may be filtering of logging events specified by the configuration variable logging_filter.

abandon()
Turn off logging capture.

If other handlers were removed by inveigle() then they are reinstated.

any_errors()
Search through the buffer for any ERROR or CRITICAL events.

Returns boolean indicating whether a match was found.

find_event(pattern)
Search through the buffer for a message that matches the given regular expression.

Returns boolean indicating whether a match was found.

flush()
Override to implement custom flushing behaviour.

This version just zaps the buffer to empty.

inveigle()
Turn on logging capture by replacing all existing handlers configured in the logging module.

If the config var logging_clear_handlers is set then we also remove all existing handlers.

We also set the level of the root logger.

The opposite of this is abandon().

The log_capture module also defines a handy logging capture decorator that’s intended to be used on your environment file functions.

behave.log_capture.capture(*args, **kw)
Decorator to wrap an environment file function in log file capture.

It configures the logging capture using the behave context, the first argument to the function being decorated (so don’t use this to decorate something that doesn’t have context as the first argument).


Then:The basic usage is:

The function prints any captured logging (at the level determined by the log_level configuration setting) directly to stdout, regardless of error conditions.

It is mostly useful for debugging in situations where you are seeing a message like:

No handlers could be found for logger ""name""
The decorator takes an optional “level” keyword argument which limits the level of logging captured, overriding the level in the run’s configuration:

This would limit the logging captured to just ERROR and above, and thus only display logged events if they are interesting.
If any output capture is enabled, provides access to a Captured object that contains a snapshot of all captured data (stdout/stderr/log).
}",/content/Feature37.feature," Scenario: All output is shown when --no-capture-stderr is used and all steps pass (CASE 1)
        When I run ""behave -f plain -T --no-capture-stderr features/capture_stderr.example1.feature""
        Then it should pass
        And the command output should contain:
            """"""
            stderr:Alice;
            stderr:Bob;
            stderr:Charly;
            """"""
        And the command output should contain:
            """"""
            Feature:
                Scenario:
                    Given a step writes ""Alice"" to stderr and passes ... passed
                    When a step writes ""Bob"" to stderr and passes ... passed
                    Then a step writes ""Charly"" to stderr and passes ... passed
            """""""
/content/Record38.rst,"{
Scenario:Link:https://behave.readthedocs.io/en/latest/api/#logging-capture
Link2:https://behave.readthedocs.io/en/latest/api/#behave.runner.Context.captured
Logging Capture
The logging capture behave uses by default is implemented by the class LoggingCapture. It has methods

class behave.log_capture.LoggingCapture(config, level=None)
Capture logging events in a memory buffer for later display or query.

Captured logging events are stored on the attribute buffer:

buffer
This is a list of captured logging events as logging.LogRecords.

Given:By default the format of the messages will be:

'%(levelname)s:%(name)s:%(message)s'
This may be overridden using standard logging formatter names in the configuration variable logging_format.

The level of logging captured is set to logging.NOTSET by default. You may override this using the configuration setting logging_level (which is set to a level name.)

Finally there may be filtering of logging events specified by the configuration variable logging_filter.

abandon()
Turn off logging capture.

If other handlers were removed by inveigle() then they are reinstated.

any_errors()
Search through the buffer for any ERROR or CRITICAL events.

Returns boolean indicating whether a match was found.

find_event(pattern)
Search through the buffer for a message that matches the given regular expression.

Returns boolean indicating whether a match was found.

flush()
Override to implement custom flushing behaviour.

This version just zaps the buffer to empty.

inveigle()
Turn on logging capture by replacing all existing handlers configured in the logging module.

If the config var logging_clear_handlers is set then we also remove all existing handlers.

We also set the level of the root logger.

The opposite of this is abandon().

The log_capture module also defines a handy logging capture decorator that’s intended to be used on your environment file functions.

behave.log_capture.capture(*args, **kw)
Decorator to wrap an environment file function in log file capture.

It configures the logging capture using the behave context, the first argument to the function being decorated (so don’t use this to decorate something that doesn’t have context as the first argument).


Then:The basic usage is:

The function prints any captured logging (at the level determined by the log_level configuration setting) directly to stdout, regardless of error conditions.

It is mostly useful for debugging in situations where you are seeing a message like:

No handlers could be found for logger ""name""
The decorator takes an optional “level” keyword argument which limits the level of logging captured, overriding the level in the run’s configuration:

This would limit the logging captured to just ERROR and above, and thus only display logged events if they are interesting.
If any output capture is enabled, provides access to a Captured object that contains a snapshot of all captured data (stdout/stderr/log).
}",/content/Feature38.feature,"Scenario: All output is shown up to first failing step when --no-capture-stderr is used (CASE 2)
        When I run ""behave -f plain -T --no-capture-stderr features/capture_stderr.example2.feature""
        Then it should fail with:
            """"""
            0 scenarios passed, 1 failed, 0 skipped
            1 step passed, 1 failed, 1 skipped, 0 undefined
            """"""
        And the command output should contain:
            """"""
            Feature:
                Scenario:
                    Given a step writes ""Alice"" to stderr and passes ... passed
                    When a step writes ""Bob"" to stderr and fails ... failed
            Assertion Failed: XFAIL, step with: Bob;
            """"""
        And the command output should contain:
            """"""
            stderr:Alice;
            stderr:Bob;
            """"""
        But the command output should not contain:
            """"""
            stderr:Charly;
            """""""
/content/Record39.rst,"{
Scenario:
Link:https://behave.readthedocs.io/en/latest/api/#logging-capture
Link2:https://behave.readthedocs.io/en/latest/api/#behave.runner.Context.captured
Logging Capture
The logging capture behave uses by default is implemented by the class LoggingCapture. It has methods

class behave.log_capture.LoggingCapture(config, level=None)
Capture logging events in a memory buffer for later display or query.

Captured logging events are stored on the attribute buffer:

buffer
This is a list of captured logging events as logging.LogRecords.


Given:By default the format of the messages will be:

'%(levelname)s:%(name)s:%(message)s'
This may be overridden using standard logging formatter names in the configuration variable logging_format.

The level of logging captured is set to logging.NOTSET by default. You may override this using the configuration setting logging_level (which is set to a level name.)

Finally there may be filtering of logging events specified by the configuration variable logging_filter.

abandon()
Turn off logging capture.

If other handlers were removed by inveigle() then they are reinstated.

any_errors()
Search through the buffer for any ERROR or CRITICAL events.

Returns boolean indicating whether a match was found.

find_event(pattern)
Search through the buffer for a message that matches the given regular expression.

Returns boolean indicating whether a match was found.

flush()
Override to implement custom flushing behaviour.

This version just zaps the buffer to empty.

inveigle()
Turn on logging capture by replacing all existing handlers configured in the logging module.

If the config var logging_clear_handlers is set then we also remove all existing handlers.

We also set the level of the root logger.

The opposite of this is abandon().

The log_capture module also defines a handy logging capture decorator that’s intended to be used on your environment file functions.

behave.log_capture.capture(*args, **kw)
Decorator to wrap an environment file function in log file capture.

It configures the logging capture using the behave context, the first argument to the function being decorated (so don’t use this to decorate something that doesn’t have context as the first argument).


Then:The basic usage is:

The function prints any captured logging (at the level determined by the log_level configuration setting) directly to stdout, regardless of error conditions.

It is mostly useful for debugging in situations where you are seeing a message like:

No handlers could be found for logger ""name""
The decorator takes an optional “level” keyword argument which limits the level of logging captured, overriding the level in the run’s configuration:

This would limit the logging captured to just ERROR and above, and thus only display logged events if they are interesting.
If any output capture is enabled, provides access to a Captured object that contains a snapshot of all captured data (stdout/stderr/log).

}",/content/Feature39.feature,"Scenario: Test Setup
        Given a new working directory
        And a file named ""features/steps/stdout_steps.py"" with:
            """"""
            from behave import step
            import sys

            @step('a step writes ""{text}"" to stdout and passes')
            def step_writes_to_stdout_and_passes(context, text):
                sys.stdout.write(""stdout:%s;\n"" % text)

            @step('a step writes ""{text}"" to stdout and fails')
            def step_writes_to_stdout_and_fails(context, text):
                sys.stdout.write(""stdout:%s;\n"" % text)
                assert False, ""XFAIL, step with: %s;"" % text
            """"""
        And a file named ""features/capture_stdout.example1.feature"" with:
            """"""
            Feature:
                Scenario:
                    Given a step writes ""Alice"" to stdout and passes
                    When a step writes ""Bob"" to stdout and passes
                    Then a step writes ""Charly"" to stdout and passes
            """"""
        And a file named ""features/capture_stdout.example2.feature"" with:
            """"""
            Feature:
                Scenario:
                    Given a step writes ""Alice"" to stdout and passes
                    When a step writes ""Bob"" to stdout and fails
                    Then a step writes ""Charly"" to stdout and fails
            """"""

"
/content/Record40.rst,"{
Scenario:
Link:https://behave.readthedocs.io/en/latest/api/#logging-capture
Link2:https://behave.readthedocs.io/en/latest/api/#behave.runner.Context.captured
Logging Capture
The logging capture behave uses by default is implemented by the class LoggingCapture. It has methods

class behave.log_capture.LoggingCapture(config, level=None)
Capture logging events in a memory buffer for later display or query.

Captured logging events are stored on the attribute buffer:

buffer
This is a list of captured logging events as logging.LogRecords.


Given:By default the format of the messages will be:

'%(levelname)s:%(name)s:%(message)s'
This may be overridden using standard logging formatter names in the configuration variable logging_format.

The level of logging captured is set to logging.NOTSET by default. You may override this using the configuration setting logging_level (which is set to a level name.)

Finally there may be filtering of logging events specified by the configuration variable logging_filter.

abandon()
Turn off logging capture.

If other handlers were removed by inveigle() then they are reinstated.

any_errors()
Search through the buffer for any ERROR or CRITICAL events.

Returns boolean indicating whether a match was found.

find_event(pattern)
Search through the buffer for a message that matches the given regular expression.

Returns boolean indicating whether a match was found.

flush()
Override to implement custom flushing behaviour.

This version just zaps the buffer to empty.

inveigle()
Turn on logging capture by replacing all existing handlers configured in the logging module.

If the config var logging_clear_handlers is set then we also remove all existing handlers.

We also set the level of the root logger.

The opposite of this is abandon().

The log_capture module also defines a handy logging capture decorator that’s intended to be used on your environment file functions.

behave.log_capture.capture(*args, **kw)
Decorator to wrap an environment file function in log file capture.

It configures the logging capture using the behave context, the first argument to the function being decorated (so don’t use this to decorate something that doesn’t have context as the first argument).


Then:The basic usage is:

The function prints any captured logging (at the level determined by the log_level configuration setting) directly to stdout, regardless of error conditions.

It is mostly useful for debugging in situations where you are seeing a message like:

No handlers could be found for logger ""name""
The decorator takes an optional “level” keyword argument which limits the level of logging captured, overriding the level in the run’s configuration:

This would limit the logging captured to just ERROR and above, and thus only display logged events if they are interesting.
If any output capture is enabled, provides access to a Captured object that contains a snapshot of all captured data (stdout/stderr/log).

}",/content/Feature40.feature," Scenario: Captured output is suppressed if scenario passes
        When I run ""behave -f plain -T --capture features/capture_stdout.example1.feature""
        Then it should pass
        And the command output should contain:
            """"""
            Feature:
                Scenario:
                    Given a step writes ""Alice"" to stdout and passes ... passed
                    When a step writes ""Bob"" to stdout and passes ... passed
                    Then a step writes ""Charly"" to stdout and passes ... passed
            """"""
        But the command output should not contain:
            """"""
            stdout:Alice;
            """""""
/content/Record41.rst,"{
Scenario:
Link:https://behave.readthedocs.io/en/latest/api/#logging-capture
Link2:https://behave.readthedocs.io/en/latest/api/#behave.runner.Context.captured
Logging Capture
The logging capture behave uses by default is implemented by the class LoggingCapture. It has methods

class behave.log_capture.LoggingCapture(config, level=None)
Capture logging events in a memory buffer for later display or query.

Captured logging events are stored on the attribute buffer:

buffer
This is a list of captured logging events as logging.LogRecords.


Given:By default the format of the messages will be:

'%(levelname)s:%(name)s:%(message)s'
This may be overridden using standard logging formatter names in the configuration variable logging_format.

The level of logging captured is set to logging.NOTSET by default. You may override this using the configuration setting logging_level (which is set to a level name.)

Finally there may be filtering of logging events specified by the configuration variable logging_filter.

abandon()
Turn off logging capture.

If other handlers were removed by inveigle() then they are reinstated.

any_errors()
Search through the buffer for any ERROR or CRITICAL events.

Returns boolean indicating whether a match was found.

find_event(pattern)
Search through the buffer for a message that matches the given regular expression.

Returns boolean indicating whether a match was found.

flush()
Override to implement custom flushing behaviour.

This version just zaps the buffer to empty.

inveigle()
Turn on logging capture by replacing all existing handlers configured in the logging module.

If the config var logging_clear_handlers is set then we also remove all existing handlers.

We also set the level of the root logger.

The opposite of this is abandon().

The log_capture module also defines a handy logging capture decorator that’s intended to be used on your environment file functions.

behave.log_capture.capture(*args, **kw)
Decorator to wrap an environment file function in log file capture.

It configures the logging capture using the behave context, the first argument to the function being decorated (so don’t use this to decorate something that doesn’t have context as the first argument).


Then:The basic usage is:

The function prints any captured logging (at the level determined by the log_level configuration setting) directly to stdout, regardless of error conditions.

It is mostly useful for debugging in situations where you are seeing a message like:

No handlers could be found for logger ""name""
The decorator takes an optional “level” keyword argument which limits the level of logging captured, overriding the level in the run’s configuration:

This would limit the logging captured to just ERROR and above, and thus only display logged events if they are interesting.
If any output capture is enabled, provides access to a Captured object that contains a snapshot of all captured data (stdout/stderr/log).

}",/content/Feature41.feature," Scenario: Captured output is shown up to first failure if scenario fails
        When I run ""behave -f plain -T --capture features/capture_stdout.example2.feature""
        Then it should fail with:
            """"""
            0 scenarios passed, 1 failed, 0 skipped
            1 step passed, 1 failed, 1 skipped, 0 undefined
            """"""
        And the command output should contain:
            """"""
            Feature:
                Scenario:
                    Given a step writes ""Alice"" to stdout and passes ... passed
                    When a step writes ""Bob"" to stdout and fails ... failed
            Assertion Failed: XFAIL, step with: Bob;
            Captured stdout:
            stdout:Alice;
            stdout:Bob;
            """"""
        But the command output should not contain:
            """"""
            stdout:Charly;
            """""""
/content/Record42.rst,"{
Scenario:
Link:https://behave.readthedocs.io/en/latest/api/#logging-capture
Link2:https://behave.readthedocs.io/en/latest/api/#behave.runner.Context.captured
Logging Capture
The logging capture behave uses by default is implemented by the class LoggingCapture. It has methods

class behave.log_capture.LoggingCapture(config, level=None)
Capture logging events in a memory buffer for later display or query.

Captured logging events are stored on the attribute buffer:

buffer
This is a list of captured logging events as logging.LogRecords.


Given:By default the format of the messages will be:

'%(levelname)s:%(name)s:%(message)s'
This may be overridden using standard logging formatter names in the configuration variable logging_format.

The level of logging captured is set to logging.NOTSET by default. You may override this using the configuration setting logging_level (which is set to a level name.)

Finally there may be filtering of logging events specified by the configuration variable logging_filter.

abandon()
Turn off logging capture.

If other handlers were removed by inveigle() then they are reinstated.

any_errors()
Search through the buffer for any ERROR or CRITICAL events.

Returns boolean indicating whether a match was found.

find_event(pattern)
Search through the buffer for a message that matches the given regular expression.

Returns boolean indicating whether a match was found.

flush()
Override to implement custom flushing behaviour.

This version just zaps the buffer to empty.

inveigle()
Turn on logging capture by replacing all existing handlers configured in the logging module.

If the config var logging_clear_handlers is set then we also remove all existing handlers.

We also set the level of the root logger.

The opposite of this is abandon().

The log_capture module also defines a handy logging capture decorator that’s intended to be used on your environment file functions.

behave.log_capture.capture(*args, **kw)
Decorator to wrap an environment file function in log file capture.

It configures the logging capture using the behave context, the first argument to the function being decorated (so don’t use this to decorate something that doesn’t have context as the first argument).


Then:The basic usage is:

The function prints any captured logging (at the level determined by the log_level configuration setting) directly to stdout, regardless of error conditions.

It is mostly useful for debugging in situations where you are seeing a message like:

No handlers could be found for logger ""name""
The decorator takes an optional “level” keyword argument which limits the level of logging captured, overriding the level in the run’s configuration:

This would limit the logging captured to just ERROR and above, and thus only display logged events if they are interesting.
If any output capture is enabled, provides access to a Captured object that contains a snapshot of all captured data (stdout/stderr/log).

}",/content/Feature42.feature," Scenario: All output is shown when --no-capture is used and all steps pass (CASE 1)
        When I run ""behave -f plain -T --no-capture features/capture_stdout.example1.feature""
        Then it should pass
        And the command output should contain:
            """"""
            Feature:
                Scenario:
                    stdout:Alice;
                    Given a step writes ""Alice"" to stdout and passes ... passed
                    stdout:Bob;
                    When a step writes ""Bob"" to stdout and passes ... passed
                    stdout:Charly;
                    Then a step writes ""Charly"" to stdout and passes ... passed
            """""""
/content/Record43.rst,"{
Scenario:
Link:https://behave.readthedocs.io/en/latest/api/#logging-capture
Link2:https://behave.readthedocs.io/en/latest/api/#behave.runner.Context.captured
Logging Capture
The logging capture behave uses by default is implemented by the class LoggingCapture. It has methods

class behave.log_capture.LoggingCapture(config, level=None)
Capture logging events in a memory buffer for later display or query.

Captured logging events are stored on the attribute buffer:

buffer
This is a list of captured logging events as logging.LogRecords.


Given:By default the format of the messages will be:

'%(levelname)s:%(name)s:%(message)s'
This may be overridden using standard logging formatter names in the configuration variable logging_format.

The level of logging captured is set to logging.NOTSET by default. You may override this using the configuration setting logging_level (which is set to a level name.)

Finally there may be filtering of logging events specified by the configuration variable logging_filter.

abandon()
Turn off logging capture.

If other handlers were removed by inveigle() then they are reinstated.

any_errors()
Search through the buffer for any ERROR or CRITICAL events.

Returns boolean indicating whether a match was found.

find_event(pattern)
Search through the buffer for a message that matches the given regular expression.

Returns boolean indicating whether a match was found.

flush()
Override to implement custom flushing behaviour.

This version just zaps the buffer to empty.

inveigle()
Turn on logging capture by replacing all existing handlers configured in the logging module.

If the config var logging_clear_handlers is set then we also remove all existing handlers.

We also set the level of the root logger.

The opposite of this is abandon().

The log_capture module also defines a handy logging capture decorator that’s intended to be used on your environment file functions.

behave.log_capture.capture(*args, **kw)
Decorator to wrap an environment file function in log file capture.

It configures the logging capture using the behave context, the first argument to the function being decorated (so don’t use this to decorate something that doesn’t have context as the first argument).


Then:The basic usage is:

The function prints any captured logging (at the level determined by the log_level configuration setting) directly to stdout, regardless of error conditions.

It is mostly useful for debugging in situations where you are seeing a message like:

No handlers could be found for logger ""name""
The decorator takes an optional “level” keyword argument which limits the level of logging captured, overriding the level in the run’s configuration:

This would limit the logging captured to just ERROR and above, and thus only display logged events if they are interesting.
If any output capture is enabled, provides access to a Captured object that contains a snapshot of all captured data (stdout/stderr/log).

}",/content/Feature43.feature,"Scenario: All output is shown up to first failing step when --no-capture is used (CASE 2)
        When I run ""behave -f plain -T --no-capture features/capture_stdout.example2.feature""
        Then it should fail with:
            """"""
            0 scenarios passed, 1 failed, 0 skipped
            1 step passed, 1 failed, 1 skipped, 0 undefined
            """"""
        And the command output should contain:
            """"""
            Feature:
                Scenario:
                    stdout:Alice;
                    Given a step writes ""Alice"" to stdout and passes ... passed
                    stdout:Bob;
                    When a step writes ""Bob"" to stdout and fails ... failed
            """"""
        But the command output should not contain:
            """"""
            stdout:Charly;
            """""""
/content/Record44.rst,"{
Scenario:Link:https://behave.readthedocs.io/en/latest/behave/#command-line-arguments
Command-Line Arguments
You may see the same information presented below at any time using behave -h.


Given:-C, --no-color
Disable colored mode.

--color
Use colored mode or not (default: auto).

-d, --dry-run
Invokes formatters without executing the steps.

-D, --define
Define user-specific data for the config.userdata dictionary. Example: -D foo=bar to store it in config.userdata[“foo”].

-e, --exclude
Don’t run feature files matching regular expression PATTERN.

-i, --include
Only run feature files matching regular expression PATTERN.

--no-junit
Don’t output JUnit-compatible reports.

--junit
Output JUnit-compatible reports. When junit is enabled, all stdout and stderr will be redirected and dumped to the junit report, regardless of the “–capture” and “–no-capture” options.

--junit-directory
Directory in which to store JUnit reports.

-j, --jobs, --parallel
Number of concurrent jobs to use (default: 1). Only supported by test runners that support parallel execution.

-f, --format
Specify a formatter. If none is specified the default formatter is used. Pass “–format help” to get a list of available formatters.

--steps-catalog
Show a catalog of all available step definitions. SAME AS: –format=steps.catalog –dry-run –no-summary -q

--no-skipped
Don’t print skipped steps (due to tags).

--show-skipped
Print skipped steps. This is the default behaviour. This switch is used to override a configuration file setting.

--no-snippets
Don’t print snippets for unimplemented steps.

--snippets
Print snippets for unimplemented steps. This is the default behaviour. This switch is used to override a configuration file setting.

--no-multiline
Don’t print multiline strings and tables under steps.

--multiline
Print multiline strings and tables under steps. This is the default behaviour. This switch is used to override a configuration file setting.

-n, --name
Select feature elements (scenarios, …) to run which match part of the given name (regex pattern). If this option is given more than once, it will match against all the given names.

--no-capture
Don’t capture stdout (any stdout output will be printed immediately.)

--capture
Capture stdout (any stdout output will be printed if there is a failure.) This is the default behaviour. This switch is used to override a configuration file setting.

--no-capture-stderr
Don’t capture stderr (any stderr output will be printed immediately.)

--capture-stderr
Capture stderr (any stderr output will be printed if there is a failure.) This is the default behaviour. This switch is used to override a configuration file setting.

--no-logcapture
Don’t capture logging. Logging configuration will be left intact.

--logcapture
Capture logging. All logging during a step will be captured and displayed in the event of a failure. This is the default behaviour. This switch is used to override a configuration file setting.

--logging-level
Specify a level to capture logging at. The default is INFO - capturing everything.

--logging-format
Specify custom format to print statements. Uses the same format as used by standard logging handlers. The default is “%(levelname)s:%(name)s:%(message)s”.

--logging-datefmt
Specify custom date/time format to print statements. Uses the same format as used by standard logging handlers.

--logging-filter
Specify which statements to filter in/out. By default, everything is captured. If the output is too verbose, use this option to filter out needless output. Example: –logging-filter=foo will capture statements issued ONLY to foo or foo.what.ever.sub but not foobar or other logger. Specify multiple loggers with comma: filter=foo,bar,baz. If any logger name is prefixed with a minus, eg filter=-foo, it will be excluded rather than included.

--logging-clear-handlers
Clear all other logging handlers.

--no-summary
Don’t display the summary at the end of the run.

--summary
Display the summary at the end of the run.

-o, --outfile
Write to specified file instead of stdout.

-q, --quiet
Alias for –no-snippets –no-source.

-r, --runner
Use own runner class, like: “behave.runner:Runner”

--no-source
Don’t print the file and line of the step definition with the steps.

--show-source
Print the file and line of the step definition with the steps. This is the default behaviour. This switch is used to override a configuration file setting.

--stage
Defines the current test stage. The test stage name is used as name prefix for the environment file and the steps directory (instead of default path names).

--stop
Stop running tests at the first failure.

-t, --tags
Only execute features or scenarios with tags matching TAG_EXPRESSION. Pass “–tags-help” for more information.

-T, --no-timings
Don’t print the time taken for each step.

--show-timings
Print the time taken, in seconds, of each step after the step has completed. This is the default behaviour. This switch is used to override a configuration file setting.

-v, --verbose
Show the files and features loaded.

-w, --wip
Only run scenarios tagged with “wip”. Additionally: use the “plain” formatter, do not capture stdout or logging output and stop at the first failure.

--lang
Use keywords for a language other than English.

--lang-list
List the languages available for –lang.

--lang-help
List the translations accepted for one language.

--tags-help
Show help for tag expressions.

--version
Show version.



Then:Refer to descriptive under command-line-arguments

}",/content/Feature44.feature,"Scenario: Use behave --lang-list
    When I run ""behave --lang-list""
    Then it should pass with:
        """"""
        AVAILABLE LANGUAGES:
          af: Afrikaans / Afrikaans
          am: հայերեն / Armenian
          amh: አማርኛ / Amharic
          an: Aragonés / Aragonese
          ar: العربية / Arabic
          ast: asturianu / Asturian
          az: Azərbaycanca / Azerbaijani
          be: Беларуская / Belarusian
          bg: български / Bulgarian
          bm: Bahasa Melayu / Malay
          bs: Bosanski / Bosnian
          ca: català / Catalan
          cs: Česky / Czech
          cy-GB: Cymraeg / Welsh
          da: dansk / Danish
          de: Deutsch / German
          el: Ελληνικά / Greek
          em: 😀 / Emoji
          en: English / English
          en-Scouse: Scouse / Scouse
          en-au: Australian / Australian
          en-lol: LOLCAT / LOLCAT
          en-old: Englisc / Old English
          en-pirate: Pirate / Pirate
          en-tx: Texas / Texas
          eo: Esperanto / Esperanto
          es: español / Spanish
          et: eesti keel / Estonian
          fa: فارسی / Persian
          fi: suomi / Finnish
          fr: français / French
          ga: Gaeilge / Irish
          gj: ગુજરાતી / Gujarati
          gl: galego / Galician
          he: עברית / Hebrew
          hi: हिंदी / Hindi
          hr: hrvatski / Croatian
          ht: kreyòl / Creole
          hu: magyar / Hungarian
          id: Bahasa Indonesia / Indonesian
          is: Íslenska / Icelandic
          it: italiano / Italian
          ja: 日本語 / Japanese
          jv: Basa Jawa / Javanese
          ka: ქართული / Georgian
          kn: ಕನ್ನಡ / Kannada
          ko: 한국어 / Korean
          lt: lietuvių kalba / Lithuanian
          lu: Lëtzebuergesch / Luxemburgish
          lv: latviešu / Latvian
          mk-Cyrl: Македонски / Macedonian
          mk-Latn: Makedonski (Latinica) / Macedonian (Latin)
          mn: монгол / Mongolian
          mr: मराठी / Marathi
          ne: नेपाली / Nepali
          nl: Nederlands / Dutch
          no: norsk / Norwegian
          pa: ਪੰਜਾਬੀ / Panjabi
          pl: polski / Polish
          pt: português / Portuguese
          ro: română / Romanian
          ru: русский / Russian
          sk: Slovensky / Slovak
          sl: Slovenski / Slovenian
          sr-Cyrl: Српски / Serbian
          sr-Latn: Srpski (Latinica) / Serbian (Latin)
          sv: Svenska / Swedish
          ta: தமிழ் / Tamil
          te: తెలుగు / Telugu
          th: ไทย / Thai
          tlh: tlhIngan / Klingon
          tr: Türkçe / Turkish
          tt: Татарча / Tatar
          uk: Українська / Ukrainian
          ur: اردو / Urdu
          uz: Узбекча / Uzbek
          vi: Tiếng Việt / Vietnamese
          zh-CN: 简体中文 / Chinese simplified
          zh-TW: 繁體中文 / Chinese traditional
        """"""
    But the command output should not contain ""Traceback""
"
/content/Record45.rst,"{

Scenario:Link:https://behave.readthedocs.io/en/latest/behave/#configuration-files
Configuration Files
Configuration files for behave are called either “.behaverc”, “behave.ini”, “setup.cfg”, “tox.ini”, or “pyproject.toml” (your preference) and are located in one of three places:

the current working directory (good for per-project settings),

your home directory ($HOME), or

on Windows, in the %APPDATA% directory.

If you are wondering where behave is getting its configuration defaults from you can use the “-v” command-line argument and it’ll tell you.

Configuration files must start with the label “[behave]” and are formatted in the Windows INI style, for example:

[behave]
format=plain
logging_clear_handlers=yes
logging_filter=-suds
Alternatively, if using “pyproject.toml” instead (note the “tool.” prefix):

[tool.behave]
format = ""plain""
logging_clear_handlers = true
logging_filter = ""-suds""
NOTE: toml does not support ‘%’ interpolations.


Given:Configuration Parameter Types
The following types are supported (and used):

text
This just assigns whatever text you supply to the configuration setting.

bool
This assigns a boolean value to the configuration setting. The text describes the functionality when the value is true. True values are “1”, “yes”, “true”, and “on”. False values are “0”, “no”, “false”, and “off”. TOML: toml only accepts its native true

sequence<text>
These fields accept one or more values on new lines, for example a tag expression might look like:

default_tags= (@foo or not @bar) and @zap
which is the equivalent of the command-line usage:

--tags=""(@foo or not @bar) and @zap""
TOML: toml can use arrays natively.

Configuration Parameters
color : Colored (Enum)
Use colored mode or not (default: auto).

dry_run : bool
Invokes formatters without executing the steps.

userdata_defines : sequence<text>
Define user-specific data for the config.userdata dictionary. Example: -D foo=bar to store it in config.userdata[“foo”].

exclude_re : text
Don’t run feature files matching regular expression PATTERN.

include_re : text
Only run feature files matching regular expression PATTERN.

junit : bool
Output JUnit-compatible reports. When junit is enabled, all stdout and stderr will be redirected and dumped to the junit report, regardless of the “–capture” and “–no-capture” options.

junit_directory : text
Directory in which to store JUnit reports.

jobs : positive_number
Number of concurrent jobs to use (default: 1). Only supported by test runners that support parallel execution.

default_format : text
Specify default formatter (default: pretty).

format : sequence<text>
Specify a formatter. If none is specified the default formatter is used. Pass “–format help” to get a list of available formatters.

steps_catalog : bool
Show a catalog of all available step definitions. SAME AS: –format=steps.catalog –dry-run –no-summary -q

scenario_outline_annotation_schema : text
Specify name annotation schema for scenario outline (default=”{name} – @{row.id} {examples.name}”).

show_skipped : bool
Print skipped steps. This is the default behaviour. This switch is used to override a configuration file setting.

show_snippets : bool
Print snippets for unimplemented steps. This is the default behaviour. This switch is used to override a configuration file setting.

show_multiline : bool
Print multiline strings and tables under steps. This is the default behaviour. This switch is used to override a configuration file setting.

name : sequence<text>
Select feature elements (scenarios, …) to run which match part of the given name (regex pattern). If this option is given more than once, it will match against all the given names.

stdout_capture : bool
Capture stdout (any stdout output will be printed if there is a failure.) This is the default behaviour. This switch is used to override a configuration file setting.

stderr_capture : bool
Capture stderr (any stderr output will be printed if there is a failure.) This is the default behaviour. This switch is used to override a configuration file setting.

log_capture : bool
Capture logging. All logging during a step will be captured and displayed in the event of a failure. This is the default behaviour. This switch is used to override a configuration file setting.

logging_level : text
Specify a level to capture logging at. The default is INFO - capturing everything.

logging_format : text
Specify custom format to print statements. Uses the same format as used by standard logging handlers. The default is “%(levelname)s:%(name)s:%(message)s”.

logging_datefmt : text
Specify custom date/time format to print statements. Uses the same format as used by standard logging handlers.

logging_filter : text
Specify which statements to filter in/out. By default, everything is captured. If the output is too verbose, use this option to filter out needless output. Example: logging_filter = foo will capture statements issued ONLY to “foo” or “foo.what.ever.sub” but not “foobar” or other logger. Specify multiple loggers with comma: logging_filter = foo,bar,baz. If any logger name is prefixed with a minus, eg logging_filter = -foo, it will be excluded rather than included.

logging_clear_handlers : bool
Clear all other logging handlers.

summary : bool
Display the summary at the end of the run.

outfiles : sequence<text>
Write to specified file instead of stdout.

paths : sequence<text>
Specify default feature paths, used when none are provided.

tag_expression_protocol : TagExpressionProtocol (Enum)
Specify the tag-expression protocol to use (default: auto_detect). With “v1”, only tag-expressions v1 are supported. With “v2”, only tag-expressions v2 are supported. With “auto_detect”, tag- expressions v1 and v2 are auto-detected.

quiet : bool
Alias for –no-snippets –no-source.

runner : text
Use own runner class, like: “behave.runner:Runner”

show_source : bool
Print the file and line of the step definition with the steps. This is the default behaviour. This switch is used to override a configuration file setting.

stage : text
Defines the current test stage. The test stage name is used as name prefix for the environment file and the steps directory (instead of default path names).

stop : bool
Stop running tests at the first failure.

default_tags : sequence<text>
Define default tags when non are provided. See –tags for more information.

tags : sequence<text>
Only execute certain features or scenarios based on the tag expression given. See below for how to code tag expressions in configuration files.

show_timings : bool
Print the time taken, in seconds, of each step after the step has completed. This is the default behaviour. This switch is used to override a configuration file setting.

verbose : bool
Show the files and features loaded.

wip : bool
Only run scenarios tagged with “wip”. Additionally: use the “plain” formatter, do not capture stdout or logging output and stop at the first failure.

lang : text
Use keywords for a language other than English.

Then:Refer to description under command-line arguments

}",/content/Feature45.feature,"Scenario: Test Setup
        Given a new working directory
        And a file named ""features/steps/steps.py"" with:
            """"""
            from behave import step

            @step('a step passes')
            def step_passes(context):
                pass
            """"""
        And a file named ""features/alice.feature"" with:
            """"""
            Feature: Alice
                Scenario: A1
                    Given a step passes
                    When a step passes
                    Then a step passes
            """"""
        And a file named ""features/bob.feature"" with:
            """"""
            Feature: Bob
                Scenario: B1
                    When a step passes
            """"""
        And a file named ""more.features/charly.feature"" with:
            """"""
            Feature: Charly
                Scenario: C1
                    Then a step passes
            """"""
        And a file named ""behave.ini"" with:
            """"""
            [behave]
            show_timings = false
            paths = features/bob.feature
                    more.features/charly.feature
            """"""



"
/content/Record46.rst,"{

Scenario:Link:https://behave.readthedocs.io/en/latest/behave/#configuration-files
Configuration Files
Configuration files for behave are called either “.behaverc”, “behave.ini”, “setup.cfg”, “tox.ini”, or “pyproject.toml” (your preference) and are located in one of three places:

the current working directory (good for per-project settings),

your home directory ($HOME), or

on Windows, in the %APPDATA% directory.

If you are wondering where behave is getting its configuration defaults from you can use the “-v” command-line argument and it’ll tell you.

Configuration files must start with the label “[behave]” and are formatted in the Windows INI style, for example:

[behave]
format=plain
logging_clear_handlers=yes
logging_filter=-suds
Alternatively, if using “pyproject.toml” instead (note the “tool.” prefix):

[tool.behave]
format = ""plain""
logging_clear_handlers = true
logging_filter = ""-suds""
NOTE: toml does not support ‘%’ interpolations.


Given:Configuration Parameter Types
The following types are supported (and used):

text
This just assigns whatever text you supply to the configuration setting.

bool
This assigns a boolean value to the configuration setting. The text describes the functionality when the value is true. True values are “1”, “yes”, “true”, and “on”. False values are “0”, “no”, “false”, and “off”. TOML: toml only accepts its native true

sequence<text>
These fields accept one or more values on new lines, for example a tag expression might look like:

default_tags= (@foo or not @bar) and @zap
which is the equivalent of the command-line usage:

--tags=""(@foo or not @bar) and @zap""
TOML: toml can use arrays natively.

Configuration Parameters
color : Colored (Enum)
Use colored mode or not (default: auto).

dry_run : bool
Invokes formatters without executing the steps.

userdata_defines : sequence<text>
Define user-specific data for the config.userdata dictionary. Example: -D foo=bar to store it in config.userdata[“foo”].

exclude_re : text
Don’t run feature files matching regular expression PATTERN.

include_re : text
Only run feature files matching regular expression PATTERN.

junit : bool
Output JUnit-compatible reports. When junit is enabled, all stdout and stderr will be redirected and dumped to the junit report, regardless of the “–capture” and “–no-capture” options.

junit_directory : text
Directory in which to store JUnit reports.

jobs : positive_number
Number of concurrent jobs to use (default: 1). Only supported by test runners that support parallel execution.

default_format : text
Specify default formatter (default: pretty).

format : sequence<text>
Specify a formatter. If none is specified the default formatter is used. Pass “–format help” to get a list of available formatters.

steps_catalog : bool
Show a catalog of all available step definitions. SAME AS: –format=steps.catalog –dry-run –no-summary -q

scenario_outline_annotation_schema : text
Specify name annotation schema for scenario outline (default=”{name} – @{row.id} {examples.name}”).

show_skipped : bool
Print skipped steps. This is the default behaviour. This switch is used to override a configuration file setting.

show_snippets : bool
Print snippets for unimplemented steps. This is the default behaviour. This switch is used to override a configuration file setting.

show_multiline : bool
Print multiline strings and tables under steps. This is the default behaviour. This switch is used to override a configuration file setting.

name : sequence<text>
Select feature elements (scenarios, …) to run which match part of the given name (regex pattern). If this option is given more than once, it will match against all the given names.

stdout_capture : bool
Capture stdout (any stdout output will be printed if there is a failure.) This is the default behaviour. This switch is used to override a configuration file setting.

stderr_capture : bool
Capture stderr (any stderr output will be printed if there is a failure.) This is the default behaviour. This switch is used to override a configuration file setting.

log_capture : bool
Capture logging. All logging during a step will be captured and displayed in the event of a failure. This is the default behaviour. This switch is used to override a configuration file setting.

logging_level : text
Specify a level to capture logging at. The default is INFO - capturing everything.

logging_format : text
Specify custom format to print statements. Uses the same format as used by standard logging handlers. The default is “%(levelname)s:%(name)s:%(message)s”.

logging_datefmt : text
Specify custom date/time format to print statements. Uses the same format as used by standard logging handlers.

logging_filter : text
Specify which statements to filter in/out. By default, everything is captured. If the output is too verbose, use this option to filter out needless output. Example: logging_filter = foo will capture statements issued ONLY to “foo” or “foo.what.ever.sub” but not “foobar” or other logger. Specify multiple loggers with comma: logging_filter = foo,bar,baz. If any logger name is prefixed with a minus, eg logging_filter = -foo, it will be excluded rather than included.

logging_clear_handlers : bool
Clear all other logging handlers.

summary : bool
Display the summary at the end of the run.

outfiles : sequence<text>
Write to specified file instead of stdout.

paths : sequence<text>
Specify default feature paths, used when none are provided.

tag_expression_protocol : TagExpressionProtocol (Enum)
Specify the tag-expression protocol to use (default: auto_detect). With “v1”, only tag-expressions v1 are supported. With “v2”, only tag-expressions v2 are supported. With “auto_detect”, tag- expressions v1 and v2 are auto-detected.

quiet : bool
Alias for –no-snippets –no-source.

runner : text
Use own runner class, like: “behave.runner:Runner”

show_source : bool
Print the file and line of the step definition with the steps. This is the default behaviour. This switch is used to override a configuration file setting.

stage : text
Defines the current test stage. The test stage name is used as name prefix for the environment file and the steps directory (instead of default path names).

stop : bool
Stop running tests at the first failure.

default_tags : sequence<text>
Define default tags when non are provided. See –tags for more information.

tags : sequence<text>
Only execute certain features or scenarios based on the tag expression given. See below for how to code tag expressions in configuration files.

show_timings : bool
Print the time taken, in seconds, of each step after the step has completed. This is the default behaviour. This switch is used to override a configuration file setting.

verbose : bool
Show the files and features loaded.

wip : bool
Only run scenarios tagged with “wip”. Additionally: use the “plain” formatter, do not capture stdout or logging output and stop at the first failure.

lang : text
Use keywords for a language other than English.

Then:Refer to description under command-line arguments

}",/content/Feature46.feature," Scenario: Used default paths from behave configfile
        When I run ""behave -f plain""
        Then it should pass with:
            """"""
            2 features passed, 0 failed, 0 skipped
            2 scenarios passed, 0 failed, 0 skipped
            """"""
        And the command output should contain:
            """"""
            Feature: Bob
                Scenario: B1
                    When a step passes ... passed

            Feature: Charly
                Scenario: C1
                    Then a step passes ... passed
            """"""
        But the command output should not contain:
            """"""
            Feature: Alice
            """""""
/content/Record47.rst,"{

Scenario:Link:https://behave.readthedocs.io/en/latest/behave/#configuration-files
Configuration Files
Configuration files for behave are called either “.behaverc”, “behave.ini”, “setup.cfg”, “tox.ini”, or “pyproject.toml” (your preference) and are located in one of three places:

the current working directory (good for per-project settings),

your home directory ($HOME), or

on Windows, in the %APPDATA% directory.

If you are wondering where behave is getting its configuration defaults from you can use the “-v” command-line argument and it’ll tell you.

Configuration files must start with the label “[behave]” and are formatted in the Windows INI style, for example:

[behave]
format=plain
logging_clear_handlers=yes
logging_filter=-suds
Alternatively, if using “pyproject.toml” instead (note the “tool.” prefix):

[tool.behave]
format = ""plain""
logging_clear_handlers = true
logging_filter = ""-suds""
NOTE: toml does not support ‘%’ interpolations.


Given:Configuration Parameter Types
The following types are supported (and used):

text
This just assigns whatever text you supply to the configuration setting.

bool
This assigns a boolean value to the configuration setting. The text describes the functionality when the value is true. True values are “1”, “yes”, “true”, and “on”. False values are “0”, “no”, “false”, and “off”. TOML: toml only accepts its native true

sequence<text>
These fields accept one or more values on new lines, for example a tag expression might look like:

default_tags= (@foo or not @bar) and @zap
which is the equivalent of the command-line usage:

--tags=""(@foo or not @bar) and @zap""
TOML: toml can use arrays natively.

Configuration Parameters
color : Colored (Enum)
Use colored mode or not (default: auto).

dry_run : bool
Invokes formatters without executing the steps.

userdata_defines : sequence<text>
Define user-specific data for the config.userdata dictionary. Example: -D foo=bar to store it in config.userdata[“foo”].

exclude_re : text
Don’t run feature files matching regular expression PATTERN.

include_re : text
Only run feature files matching regular expression PATTERN.

junit : bool
Output JUnit-compatible reports. When junit is enabled, all stdout and stderr will be redirected and dumped to the junit report, regardless of the “–capture” and “–no-capture” options.

junit_directory : text
Directory in which to store JUnit reports.

jobs : positive_number
Number of concurrent jobs to use (default: 1). Only supported by test runners that support parallel execution.

default_format : text
Specify default formatter (default: pretty).

format : sequence<text>
Specify a formatter. If none is specified the default formatter is used. Pass “–format help” to get a list of available formatters.

steps_catalog : bool
Show a catalog of all available step definitions. SAME AS: –format=steps.catalog –dry-run –no-summary -q

scenario_outline_annotation_schema : text
Specify name annotation schema for scenario outline (default=”{name} – @{row.id} {examples.name}”).

show_skipped : bool
Print skipped steps. This is the default behaviour. This switch is used to override a configuration file setting.

show_snippets : bool
Print snippets for unimplemented steps. This is the default behaviour. This switch is used to override a configuration file setting.

show_multiline : bool
Print multiline strings and tables under steps. This is the default behaviour. This switch is used to override a configuration file setting.

name : sequence<text>
Select feature elements (scenarios, …) to run which match part of the given name (regex pattern). If this option is given more than once, it will match against all the given names.

stdout_capture : bool
Capture stdout (any stdout output will be printed if there is a failure.) This is the default behaviour. This switch is used to override a configuration file setting.

stderr_capture : bool
Capture stderr (any stderr output will be printed if there is a failure.) This is the default behaviour. This switch is used to override a configuration file setting.

log_capture : bool
Capture logging. All logging during a step will be captured and displayed in the event of a failure. This is the default behaviour. This switch is used to override a configuration file setting.

logging_level : text
Specify a level to capture logging at. The default is INFO - capturing everything.

logging_format : text
Specify custom format to print statements. Uses the same format as used by standard logging handlers. The default is “%(levelname)s:%(name)s:%(message)s”.

logging_datefmt : text
Specify custom date/time format to print statements. Uses the same format as used by standard logging handlers.

logging_filter : text
Specify which statements to filter in/out. By default, everything is captured. If the output is too verbose, use this option to filter out needless output. Example: logging_filter = foo will capture statements issued ONLY to “foo” or “foo.what.ever.sub” but not “foobar” or other logger. Specify multiple loggers with comma: logging_filter = foo,bar,baz. If any logger name is prefixed with a minus, eg logging_filter = -foo, it will be excluded rather than included.

logging_clear_handlers : bool
Clear all other logging handlers.

summary : bool
Display the summary at the end of the run.

outfiles : sequence<text>
Write to specified file instead of stdout.

paths : sequence<text>
Specify default feature paths, used when none are provided.

tag_expression_protocol : TagExpressionProtocol (Enum)
Specify the tag-expression protocol to use (default: auto_detect). With “v1”, only tag-expressions v1 are supported. With “v2”, only tag-expressions v2 are supported. With “auto_detect”, tag- expressions v1 and v2 are auto-detected.

quiet : bool
Alias for –no-snippets –no-source.

runner : text
Use own runner class, like: “behave.runner:Runner”

show_source : bool
Print the file and line of the step definition with the steps. This is the default behaviour. This switch is used to override a configuration file setting.

stage : text
Defines the current test stage. The test stage name is used as name prefix for the environment file and the steps directory (instead of default path names).

stop : bool
Stop running tests at the first failure.

default_tags : sequence<text>
Define default tags when non are provided. See –tags for more information.

tags : sequence<text>
Only execute certain features or scenarios based on the tag expression given. See below for how to code tag expressions in configuration files.

show_timings : bool
Print the time taken, in seconds, of each step after the step has completed. This is the default behaviour. This switch is used to override a configuration file setting.

verbose : bool
Show the files and features loaded.

wip : bool
Only run scenarios tagged with “wip”. Additionally: use the “plain” formatter, do not capture stdout or logging output and stop at the first failure.

lang : text
Use keywords for a language other than English.

Then:Refer to description under command-line arguments

}",/content/Feature47.feature,"Scenario: Command-line args can override default paths in configfile
        When I run ""behave -f plain features/alice.feature""
        Then it should pass with:
            """"""
            1 feature passed, 0 failed, 0 skipped
            1 scenario passed, 0 failed, 0 skipped
            3 steps passed, 0 failed, 0 skipped, 0 undefined
             """"""
        And the command output should contain:
            """"""
            Feature: Alice
                Scenario: A1
            """"""
        But the command output should not contain:
            """"""
            Feature: Bob
            """"""
        And the command output should not contain:
            """"""
            Feature: Charly
            """""""
/content/Record48.rst,"{

Scenario:Link:https://behave.readthedocs.io/en/latest/behave/#configuration-files
Configuration Files
Configuration files for behave are called either “.behaverc”, “behave.ini”, “setup.cfg”, “tox.ini”, or “pyproject.toml” (your preference) and are located in one of three places:

the current working directory (good for per-project settings),

your home directory ($HOME), or

on Windows, in the %APPDATA% directory.

If you are wondering where behave is getting its configuration defaults from you can use the “-v” command-line argument and it’ll tell you.

Configuration files must start with the label “[behave]” and are formatted in the Windows INI style, for example:

[behave]
format=plain
logging_clear_handlers=yes
logging_filter=-suds
Alternatively, if using “pyproject.toml” instead (note the “tool.” prefix):

[tool.behave]
format = ""plain""
logging_clear_handlers = true
logging_filter = ""-suds""
NOTE: toml does not support ‘%’ interpolations.


Given:Configuration Parameter Types
The following types are supported (and used):

text
This just assigns whatever text you supply to the configuration setting.

bool
This assigns a boolean value to the configuration setting. The text describes the functionality when the value is true. True values are “1”, “yes”, “true”, and “on”. False values are “0”, “no”, “false”, and “off”. TOML: toml only accepts its native true

sequence<text>
These fields accept one or more values on new lines, for example a tag expression might look like:

default_tags= (@foo or not @bar) and @zap
which is the equivalent of the command-line usage:

--tags=""(@foo or not @bar) and @zap""
TOML: toml can use arrays natively.

Configuration Parameters
color : Colored (Enum)
Use colored mode or not (default: auto).

dry_run : bool
Invokes formatters without executing the steps.

userdata_defines : sequence<text>
Define user-specific data for the config.userdata dictionary. Example: -D foo=bar to store it in config.userdata[“foo”].

exclude_re : text
Don’t run feature files matching regular expression PATTERN.

include_re : text
Only run feature files matching regular expression PATTERN.

junit : bool
Output JUnit-compatible reports. When junit is enabled, all stdout and stderr will be redirected and dumped to the junit report, regardless of the “–capture” and “–no-capture” options.

junit_directory : text
Directory in which to store JUnit reports.

jobs : positive_number
Number of concurrent jobs to use (default: 1). Only supported by test runners that support parallel execution.

default_format : text
Specify default formatter (default: pretty).

format : sequence<text>
Specify a formatter. If none is specified the default formatter is used. Pass “–format help” to get a list of available formatters.

steps_catalog : bool
Show a catalog of all available step definitions. SAME AS: –format=steps.catalog –dry-run –no-summary -q

scenario_outline_annotation_schema : text
Specify name annotation schema for scenario outline (default=”{name} – @{row.id} {examples.name}”).

show_skipped : bool
Print skipped steps. This is the default behaviour. This switch is used to override a configuration file setting.

show_snippets : bool
Print snippets for unimplemented steps. This is the default behaviour. This switch is used to override a configuration file setting.

show_multiline : bool
Print multiline strings and tables under steps. This is the default behaviour. This switch is used to override a configuration file setting.

name : sequence<text>
Select feature elements (scenarios, …) to run which match part of the given name (regex pattern). If this option is given more than once, it will match against all the given names.

stdout_capture : bool
Capture stdout (any stdout output will be printed if there is a failure.) This is the default behaviour. This switch is used to override a configuration file setting.

stderr_capture : bool
Capture stderr (any stderr output will be printed if there is a failure.) This is the default behaviour. This switch is used to override a configuration file setting.

log_capture : bool
Capture logging. All logging during a step will be captured and displayed in the event of a failure. This is the default behaviour. This switch is used to override a configuration file setting.

logging_level : text
Specify a level to capture logging at. The default is INFO - capturing everything.

logging_format : text
Specify custom format to print statements. Uses the same format as used by standard logging handlers. The default is “%(levelname)s:%(name)s:%(message)s”.

logging_datefmt : text
Specify custom date/time format to print statements. Uses the same format as used by standard logging handlers.

logging_filter : text
Specify which statements to filter in/out. By default, everything is captured. If the output is too verbose, use this option to filter out needless output. Example: logging_filter = foo will capture statements issued ONLY to “foo” or “foo.what.ever.sub” but not “foobar” or other logger. Specify multiple loggers with comma: logging_filter = foo,bar,baz. If any logger name is prefixed with a minus, eg logging_filter = -foo, it will be excluded rather than included.

logging_clear_handlers : bool
Clear all other logging handlers.

summary : bool
Display the summary at the end of the run.

outfiles : sequence<text>
Write to specified file instead of stdout.

paths : sequence<text>
Specify default feature paths, used when none are provided.

tag_expression_protocol : TagExpressionProtocol (Enum)
Specify the tag-expression protocol to use (default: auto_detect). With “v1”, only tag-expressions v1 are supported. With “v2”, only tag-expressions v2 are supported. With “auto_detect”, tag- expressions v1 and v2 are auto-detected.

quiet : bool
Alias for –no-snippets –no-source.

runner : text
Use own runner class, like: “behave.runner:Runner”

show_source : bool
Print the file and line of the step definition with the steps. This is the default behaviour. This switch is used to override a configuration file setting.

stage : text
Defines the current test stage. The test stage name is used as name prefix for the environment file and the steps directory (instead of default path names).

stop : bool
Stop running tests at the first failure.

default_tags : sequence<text>
Define default tags when non are provided. See –tags for more information.

tags : sequence<text>
Only execute certain features or scenarios based on the tag expression given. See below for how to code tag expressions in configuration files.

show_timings : bool
Print the time taken, in seconds, of each step after the step has completed. This is the default behaviour. This switch is used to override a configuration file setting.

verbose : bool
Show the files and features loaded.

wip : bool
Only run scenarios tagged with “wip”. Additionally: use the “plain” formatter, do not capture stdout or logging output and stop at the first failure.

lang : text
Use keywords for a language other than English.

Then:Refer to description under command-line arguments

}",/content/Feature48.feature,"Scenario: Command-line args are provided (CASE 2)
        When I run ""behave -f plain features""
        Then it should pass with:
            """"""
            2 features passed, 0 failed, 0 skipped
            2 scenarios passed, 0 failed, 0 skipped
             """"""
        And the command output should contain:
            """"""
            Feature: Alice
                Scenario: A1
                    Given a step passes ... passed
                    When a step passes ... passed
                    Then a step passes ... passed

            Feature: Bob
                Scenario: B1
                    When a step passes ... passed
            """"""
        But the command output should not contain:
            """"""
            Feature: Charly
            """""""
/content/Record49.rst,"{
Scenario:Link:https://behave.readthedocs.io/en/latest/gherkin/#layout-variations
Layout Variations
behave has some flexibility built in. It will actually try quite hard to find feature specifications. 


Given:When launched you may pass on the command line:

nothing
In the absence of any information behave will attempt to load your features from a subdirectory called “features” in the directory you launched behave.

a features directory path
This is the path to a features directory laid out as described above. It may be called anything but must contain at least one “name.feature” file and a directory called “steps”. The “environment.py” file, if present, must be in the same directory that contains the “steps” directory (not in the “steps” directory).

the path to a “*name*.feature” file
This tells behave where to find the feature file. To find the steps directory behave will look in the directory containing the feature file. If it is not present, behave will look in the parent directory, and then its parent, and so on until it hits the root of the filesystem. The “environment.py” file, if present, must be in the same directory that contains the “steps” directory (not in the “steps” directory).

Then:a directory containing your feature files
Similar to the approach above, you’re identifying the directory where your “name.feature” files are, and if the “steps” directory is not in the same place then behave will search for it just like above.
 This allows you to have a layout like:

+--tests/
|    +-- steps/
|    |    +-- use_steplib_xyz.py
|    |    +-- website_steps.py
|    |    +-- utils.py
|    +-- environment.py
|    +-- signup.feature
|    +-- login.feature
|    +-- account_details.feature
Note that with this approach, if you want to execute behave without having to explicitly specify the directory (first option) you can set the paths setting in your configuration file (e.g. paths=tests).

If you’re having trouble setting things up and want to see what behave is doing in attempting to find your features use the “-v” (verbose) command-line switch.

}

",/content/Feature49.feature,"Scenario: Setup directory structure
        Given a new working directory
        And a file named ""features/steps/steps.py"" with:
            """"""
            from behave import step

            @step('{word:w} step passes')
            def step_passes(context, word):
                pass

            @step('{word:w} step fails')
            def step_fails(context, word):
                assert False, ""XFAIL-STEP""
            """"""
        And a file named ""features/steps/environment_steps.py"" with:
            """"""
            from behave import step

            @step('environment setup was done')
            def step_ensure_environment_setup(context):
                assert context.setup_magic == 42
            """"""
        And a file named ""features/environment.py"" with:
            """"""
            def before_all(context):
                context.setup_magic = 42
            """"""
        And a file named ""features/group1/alice.feature"" with:
            """"""
            Feature: Alice
                Scenario: A1
                  Given a step passes
                  When another step passes
                  Then a step passes

                Scenario: A2
                  Then environment setup was done
            """"""
        And a file named ""features/group1/bob.feature"" with:
            """"""
            Feature: Bob
                Scenario: B1
                  When a step passes
                  Then another step passes
            """"""
        And a file named ""features/group2/charly.feature"" with:
            """"""
            Feature: Charly
                Scenario: C1
                  Given another step passes
                  Then a step passes
            """"""



"
/content/Record50.rst,"{
Scenario:Link:https://behave.readthedocs.io/en/latest/gherkin/#layout-variations
Layout Variations
behave has some flexibility built in. It will actually try quite hard to find feature specifications. 


Given:When launched you may pass on the command line:

nothing
In the absence of any information behave will attempt to load your features from a subdirectory called “features” in the directory you launched behave.

a features directory path
This is the path to a features directory laid out as described above. It may be called anything but must contain at least one “name.feature” file and a directory called “steps”. The “environment.py” file, if present, must be in the same directory that contains the “steps” directory (not in the “steps” directory).

the path to a “*name*.feature” file
This tells behave where to find the feature file. To find the steps directory behave will look in the directory containing the feature file. If it is not present, behave will look in the parent directory, and then its parent, and so on until it hits the root of the filesystem. The “environment.py” file, if present, must be in the same directory that contains the “steps” directory (not in the “steps” directory).

Then:a directory containing your feature files
Similar to the approach above, you’re identifying the directory where your “name.feature” files are, and if the “steps” directory is not in the same place then behave will search for it just like above.
 This allows you to have a layout like:

+--tests/
|    +-- steps/
|    |    +-- use_steplib_xyz.py
|    |    +-- website_steps.py
|    |    +-- utils.py
|    +-- environment.py
|    +-- signup.feature
|    +-- login.feature
|    +-- account_details.feature
Note that with this approach, if you want to execute behave without having to explicitly specify the directory (first option) you can set the paths setting in your configuration file (e.g. paths=tests).

If you’re having trouble setting things up and want to see what behave is doing in attempting to find your features use the “-v” (verbose) command-line switch.

}

",/content/Feature50.feature,"Scenario: Run behave with feature directory
        When I run ""behave -f progress features/""
        Then it should pass with:
            """"""
            3 features passed, 0 failed, 0 skipped
            4 scenarios passed, 0 failed, 0 skipped
            8 steps passed, 0 failed, 0 skipped, 0 undefined
            """""""
/content/Record51.rst,"{
Scenario:Link:https://behave.readthedocs.io/en/latest/gherkin/#layout-variations
Layout Variations
behave has some flexibility built in. It will actually try quite hard to find feature specifications. 


Given:When launched you may pass on the command line:

nothing
In the absence of any information behave will attempt to load your features from a subdirectory called “features” in the directory you launched behave.

a features directory path
This is the path to a features directory laid out as described above. It may be called anything but must contain at least one “name.feature” file and a directory called “steps”. The “environment.py” file, if present, must be in the same directory that contains the “steps” directory (not in the “steps” directory).

the path to a “*name*.feature” file
This tells behave where to find the feature file. To find the steps directory behave will look in the directory containing the feature file. If it is not present, behave will look in the parent directory, and then its parent, and so on until it hits the root of the filesystem. The “environment.py” file, if present, must be in the same directory that contains the “steps” directory (not in the “steps” directory).

Then:a directory containing your feature files
Similar to the approach above, you’re identifying the directory where your “name.feature” files are, and if the “steps” directory is not in the same place then behave will search for it just like above.
 This allows you to have a layout like:

+--tests/
|    +-- steps/
|    |    +-- use_steplib_xyz.py
|    |    +-- website_steps.py
|    |    +-- utils.py
|    +-- environment.py
|    +-- signup.feature
|    +-- login.feature
|    +-- account_details.feature
Note that with this approach, if you want to execute behave without having to explicitly specify the directory (first option) you can set the paths setting in your configuration file (e.g. paths=tests).

If you’re having trouble setting things up and want to see what behave is doing in attempting to find your features use the “-v” (verbose) command-line switch.

}

",/content/Feature51.feature,"Scenario: Run behave with feature subdirectory (CASE 1)
        When I run ""behave -f progress features/group1/""
        Then it should pass with:
            """"""
            2 features passed, 0 failed, 0 skipped
            3 scenarios passed, 0 failed, 0 skipped
            6 steps passed, 0 failed, 0 skipped, 0 undefined
            """""""
/content/Record52.rst,"{
Scenario:Link:https://behave.readthedocs.io/en/latest/gherkin/#layout-variations
Layout Variations
behave has some flexibility built in. It will actually try quite hard to find feature specifications. 


Given:When launched you may pass on the command line:

nothing
In the absence of any information behave will attempt to load your features from a subdirectory called “features” in the directory you launched behave.

a features directory path
This is the path to a features directory laid out as described above. It may be called anything but must contain at least one “name.feature” file and a directory called “steps”. The “environment.py” file, if present, must be in the same directory that contains the “steps” directory (not in the “steps” directory).

the path to a “*name*.feature” file
This tells behave where to find the feature file. To find the steps directory behave will look in the directory containing the feature file. If it is not present, behave will look in the parent directory, and then its parent, and so on until it hits the root of the filesystem. The “environment.py” file, if present, must be in the same directory that contains the “steps” directory (not in the “steps” directory).

Then:a directory containing your feature files
Similar to the approach above, you’re identifying the directory where your “name.feature” files are, and if the “steps” directory is not in the same place then behave will search for it just like above.
 This allows you to have a layout like:

+--tests/
|    +-- steps/
|    |    +-- use_steplib_xyz.py
|    |    +-- website_steps.py
|    |    +-- utils.py
|    +-- environment.py
|    +-- signup.feature
|    +-- login.feature
|    +-- account_details.feature
Note that with this approach, if you want to execute behave without having to explicitly specify the directory (first option) you can set the paths setting in your configuration file (e.g. paths=tests).

If you’re having trouble setting things up and want to see what behave is doing in attempting to find your features use the “-v” (verbose) command-line switch.

}

",/content/Feature52.feature,"Scenario: Run behave with feature subdirectory (CASE 2)
        When I run ""behave -f progress features/group2/""
        Then it should pass with:
            """"""
            1 feature passed, 0 failed, 0 skipped
            1 scenario passed, 0 failed, 0 skipped
            2 steps passed, 0 failed, 0 skipped, 0 undefined
            """""""
/content/Record53.rst,"{
Scenario:Link:https://behave.readthedocs.io/en/latest/gherkin/#layout-variations
Layout Variations
behave has some flexibility built in. It will actually try quite hard to find feature specifications. 


Given:When launched you may pass on the command line:

nothing
In the absence of any information behave will attempt to load your features from a subdirectory called “features” in the directory you launched behave.

a features directory path
This is the path to a features directory laid out as described above. It may be called anything but must contain at least one “name.feature” file and a directory called “steps”. The “environment.py” file, if present, must be in the same directory that contains the “steps” directory (not in the “steps” directory).

the path to a “*name*.feature” file
This tells behave where to find the feature file. To find the steps directory behave will look in the directory containing the feature file. If it is not present, behave will look in the parent directory, and then its parent, and so on until it hits the root of the filesystem. The “environment.py” file, if present, must be in the same directory that contains the “steps” directory (not in the “steps” directory).

Then:a directory containing your feature files
Similar to the approach above, you’re identifying the directory where your “name.feature” files are, and if the “steps” directory is not in the same place then behave will search for it just like above.
 This allows you to have a layout like:

+--tests/
|    +-- steps/
|    |    +-- use_steplib_xyz.py
|    |    +-- website_steps.py
|    |    +-- utils.py
|    +-- environment.py
|    +-- signup.feature
|    +-- login.feature
|    +-- account_details.feature
Note that with this approach, if you want to execute behave without having to explicitly specify the directory (first option) you can set the paths setting in your configuration file (e.g. paths=tests).

If you’re having trouble setting things up and want to see what behave is doing in attempting to find your features use the “-v” (verbose) command-line switch.

}

",/content/Feature53.feature," Scenario: Run behave with one feature file
        When I run ""behave -f progress features/group1/alice.feature""
        Then it should pass with:
            """"""
            1 feature passed, 0 failed, 0 skipped
            2 scenarios passed, 0 failed, 0 skipped
            4 steps passed, 0 failed, 0 skipped, 0 undefined
            """"""
        When I run ""behave -f progress features/group2/charly.feature""
        Then it should pass with:
            """"""
            1 feature passed, 0 failed, 0 skipped
            1 scenario passed, 0 failed, 0 skipped
            2 steps passed, 0 failed, 0 skipped, 0 undefined
            """""""
/content/Record54.rst,"{
Scenario:Link:https://behave.readthedocs.io/en/latest/gherkin/#layout-variations
Layout Variations
behave has some flexibility built in. It will actually try quite hard to find feature specifications. 


Given:When launched you may pass on the command line:

nothing
In the absence of any information behave will attempt to load your features from a subdirectory called “features” in the directory you launched behave.

a features directory path
This is the path to a features directory laid out as described above. It may be called anything but must contain at least one “name.feature” file and a directory called “steps”. The “environment.py” file, if present, must be in the same directory that contains the “steps” directory (not in the “steps” directory).

the path to a “*name*.feature” file
This tells behave where to find the feature file. To find the steps directory behave will look in the directory containing the feature file. If it is not present, behave will look in the parent directory, and then its parent, and so on until it hits the root of the filesystem. The “environment.py” file, if present, must be in the same directory that contains the “steps” directory (not in the “steps” directory).

Then:a directory containing your feature files
Similar to the approach above, you’re identifying the directory where your “name.feature” files are, and if the “steps” directory is not in the same place then behave will search for it just like above.
 This allows you to have a layout like:

+--tests/
|    +-- steps/
|    |    +-- use_steplib_xyz.py
|    |    +-- website_steps.py
|    |    +-- utils.py
|    +-- environment.py
|    +-- signup.feature
|    +-- login.feature
|    +-- account_details.feature
Note that with this approach, if you want to execute behave without having to explicitly specify the directory (first option) you can set the paths setting in your configuration file (e.g. paths=tests).

If you’re having trouble setting things up and want to see what behave is doing in attempting to find your features use the “-v” (verbose) command-line switch.

}

",/content/Feature54.feature," Scenario: Run behave with two feature files (CASE 1)
        Given a file named ""one.featureset"" with:
            """"""
            features/group1/alice.feature
            features/group2/charly.feature
            """"""
        When I run ""behave -f progress @one.featureset""
        Then it should pass with:
            """"""
            2 features passed, 0 failed, 0 skipped
            3 scenarios passed, 0 failed, 0 skipped
            6 steps passed, 0 failed, 0 skipped, 0 undefined
            """""""
/content/Record55.rst,"{
Scenario:Link:https://behave.readthedocs.io/en/latest/gherkin/#layout-variations
Layout Variations
behave has some flexibility built in. It will actually try quite hard to find feature specifications. 


Given:When launched you may pass on the command line:

nothing
In the absence of any information behave will attempt to load your features from a subdirectory called “features” in the directory you launched behave.

a features directory path
This is the path to a features directory laid out as described above. It may be called anything but must contain at least one “name.feature” file and a directory called “steps”. The “environment.py” file, if present, must be in the same directory that contains the “steps” directory (not in the “steps” directory).

the path to a “*name*.feature” file
This tells behave where to find the feature file. To find the steps directory behave will look in the directory containing the feature file. If it is not present, behave will look in the parent directory, and then its parent, and so on until it hits the root of the filesystem. The “environment.py” file, if present, must be in the same directory that contains the “steps” directory (not in the “steps” directory).

Then:a directory containing your feature files
Similar to the approach above, you’re identifying the directory where your “name.feature” files are, and if the “steps” directory is not in the same place then behave will search for it just like above.
 This allows you to have a layout like:

+--tests/
|    +-- steps/
|    |    +-- use_steplib_xyz.py
|    |    +-- website_steps.py
|    |    +-- utils.py
|    +-- environment.py
|    +-- signup.feature
|    +-- login.feature
|    +-- account_details.feature
Note that with this approach, if you want to execute behave without having to explicitly specify the directory (first option) you can set the paths setting in your configuration file (e.g. paths=tests).

If you’re having trouble setting things up and want to see what behave is doing in attempting to find your features use the “-v” (verbose) command-line switch.

}

",/content/Feature55.feature," Scenario: Run behave with two feature files (CASE 2: different ordering)
        Given a file named ""two.featureset"" with:
            """"""
            features/group2/charly.feature
            features/group1/alice.feature
            """"""
        When I run ""behave -f progress @two.featureset""
        Then it should pass with:
            """"""
            2 features passed, 0 failed, 0 skipped
            3 scenarios passed, 0 failed, 0 skipped
            6 steps passed, 0 failed, 0 skipped, 0 undefined
            """""""
/content/Record56.rst,"{
Scenario:Link:https://behave.readthedocs.io/en/latest/gherkin/#layout-variations
Layout Variations
behave has some flexibility built in. It will actually try quite hard to find feature specifications. When launched you may pass on the command line:


Given:nothing
In the absence of any information behave will attempt to load your features from a subdirectory called “features” in the directory you launched behave.

a features directory path
This is the path to a features directory laid out as described above. It may be called anything but must contain at least one “name.feature” file and a directory called “steps”. The “environment.py” file, if present, must be in the same directory that contains the “steps” directory (not in the “steps” directory).

the path to a “*name*.feature” file
This tells behave where to find the feature file. To find the steps directory behave will look in the directory containing the feature file. If it is not present, behave will look in the parent directory, and then its parent, and so on until it hits the root of the filesystem. The “environment.py” file, if present, must be in the same directory that contains the “steps” directory (not in the “steps” directory).

a directory containing your feature files
Similar to the approach above, you’re identifying the directory where your “name.feature” files are, and if the “steps” directory is not in the same place then behave will search for it just like above.


Then: This allows you to have a layout like:

+--tests/
|    +-- steps/
|    |    +-- use_steplib_xyz.py
|    |    +-- website_steps.py
|    |    +-- utils.py
|    +-- environment.py
|    +-- signup.feature
|    +-- login.feature
|    +-- account_details.feature
Note that with this approach, if you want to execute behave without having to explicitly specify the directory (first option) you can set the paths setting in your configuration file (e.g. paths=tests).

If you’re having trouble setting things up and want to see what behave is doing in attempting to find your features use the “-v” (verbose) command-line switch.

}

",/content/Feature56.feature,"Scenario: Setup directory structure
        Given a new working directory
        And a file named ""features/steps/steps.py"" with:
            """"""
            from behave import step

            @step('{word:w} step passes')
            def step_passes(context, word):
                pass

            @step('{word:w} step fails')
            def step_fails(context, word):
                assert False, ""XFAIL-STEP""
            """"""
        And a file named ""features/alice.feature"" with:
            """"""
            Feature: Alice
                Scenario: A1
                  Given a step passes
                  When another step passes
                  Then a step passes
            """"""
        And a file named ""features/bob.feature"" with:
            """"""
            Feature: Bob
                Scenario: B1
                  When a step passes
                  Then another step passes
            """"""


"
/content/Record57.rst,"{
Scenario:Link:https://behave.readthedocs.io/en/latest/gherkin/#layout-variations
Layout Variations
behave has some flexibility built in. It will actually try quite hard to find feature specifications. When launched you may pass on the command line:


Given:nothing
In the absence of any information behave will attempt to load your features from a subdirectory called “features” in the directory you launched behave.

a features directory path
This is the path to a features directory laid out as described above. It may be called anything but must contain at least one “name.feature” file and a directory called “steps”. The “environment.py” file, if present, must be in the same directory that contains the “steps” directory (not in the “steps” directory).

the path to a “*name*.feature” file
This tells behave where to find the feature file. To find the steps directory behave will look in the directory containing the feature file. If it is not present, behave will look in the parent directory, and then its parent, and so on until it hits the root of the filesystem. The “environment.py” file, if present, must be in the same directory that contains the “steps” directory (not in the “steps” directory).

a directory containing your feature files
Similar to the approach above, you’re identifying the directory where your “name.feature” files are, and if the “steps” directory is not in the same place then behave will search for it just like above.


Then: This allows you to have a layout like:

+--tests/
|    +-- steps/
|    |    +-- use_steplib_xyz.py
|    |    +-- website_steps.py
|    |    +-- utils.py
|    +-- environment.py
|    +-- signup.feature
|    +-- login.feature
|    +-- account_details.feature
Note that with this approach, if you want to execute behave without having to explicitly specify the directory (first option) you can set the paths setting in your configuration file (e.g. paths=tests).

If you’re having trouble setting things up and want to see what behave is doing in attempting to find your features use the “-v” (verbose) command-line switch.

}

",/content/Feature57.feature," Scenario: Run behave with feature directory
        When I run ""behave -f progress features/""
        Then it should pass with:
            """"""
            2 features passed, 0 failed, 0 skipped
            2 scenarios passed, 0 failed, 0 skipped
            5 steps passed, 0 failed, 0 skipped, 0 undefined
            """""""
/content/Record58.rst,"{
Scenario:Link:https://behave.readthedocs.io/en/latest/gherkin/#layout-variations
Layout Variations
behave has some flexibility built in. It will actually try quite hard to find feature specifications. When launched you may pass on the command line:


Given:nothing
In the absence of any information behave will attempt to load your features from a subdirectory called “features” in the directory you launched behave.

a features directory path
This is the path to a features directory laid out as described above. It may be called anything but must contain at least one “name.feature” file and a directory called “steps”. The “environment.py” file, if present, must be in the same directory that contains the “steps” directory (not in the “steps” directory).

the path to a “*name*.feature” file
This tells behave where to find the feature file. To find the steps directory behave will look in the directory containing the feature file. If it is not present, behave will look in the parent directory, and then its parent, and so on until it hits the root of the filesystem. The “environment.py” file, if present, must be in the same directory that contains the “steps” directory (not in the “steps” directory).

a directory containing your feature files
Similar to the approach above, you’re identifying the directory where your “name.feature” files are, and if the “steps” directory is not in the same place then behave will search for it just like above.


Then: This allows you to have a layout like:

+--tests/
|    +-- steps/
|    |    +-- use_steplib_xyz.py
|    |    +-- website_steps.py
|    |    +-- utils.py
|    +-- environment.py
|    +-- signup.feature
|    +-- login.feature
|    +-- account_details.feature
Note that with this approach, if you want to execute behave without having to explicitly specify the directory (first option) you can set the paths setting in your configuration file (e.g. paths=tests).

If you’re having trouble setting things up and want to see what behave is doing in attempting to find your features use the “-v” (verbose) command-line switch.

}

",/content/Feature58.feature,"  Scenario: Run behave with one feature file
        When I run ""behave -f progress features/alice.feature""
        Then it should pass with:
            """"""
            1 feature passed, 0 failed, 0 skipped
            1 scenario passed, 0 failed, 0 skipped
            3 steps passed, 0 failed, 0 skipped, 0 undefined
            """""""
/content/Record59.rst,"{
Scenario:Link:https://behave.readthedocs.io/en/latest/gherkin/#layout-variations
Layout Variations
behave has some flexibility built in. It will actually try quite hard to find feature specifications. When launched you may pass on the command line:


Given:nothing
In the absence of any information behave will attempt to load your features from a subdirectory called “features” in the directory you launched behave.

a features directory path
This is the path to a features directory laid out as described above. It may be called anything but must contain at least one “name.feature” file and a directory called “steps”. The “environment.py” file, if present, must be in the same directory that contains the “steps” directory (not in the “steps” directory).

the path to a “*name*.feature” file
This tells behave where to find the feature file. To find the steps directory behave will look in the directory containing the feature file. If it is not present, behave will look in the parent directory, and then its parent, and so on until it hits the root of the filesystem. The “environment.py” file, if present, must be in the same directory that contains the “steps” directory (not in the “steps” directory).

a directory containing your feature files
Similar to the approach above, you’re identifying the directory where your “name.feature” files are, and if the “steps” directory is not in the same place then behave will search for it just like above.


Then: This allows you to have a layout like:

+--tests/
|    +-- steps/
|    |    +-- use_steplib_xyz.py
|    |    +-- website_steps.py
|    |    +-- utils.py
|    +-- environment.py
|    +-- signup.feature
|    +-- login.feature
|    +-- account_details.feature
Note that with this approach, if you want to execute behave without having to explicitly specify the directory (first option) you can set the paths setting in your configuration file (e.g. paths=tests).

If you’re having trouble setting things up and want to see what behave is doing in attempting to find your features use the “-v” (verbose) command-line switch.

}

",/content/Feature59.feature,"  Scenario: Run behave with two feature files
        When I run ""behave -f progress features/alice.feature features/bob.feature""
        Then it should pass with:
            """"""
            2 features passed, 0 failed, 0 skipped
            2 scenarios passed, 0 failed, 0 skipped
            5 steps passed, 0 failed, 0 skipped, 0 undefined
            """""""
/content/Record60.rst,"{
Scenario:Link:https://behave.readthedocs.io/en/latest/gherkin/#layout-variations
Layout Variations
behave has some flexibility built in. It will actually try quite hard to find feature specifications.


Given:When launched you may pass on the command line:

nothing
In the absence of any information behave will attempt to load your features from a subdirectory called “features” in the directory you launched behave.

a features directory path
This is the path to a features directory laid out as described above. It may be called anything but must contain at least one “name.feature” file and a directory called “steps”. The “environment.py” file, if present, must be in the same directory that contains the “steps” directory (not in the “steps” directory).

the path to a “*name*.feature” file
This tells behave where to find the feature file. To find the steps directory behave will look in the directory containing the feature file. If it is not present, behave will look in the parent directory, and then its parent, and so on until it hits the root of the filesystem. The “environment.py” file, if present, must be in the same directory that contains the “steps” directory (not in the “steps” directory).

a directory containing your feature files
Similar to the approach above, you’re identifying the directory where your “name.feature” files are, and if the “steps” directory is not in the same place then behave will search for it just like above.


Then:This allows you to have a layout like:

+--tests/
|    +-- steps/
|    |    +-- use_steplib_xyz.py
|    |    +-- website_steps.py
|    |    +-- utils.py
|    +-- environment.py
|    +-- signup.feature
|    +-- login.feature
|    +-- account_details.feature
Note that with this approach, if you want to execute behave without having to explicitly specify the directory (first option) you can set the paths setting in your configuration file (e.g. paths=tests).

If you’re having trouble setting things up and want to see what behave is doing in attempting to find your features use the “-v” (verbose) command-line switch.


}
",/content/Feature60.feature,"Scenario: Setup directory structure
        Given a new working directory
        And a file named ""testing/features/steps/steps.py"" with:
            """"""
            from behave import step

            @step('{word:w} step passes')
            def step_passes(context, word):
                pass

            @step('{word:w} step fails')
            def step_fails(context, word):
                assert False, ""XFAIL-STEP""
            """"""
        And a file named ""testing/features/alice.feature"" with:
            """"""
            Feature: Alice
                Scenario: A1
                  Given a step passes
                  When another step passes
                  Then a step passes
            """"""
        And a file named ""testing/features/bob.feature"" with:
            """"""
            Feature: Bob
                Scenario: B1
                  When a step passes
                  Then another step passes
            """"""


"
/content/Record61.rst,"{
Scenario:Link:https://behave.readthedocs.io/en/latest/gherkin/#layout-variations
Layout Variations
behave has some flexibility built in. It will actually try quite hard to find feature specifications.


Given:When launched you may pass on the command line:

nothing
In the absence of any information behave will attempt to load your features from a subdirectory called “features” in the directory you launched behave.

a features directory path
This is the path to a features directory laid out as described above. It may be called anything but must contain at least one “name.feature” file and a directory called “steps”. The “environment.py” file, if present, must be in the same directory that contains the “steps” directory (not in the “steps” directory).

the path to a “*name*.feature” file
This tells behave where to find the feature file. To find the steps directory behave will look in the directory containing the feature file. If it is not present, behave will look in the parent directory, and then its parent, and so on until it hits the root of the filesystem. The “environment.py” file, if present, must be in the same directory that contains the “steps” directory (not in the “steps” directory).

a directory containing your feature files
Similar to the approach above, you’re identifying the directory where your “name.feature” files are, and if the “steps” directory is not in the same place then behave will search for it just like above.


Then:This allows you to have a layout like:

+--tests/
|    +-- steps/
|    |    +-- use_steplib_xyz.py
|    |    +-- website_steps.py
|    |    +-- utils.py
|    +-- environment.py
|    +-- signup.feature
|    +-- login.feature
|    +-- account_details.feature
Note that with this approach, if you want to execute behave without having to explicitly specify the directory (first option) you can set the paths setting in your configuration file (e.g. paths=tests).

If you’re having trouble setting things up and want to see what behave is doing in attempting to find your features use the “-v” (verbose) command-line switch.


}
",/content/Feature61.feature,"Scenario: Run behave with testing directory
        When I run ""behave -f progress testing/""
        Then it should fail with:
            """"""
            ConfigError: No steps directory in '{__WORKDIR__}/testing'
            """""""
/content/Record62.rst,"{
Scenario:Link:https://behave.readthedocs.io/en/latest/gherkin/#layout-variations
Layout Variations
behave has some flexibility built in. It will actually try quite hard to find feature specifications.


Given:When launched you may pass on the command line:

nothing
In the absence of any information behave will attempt to load your features from a subdirectory called “features” in the directory you launched behave.

a features directory path
This is the path to a features directory laid out as described above. It may be called anything but must contain at least one “name.feature” file and a directory called “steps”. The “environment.py” file, if present, must be in the same directory that contains the “steps” directory (not in the “steps” directory).

the path to a “*name*.feature” file
This tells behave where to find the feature file. To find the steps directory behave will look in the directory containing the feature file. If it is not present, behave will look in the parent directory, and then its parent, and so on until it hits the root of the filesystem. The “environment.py” file, if present, must be in the same directory that contains the “steps” directory (not in the “steps” directory).

a directory containing your feature files
Similar to the approach above, you’re identifying the directory where your “name.feature” files are, and if the “steps” directory is not in the same place then behave will search for it just like above.


Then:This allows you to have a layout like:

+--tests/
|    +-- steps/
|    |    +-- use_steplib_xyz.py
|    |    +-- website_steps.py
|    |    +-- utils.py
|    +-- environment.py
|    +-- signup.feature
|    +-- login.feature
|    +-- account_details.feature
Note that with this approach, if you want to execute behave without having to explicitly specify the directory (first option) you can set the paths setting in your configuration file (e.g. paths=tests).

If you’re having trouble setting things up and want to see what behave is doing in attempting to find your features use the “-v” (verbose) command-line switch.


}
",/content/Feature62.feature,"Scenario: Run behave with feature subdirectory
        When I run ""behave -f progress testing/features/""
        Then it should pass with:
            """"""
            2 features passed, 0 failed, 0 skipped
            2 scenarios passed, 0 failed, 0 skipped
            5 steps passed, 0 failed, 0 skipped, 0 undefined
            """""""
/content/Record63.rst,"{
Scenario:Link:https://behave.readthedocs.io/en/latest/gherkin/#layout-variations
Layout Variations
behave has some flexibility built in. It will actually try quite hard to find feature specifications.


Given:When launched you may pass on the command line:

nothing
In the absence of any information behave will attempt to load your features from a subdirectory called “features” in the directory you launched behave.

a features directory path
This is the path to a features directory laid out as described above. It may be called anything but must contain at least one “name.feature” file and a directory called “steps”. The “environment.py” file, if present, must be in the same directory that contains the “steps” directory (not in the “steps” directory).

the path to a “*name*.feature” file
This tells behave where to find the feature file. To find the steps directory behave will look in the directory containing the feature file. If it is not present, behave will look in the parent directory, and then its parent, and so on until it hits the root of the filesystem. The “environment.py” file, if present, must be in the same directory that contains the “steps” directory (not in the “steps” directory).

a directory containing your feature files
Similar to the approach above, you’re identifying the directory where your “name.feature” files are, and if the “steps” directory is not in the same place then behave will search for it just like above.


Then:This allows you to have a layout like:

+--tests/
|    +-- steps/
|    |    +-- use_steplib_xyz.py
|    |    +-- website_steps.py
|    |    +-- utils.py
|    +-- environment.py
|    +-- signup.feature
|    +-- login.feature
|    +-- account_details.feature
Note that with this approach, if you want to execute behave without having to explicitly specify the directory (first option) you can set the paths setting in your configuration file (e.g. paths=tests).

If you’re having trouble setting things up and want to see what behave is doing in attempting to find your features use the “-v” (verbose) command-line switch.


}
",/content/Feature63.feature,"Scenario: Run behave with one feature file
        When I run ""behave -f progress testing/features/alice.feature""
        Then it should pass with:
            """"""
            1 feature passed, 0 failed, 0 skipped
            1 scenario passed, 0 failed, 0 skipped
            3 steps passed, 0 failed, 0 skipped, 0 undefined
            """""""
/content/Record64.rst,"{
Scenario:Link:https://behave.readthedocs.io/en/latest/gherkin/#layout-variations
Layout Variations
behave has some flexibility built in. It will actually try quite hard to find feature specifications.


Given:When launched you may pass on the command line:

nothing
In the absence of any information behave will attempt to load your features from a subdirectory called “features” in the directory you launched behave.

a features directory path
This is the path to a features directory laid out as described above. It may be called anything but must contain at least one “name.feature” file and a directory called “steps”. The “environment.py” file, if present, must be in the same directory that contains the “steps” directory (not in the “steps” directory).

the path to a “*name*.feature” file
This tells behave where to find the feature file. To find the steps directory behave will look in the directory containing the feature file. If it is not present, behave will look in the parent directory, and then its parent, and so on until it hits the root of the filesystem. The “environment.py” file, if present, must be in the same directory that contains the “steps” directory (not in the “steps” directory).

a directory containing your feature files
Similar to the approach above, you’re identifying the directory where your “name.feature” files are, and if the “steps” directory is not in the same place then behave will search for it just like above.


Then:This allows you to have a layout like:

+--tests/
|    +-- steps/
|    |    +-- use_steplib_xyz.py
|    |    +-- website_steps.py
|    |    +-- utils.py
|    +-- environment.py
|    +-- signup.feature
|    +-- login.feature
|    +-- account_details.feature
Note that with this approach, if you want to execute behave without having to explicitly specify the directory (first option) you can set the paths setting in your configuration file (e.g. paths=tests).

If you’re having trouble setting things up and want to see what behave is doing in attempting to find your features use the “-v” (verbose) command-line switch.


}
",/content/Feature64.feature,"Scenario: Run behave with two feature files
        Given a file named ""one.featureset"" with:
            """"""
            testing/features/alice.feature
            testing/features/bob.feature
            """"""
        When I run ""behave -f progress @one.featureset""
        Then it should pass with:
            """"""
            2 features passed, 0 failed, 0 skipped
            2 scenarios passed, 0 failed, 0 skipped
            5 steps passed, 0 failed, 0 skipped, 0 undefined
            """""""
/content/Record65.rst,"{
Scenario:Link:https://behave.readthedocs.io/en/latest/gherkin/#background
Background
A background consists of a series of steps similar to scenarios. It allows you to add some context to the scenarios of a feature. A background is executed before each scenario of this feature but after any of the before hooks.


Given:It is useful for performing setup operations like:

logging into a web browser or

setting up a database with test data used by the scenarios.

The background description is for the benefit of humans reading the feature text. Again the background name should just be a reasonably descriptive title for the background operation being performed or requirement being met.

A background section may exist only once within a feature file. In addition, a background must be defined before any scenario or scenario outline.


Then:It contains steps as described below.

Good practices for using Background

Don’t use “Background” to set up complicated state unless that state is actually something the client needs to know.
For example, if the user and site names don’t matter to the client, you should use a high-level step such as “Given that I am logged in as a site owner”.

Keep your “Background” section short.
You’re expecting the user to actually remember this stuff when reading your scenarios. If the background is more than 4 lines long, can you move some of the irrelevant details into high-level steps? See calling steps from other steps.

Make your “Background” section vivid.
You should use colorful names and try to tell a story, because the human brain can keep track of stories much better than it can keep track of names like “User A”, “User B”, “Site 1”, and so on.

Keep your scenarios short, and don’t have too many.
If the background section has scrolled off the screen, you should think about using higher-level steps, or splitting the features file in two.

}
",/content/Feature65.feature,"Scenario: Feature Setup
    Given a new working directory
    And a file named ""features/steps/background_steps.py"" with:
        """"""
        from behave import step

        @step('{word} background step passes')
        def step_background_step_passes(context, word):
            pass

        @step('{word} background step fails')
        def step_background_step_fails(context, word):
            assert False, ""XFAIL: background step""

        @step('{word} background step fails sometimes')
        def step_background_step_fails_sometimes(context, word):
            should_fail = (context.scenarios_count % 2) == 0
            if should_fail:
                step_background_step_fails(context, word)
        """"""
    And a file named ""features/steps/passing_steps.py"" with:
        """"""
        from behave import step

        @step('{word} step passes')
        def step_passes(context, word):
            pass

        @step('{word} step fails')
        def step_fails(context, word):
            assert False, ""XFAIL""
        """"""
"
/content/Record66.rst,"{
Scenario:Link:https://behave.readthedocs.io/en/latest/gherkin/#background
Background
A background consists of a series of steps similar to scenarios. It allows you to add some context to the scenarios of a feature. A background is executed before each scenario of this feature but after any of the before hooks.


Given:It is useful for performing setup operations like:

logging into a web browser or

setting up a database with test data used by the scenarios.

The background description is for the benefit of humans reading the feature text. Again the background name should just be a reasonably descriptive title for the background operation being performed or requirement being met.

A background section may exist only once within a feature file. In addition, a background must be defined before any scenario or scenario outline.


Then:It contains steps as described below.

Good practices for using Background

Don’t use “Background” to set up complicated state unless that state is actually something the client needs to know.
For example, if the user and site names don’t matter to the client, you should use a high-level step such as “Given that I am logged in as a site owner”.

Keep your “Background” section short.
You’re expecting the user to actually remember this stuff when reading your scenarios. If the background is more than 4 lines long, can you move some of the irrelevant details into high-level steps? See calling steps from other steps.

Make your “Background” section vivid.
You should use colorful names and try to tell a story, because the human brain can keep track of stories much better than it can keep track of names like “User A”, “User B”, “Site 1”, and so on.

Keep your scenarios short, and don’t have too many.
If the background section has scrolled off the screen, you should think about using higher-level steps, or splitting the features file in two.

}
",/content/Feature66.feature," Scenario: Feature with a Background and Scenarios
    Given a file named ""features/background_example1.feature"" with:
        """"""
        Feature:
          Background:
            Given a background step passes
            And another background step passes

          Scenario: S1
            When a step passes

          Scenario: S2
            Then a step passes
            And another step passes
        """"""
    When I run ""behave -f plain -T features/background_example1.feature""
    Then it should pass with:
        """"""
        2 scenarios passed, 0 failed, 0 skipped
        7 steps passed, 0 failed, 0 skipped, 0 undefined
        """"""
    And the command output should contain:
        """"""
        Feature:
          Background:

          Scenario: S1
            Given a background step passes ... passed
            And another background step passes ... passed
            When a step passes ... passed

          Scenario: S2
            Given a background step passes ... passed
            And another background step passes ... passed
            Then a step passes ... passed
            And another step passes ... passed
        """"""
    But note that ""the Background steps are injected into each Scenario""
    And note that ""the Background steps are executed before any Scenario steps"""
/content/Record67.rst,"{
Scenario:Link:https://behave.readthedocs.io/en/latest/gherkin/#background
Background
A background consists of a series of steps similar to scenarios. It allows you to add some context to the scenarios of a feature. A background is executed before each scenario of this feature but after any of the before hooks.


Given:It is useful for performing setup operations like:

logging into a web browser or

setting up a database with test data used by the scenarios.

The background description is for the benefit of humans reading the feature text. Again the background name should just be a reasonably descriptive title for the background operation being performed or requirement being met.

A background section may exist only once within a feature file. In addition, a background must be defined before any scenario or scenario outline.


Then:It contains steps as described below.

Good practices for using Background

Don’t use “Background” to set up complicated state unless that state is actually something the client needs to know.
For example, if the user and site names don’t matter to the client, you should use a high-level step such as “Given that I am logged in as a site owner”.

Keep your “Background” section short.
You’re expecting the user to actually remember this stuff when reading your scenarios. If the background is more than 4 lines long, can you move some of the irrelevant details into high-level steps? See calling steps from other steps.

Make your “Background” section vivid.
You should use colorful names and try to tell a story, because the human brain can keep track of stories much better than it can keep track of names like “User A”, “User B”, “Site 1”, and so on.

Keep your scenarios short, and don’t have too many.
If the background section has scrolled off the screen, you should think about using higher-level steps, or splitting the features file in two.

}
",/content/Feature67.feature," Scenario: Failing Background Step causes all Scenarios to fail
    Given a file named ""features/background_fail_example.feature"" with:
        """"""
        Feature:

          Background: B1
            Given a background step passes
            And a background step fails
            And another background step passes

          Scenario: S1
            When a step passes

          Scenario: S2
            Then a step passes
            And another step passes
        """"""
    When I run ""behave -f plain -T features/background_fail_example.feature""
    Then it should fail with:
        """"""
        0 scenarios passed, 2 failed, 0 skipped
        2 steps passed, 2 failed, 5 skipped, 0 undefined
        """"""
    And the command output should contain:
        """"""
        Feature:
          Background: B1

          Scenario: S1
            Given a background step passes ... passed
            And a background step fails ... failed
        Assertion Failed: XFAIL: background step

          Scenario: S2
            Given a background step passes ... passed
            And a background step fails ... failed
        Assertion Failed: XFAIL: background step
        """"""
    And note that ""the failing Background step causes all Scenarios to fail"""
/content/Record68.rst,"{
Scenario:Link:https://behave.readthedocs.io/en/latest/gherkin/#background
Background
A background consists of a series of steps similar to scenarios. It allows you to add some context to the scenarios of a feature. A background is executed before each scenario of this feature but after any of the before hooks.


Given:It is useful for performing setup operations like:

logging into a web browser or

setting up a database with test data used by the scenarios.

The background description is for the benefit of humans reading the feature text. Again the background name should just be a reasonably descriptive title for the background operation being performed or requirement being met.

A background section may exist only once within a feature file. In addition, a background must be defined before any scenario or scenario outline.


Then:It contains steps as described below.

Good practices for using Background

Don’t use “Background” to set up complicated state unless that state is actually something the client needs to know.
For example, if the user and site names don’t matter to the client, you should use a high-level step such as “Given that I am logged in as a site owner”.

Keep your “Background” section short.
You’re expecting the user to actually remember this stuff when reading your scenarios. If the background is more than 4 lines long, can you move some of the irrelevant details into high-level steps? See calling steps from other steps.

Make your “Background” section vivid.
You should use colorful names and try to tell a story, because the human brain can keep track of stories much better than it can keep track of names like “User A”, “User B”, “Site 1”, and so on.

Keep your scenarios short, and don’t have too many.
If the background section has scrolled off the screen, you should think about using higher-level steps, or splitting the features file in two.

}
",/content/Feature68.feature,"  Scenario: Failing Background Step does not prevent that other Scenarios are executed

    If a Background step fails sometimes
    it should be retried in the remaining Scenarios where it might pass.

    Given a file named ""features/background_fails_sometimes_example.feature"" with:
        """"""
        Feature:

          Background: B2
            Given a background step fails sometimes

          Scenario: S1
            Given a step passes

          Scenario: S2
            When another step passes

          Scenario: S3
            Then another step passes
        """"""
    And a file named ""features/environment.py"" with:
        """"""
        scenarios_count = 0

        def before_scenario(context, scenario):
            global scenarios_count
            context.scenarios_count = scenarios_count
            scenarios_count += 1
        """"""
    When I run ""behave -f plain -T features/background_fails_sometimes_example.feature""
    Then it should fail with:
        """"""
        1 scenario passed, 2 failed, 0 skipped
        2 steps passed, 2 failed, 2 skipped, 0 undefined
        """"""
    And the command output should contain:
        """"""
        Feature:
            Background: B2

            Scenario: S1
              Given a background step fails sometimes ... failed
          Assertion Failed: XFAIL: background step

            Scenario: S2
              Given a background step fails sometimes ... passed
              When another step passes ... passed

            Scenario: S3
              Given a background step fails sometimes ... failed
          Assertion Failed: XFAIL: background step
        """""""
/content/Record69.rst,"{
Scenario:Link:https://behave.readthedocs.io/en/latest/gherkin/#background
Background
A background consists of a series of steps similar to scenarios. It allows you to add some context to the scenarios of a feature. A background is executed before each scenario of this feature but after any of the before hooks.


Given:It is useful for performing setup operations like:

logging into a web browser or

setting up a database with test data used by the scenarios.

The background description is for the benefit of humans reading the feature text. Again the background name should just be a reasonably descriptive title for the background operation being performed or requirement being met.

A background section may exist only once within a feature file. In addition, a background must be defined before any scenario or scenario outline.


Then:It contains steps as described below.

Good practices for using Background

Don’t use “Background” to set up complicated state unless that state is actually something the client needs to know.
For example, if the user and site names don’t matter to the client, you should use a high-level step such as “Given that I am logged in as a site owner”.

Keep your “Background” section short.
You’re expecting the user to actually remember this stuff when reading your scenarios. If the background is more than 4 lines long, can you move some of the irrelevant details into high-level steps? See calling steps from other steps.

Make your “Background” section vivid.
You should use colorful names and try to tell a story, because the human brain can keep track of stories much better than it can keep track of names like “User A”, “User B”, “Site 1”, and so on.

Keep your scenarios short, and don’t have too many.
If the background section has scrolled off the screen, you should think about using higher-level steps, or splitting the features file in two.

}
",/content/Feature69.feature," Scenario: Feature with a Background and ScenarioOutlines
    Given a file named ""features/background_outline_example.feature"" with:
        """"""
        Feature:
          Background:
            Given a background step passes
            And another background step passes

          Scenario Outline: SO1
            When a step <outcome1>
            Then another step <outcome2>

            Examples: Alpha
              | outcome1 | outcome2 |
              | passes   | passes   |
              | passes   | passes   |

          Scenario Outline: SO2
            Given <word1> step passes
            Then <word2> step passes

            Examples:
              | word1   | word2   |
              | a       | a       |
              | a       | another |
              | another | a       |
        """"""
    When I run ""behave -f plain -T features/background_outline_example.feature""
    Then it should pass with:
        """"""
        5 scenarios passed, 0 failed, 0 skipped
        20 steps passed, 0 failed, 0 skipped, 0 undefined
        """"""
    And the command output should contain:
        """"""
        Feature:
          Background:

          Scenario Outline: SO1 -- @1.1 Alpha
            Given a background step passes ... passed
            And another background step passes ... passed
            When a step passes ... passed
            Then another step passes ... passed

          Scenario Outline: SO1 -- @1.2 Alpha
            Given a background step passes ... passed
            And another background step passes ... passed
            When a step passes ... passed
            Then another step passes ... passed

          Scenario Outline: SO2 -- @1.1
            Given a background step passes ... passed
            And another background step passes ... passed
            Given a step passes ... passed
            Then a step passes ... passed

          Scenario Outline: SO2 -- @1.2
            Given a background step passes ... passed
            And another background step passes ... passed
            Given a step passes ... passed
            Then another step passes ... passed

          Scenario Outline: SO2 -- @1.3
            Given a background step passes ... passed
            And another background step passes ... passed
            Given another step passes ... passed
            Then a step passes ... passed
        """"""
    But note that ""the Background steps are injected into each ScenarioOutline""
    And note that ""the Background steps are executed before any ScenarioOutline steps"""
/content/Record70.rst,"{
Scenario:Link:https://behave.readthedocs.io/en/latest/gherkin/#background
Background
A background consists of a series of steps similar to scenarios. It allows you to add some context to the scenarios of a feature. A background is executed before each scenario of this feature but after any of the before hooks.


Given:It is useful for performing setup operations like:

logging into a web browser or

setting up a database with test data used by the scenarios.

The background description is for the benefit of humans reading the feature text. Again the background name should just be a reasonably descriptive title for the background operation being performed or requirement being met.

A background section may exist only once within a feature file. In addition, a background must be defined before any scenario or scenario outline.


Then:It contains steps as described below.

Good practices for using Background

Don’t use “Background” to set up complicated state unless that state is actually something the client needs to know.
For example, if the user and site names don’t matter to the client, you should use a high-level step such as “Given that I am logged in as a site owner”.

Keep your “Background” section short.
You’re expecting the user to actually remember this stuff when reading your scenarios. If the background is more than 4 lines long, can you move some of the irrelevant details into high-level steps? See calling steps from other steps.

Make your “Background” section vivid.
You should use colorful names and try to tell a story, because the human brain can keep track of stories much better than it can keep track of names like “User A”, “User B”, “Site 1”, and so on.

Keep your scenarios short, and don’t have too many.
If the background section has scrolled off the screen, you should think about using higher-level steps, or splitting the features file in two.

}
",/content/Feature70.feature," Scenario: Failing Background Step causes all ScenarioOutlines to fail
    Given a file named ""features/background_fail_outline_example.feature"" with:
        """"""
        Feature:
          Background:
            Given a background step passes
            And a background step fails
            But another background step passes

          Scenario Outline: SO1
            When a step <outcome1>
            Then another step <outcome2>

            Examples: Alpha
              | outcome1 | outcome2 |
              | passes   | passes   |
              | passes   | fails    |
              | fails    | passes   |
              | fails    | fails    |

          Scenario Outline: SO2
              When <word1> step passes

            Examples: Beta
              | word1   |
              | a       |
              | another |
        """"""
    When I run ""behave -f plain -T features/background_fail_outline_example.feature""
    Then it should fail with:
        """"""
        0 scenarios passed, 6 failed, 0 skipped
        6 steps passed, 6 failed, 16 skipped, 0 undefined
        """"""
    And the command output should contain:
        """"""
        Feature:
          Background:

          Scenario Outline: SO1 -- @1.1 Alpha
            Given a background step passes ... passed
            And a background step fails ... failed
        Assertion Failed: XFAIL: background step

          Scenario Outline: SO1 -- @1.2 Alpha
            Given a background step passes ... passed
            And a background step fails ... failed
        Assertion Failed: XFAIL: background step

          Scenario Outline: SO1 -- @1.3 Alpha
            Given a background step passes ... passed
            And a background step fails ... failed
        Assertion Failed: XFAIL: background step

          Scenario Outline: SO1 -- @1.4 Alpha
            Given a background step passes ... passed
            And a background step fails ... failed
        Assertion Failed: XFAIL: background step

          Scenario Outline: SO2 -- @1.1 Beta
            Given a background step passes ... passed
            And a background step fails ... failed
        Assertion Failed: XFAIL: background step

          Scenario Outline: SO2 -- @1.2 Beta
            Given a background step passes ... passed
            And a background step fails ... failed
        Assertion Failed: XFAIL: background step
        """"""
    But note that ""the failing Background step causes each ScenarioOutline to be marked as skipped"""
/content/Record71.rst,"{
Scenario:Link:https://behave.readthedocs.io/en/latest/gherkin/#background
Background
A background consists of a series of steps similar to scenarios. It allows you to add some context to the scenarios of a feature. A background is executed before each scenario of this feature but after any of the before hooks.


Given:It is useful for performing setup operations like:

logging into a web browser or

setting up a database with test data used by the scenarios.

The background description is for the benefit of humans reading the feature text. Again the background name should just be a reasonably descriptive title for the background operation being performed or requirement being met.

A background section may exist only once within a feature file. In addition, a background must be defined before any scenario or scenario outline.


Then:It contains steps as described below.

Good practices for using Background

Don’t use “Background” to set up complicated state unless that state is actually something the client needs to know.
For example, if the user and site names don’t matter to the client, you should use a high-level step such as “Given that I am logged in as a site owner”.

Keep your “Background” section short.
You’re expecting the user to actually remember this stuff when reading your scenarios. If the background is more than 4 lines long, can you move some of the irrelevant details into high-level steps? See calling steps from other steps.

Make your “Background” section vivid.
You should use colorful names and try to tell a story, because the human brain can keep track of stories much better than it can keep track of names like “User A”, “User B”, “Site 1”, and so on.

Keep your scenarios short, and don’t have too many.
If the background section has scrolled off the screen, you should think about using higher-level steps, or splitting the features file in two.

}
",/content/Feature71.feature," Scenario: Feature with Background after first Scenario should fail (SAD CASE)
    Given a file named ""features/background_sad_example1.feature"" with:
        """"""
        Feature:
          Scenario: S1
            When a step passes

          Background: B1
            Given a background step passes

          Scenario: S2
            Then a step passes
            And another step passes
        """"""
    When I run ""behave -f plain -T features/background_sad_example1.feature""
    Then it should fail with:
        """"""
        Parser failure in state=steps at line 5: ""Background: B1""
        REASON: Background may not occur after Scenario/ScenarioOutline.
        """""""
/content/Record72.rst,"{
Scenario:Link:https://behave.readthedocs.io/en/latest/gherkin/#background
Background
A background consists of a series of steps similar to scenarios. It allows you to add some context to the scenarios of a feature. A background is executed before each scenario of this feature but after any of the before hooks.


Given:It is useful for performing setup operations like:

logging into a web browser or

setting up a database with test data used by the scenarios.

The background description is for the benefit of humans reading the feature text. Again the background name should just be a reasonably descriptive title for the background operation being performed or requirement being met.

A background section may exist only once within a feature file. In addition, a background must be defined before any scenario or scenario outline.


Then:It contains steps as described below.

Good practices for using Background

Don’t use “Background” to set up complicated state unless that state is actually something the client needs to know.
For example, if the user and site names don’t matter to the client, you should use a high-level step such as “Given that I am logged in as a site owner”.

Keep your “Background” section short.
You’re expecting the user to actually remember this stuff when reading your scenarios. If the background is more than 4 lines long, can you move some of the irrelevant details into high-level steps? See calling steps from other steps.

Make your “Background” section vivid.
You should use colorful names and try to tell a story, because the human brain can keep track of stories much better than it can keep track of names like “User A”, “User B”, “Site 1”, and so on.

Keep your scenarios short, and don’t have too many.
If the background section has scrolled off the screen, you should think about using higher-level steps, or splitting the features file in two.

}
",/content/Feature72.feature,"Scenario: Feature with two Backgrounds should fail (SAD CASE)
    Given a file named ""features/background_sad_example2.feature"" with:
        """"""
        Feature:
          Background: B1
            Given a background step passes

          Background: B2 (XFAIL)
            Given another background step passes

          Scenario: S1
            When a step passes

          Scenario: S2
            Then a step passes
            And another step passes
        """"""
    When I run ""behave -f plain -T features/background_sad_example2.feature""
    Then it should fail with:
        """"""
        Parser failure in state=steps at line 5: ""Background: B2 (XFAIL)""
        REASON: Background should not be used here.
        """""""
/content/Record73.rst,"Scenario:Link:https://behave.readthedocs.io/en/latest/formatters/#formatters
Formatters

Given:The following formatters are currently supported:

Name

Mode

Description

help

normal

Shows all registered formatters.

bad_steps

dry-run

Shows BAD STEP-DEFINITIONS (if any exist).

json

normal

JSON dump of test run

json.pretty

normal

JSON dump of test run (human readable)

plain

normal

Very basic formatter with maximum compatibility

pretty

normal

Standard colourised pretty formatter

progress

normal

Shows dotted progress for each executed scenario.

progress2

normal

Shows dotted progress for each executed step.

progress3

normal

Shows detailed progress for each step of a scenario.

rerun

normal

Emits scenario file locations of failing scenarios

sphinx.steps

dry-run

Generate sphinx-based documentation for step definitions.

steps

dry-run

Shows step definitions (step implementations).

steps.catalog

dry-run

Shows non-technical documentation for step definitions.

steps.code

dry-run

Shows executed steps combined with their code.

steps.doc

dry-run

Shows documentation for step definitions.

steps.usage

dry-run

Shows how step definitions are used by steps (in feature files).

tags

dry-run

Shows tags (and how often they are used).

tags.location

dry-run

Shows tags and the location where they are used.

Note

You can use more than one formatter during a test run. But in general you have only one formatter that writes to stdout.

The “Mode” column indicates if a formatter is intended to be used in dry-run (--dry-run command-line option) or normal mode.

Then:Refer to the description of the command line functions

}",/content/Feature73.feature," Scenario: Good case (with builtin formatters)
      Given an empty file named ""behave.ini""
      When I run ""behave --format=help""
      Then it should pass
      And the command output should contain:
        """"""
        AVAILABLE FORMATTERS:
          json           JSON dump of test run
          json.pretty    JSON dump of test run (human readable)
          null           Provides formatter that does not output anything.
          plain          Very basic formatter with maximum compatibility
          pretty         Standard colourised pretty formatter
          progress       Shows dotted progress for each executed scenario.
          progress2      Shows dotted progress for each executed step.
          progress3      Shows detailed progress for each step of a scenario.
          rerun          Emits scenario file locations of failing scenarios
          sphinx.steps   Generate sphinx-based documentation for step definitions.
          steps          Shows step definitions (step implementations).
          steps.bad      Shows BAD STEP-DEFINITION(s) (if any exist).
          steps.catalog  Shows non-technical documentation for step definitions.
          steps.code     Shows executed steps combined with their code.
          steps.doc      Shows documentation for step definitions.
          steps.missing  Shows undefined/missing steps definitions, implements them.
          steps.usage    Shows how step definitions are used by steps.
          tags           Shows tags (and how often they are used).
          tags.location  Shows tags and the location where they are used.
        """""""
/content/Record74.rst,"Scenario:Link:https://behave.readthedocs.io/en/latest/formatters/#formatters
Formatters

Given:The following formatters are currently supported:

Name

Mode

Description

help

normal

Shows all registered formatters.

bad_steps

dry-run

Shows BAD STEP-DEFINITIONS (if any exist).

json

normal

JSON dump of test run

json.pretty

normal

JSON dump of test run (human readable)

plain

normal

Very basic formatter with maximum compatibility

pretty

normal

Standard colourised pretty formatter

progress

normal

Shows dotted progress for each executed scenario.

progress2

normal

Shows dotted progress for each executed step.

progress3

normal

Shows detailed progress for each step of a scenario.

rerun

normal

Emits scenario file locations of failing scenarios

sphinx.steps

dry-run

Generate sphinx-based documentation for step definitions.

steps

dry-run

Shows step definitions (step implementations).

steps.catalog

dry-run

Shows non-technical documentation for step definitions.

steps.code

dry-run

Shows executed steps combined with their code.

steps.doc

dry-run

Shows documentation for step definitions.

steps.usage

dry-run

Shows how step definitions are used by steps (in feature files).

tags

dry-run

Shows tags (and how often they are used).

tags.location

dry-run

Shows tags and the location where they are used.

Note

You can use more than one formatter during a test run. But in general you have only one formatter that writes to stdout.

The “Mode” column indicates if a formatter is intended to be used in dry-run (--dry-run command-line option) or normal mode.

Then:Refer to the description of the command line functions

}",/content/Feature74.feature,"Scenario: Good Formatter by using a Formatter-Alias
      Given an empty file named ""behave4me/__init__.py""
      And a file named ""behave4me/good_formatter.py"" with:
        """"""
        from behave.formatter.base import Formatter

        class SomeFormatter(Formatter):
            name = ""some""
            description = ""Very basic formatter for Some format.""

            def __init__(self, stream_opener, config):
                super(SomeFormatter, self).__init__(stream_opener, config)
        """"""
      And a file named ""behave.ini"" with:
        """"""
        [behave.formatters]
        some = behave4me.good_formatter:SomeFormatter
        """"""
      When I run ""behave --format=help""
      Then it should pass
      And the command output should contain:
        """"""
        rerun          Emits scenario file locations of failing scenarios
        some           Very basic formatter for Some format.
        sphinx.steps   Generate sphinx-based documentation for step definitions.
        """"""
      And note that ""the new formatter appears in the sorted list of formatters""
      But the command output should not contain ""UNAVAILABLE FORMATTERS"""
/content/Record75.rst,"Scenario:Link:https://behave.readthedocs.io/en/latest/formatters/#formatters
Formatters

Given:The following formatters are currently supported:

Name

Mode

Description

help

normal

Shows all registered formatters.

bad_steps

dry-run

Shows BAD STEP-DEFINITIONS (if any exist).

json

normal

JSON dump of test run

json.pretty

normal

JSON dump of test run (human readable)

plain

normal

Very basic formatter with maximum compatibility

pretty

normal

Standard colourised pretty formatter

progress

normal

Shows dotted progress for each executed scenario.

progress2

normal

Shows dotted progress for each executed step.

progress3

normal

Shows detailed progress for each step of a scenario.

rerun

normal

Emits scenario file locations of failing scenarios

sphinx.steps

dry-run

Generate sphinx-based documentation for step definitions.

steps

dry-run

Shows step definitions (step implementations).

steps.catalog

dry-run

Shows non-technical documentation for step definitions.

steps.code

dry-run

Shows executed steps combined with their code.

steps.doc

dry-run

Shows documentation for step definitions.

steps.usage

dry-run

Shows how step definitions are used by steps (in feature files).

tags

dry-run

Shows tags (and how often they are used).

tags.location

dry-run

Shows tags and the location where they are used.

Note

You can use more than one formatter during a test run. But in general you have only one formatter that writes to stdout.

The “Mode” column indicates if a formatter is intended to be used in dry-run (--dry-run command-line option) or normal mode.

Then:Refer to the description of the command line functions

}",/content/Feature75.feature,"Scenario Template: Bad Formatter with <formatter_syndrome>
      Given a file named ""behave.ini"" with:
        """"""
        [behave.formatters]
        <formatter_name> = <formatter_class>
        """"""
      When I run ""behave --format=help""
      Then it should pass
      And the command output should contain:
        """"""
        UNAVAILABLE FORMATTERS:
          <formatter_name>  <formatter_syndrome>: <problem_description>
        """"""

      @use.with_python.min_version=3.6
      Examples: For Python >= 3.6
        | formatter_name | formatter_class                           | formatter_syndrome  | problem_description |
        | bad_formatter1 | behave4me.unknown:Formatter               | ModuleNotFoundError | No module named 'behave4me.unknown' |

      @not.with_python.min_version=3.6
      @use.with_pypy=true
      Examples: For Python < 3.6
        | formatter_name | formatter_class                           | formatter_syndrome  | problem_description |
        | bad_formatter1 | behave4me.unknown:Formatter               | ModuleNotFoundError | No module named 'behave4me.unknown' |

      @not.with_python.min_version=3.6
      @not.with_pypy=true
      Examples: For Python < 3.6
        | formatter_name | formatter_class                           | formatter_syndrome  | problem_description |
        | bad_formatter1 | behave4me.unknown:Formatter               | ModuleNotFoundError | No module named 'unknown' |

      Examples:
        | formatter_name | formatter_class                           | formatter_syndrome  | problem_description |
        | bad_formatter2 | behave4me.bad_formatter:UnknownFormatter  | ClassNotFoundError  | behave4me.bad_formatter:UnknownFormatter |
        | bad_formatter3 | behave4me.bad_formatter:InvalidFormatter1 | InvalidClassError   | is not a subclass-of Formatter |
        | bad_formatter4 | behave4me.bad_formatter:InvalidFormatter2 | InvalidClassError   | is not a class |"
/content/Record76.rst,"Scenario:Link:https://behave.readthedocs.io/en/latest/formatters/#formatters
Formatters

Given:The following formatters are currently supported:

Name

Mode

Description

help

normal

Shows all registered formatters.

bad_steps

dry-run

Shows BAD STEP-DEFINITIONS (if any exist).

json

normal

JSON dump of test run

json.pretty

normal

JSON dump of test run (human readable)

plain

normal

Very basic formatter with maximum compatibility

pretty

normal

Standard colourised pretty formatter

progress

normal

Shows dotted progress for each executed scenario.

progress2

normal

Shows dotted progress for each executed step.

progress3

normal

Shows detailed progress for each step of a scenario.

rerun

normal

Emits scenario file locations of failing scenarios

sphinx.steps

dry-run

Generate sphinx-based documentation for step definitions.

steps

dry-run

Shows step definitions (step implementations).

steps.catalog

dry-run

Shows non-technical documentation for step definitions.

steps.code

dry-run

Shows executed steps combined with their code.

steps.doc

dry-run

Shows documentation for step definitions.

steps.usage

dry-run

Shows how step definitions are used by steps (in feature files).

tags

dry-run

Shows tags (and how often they are used).

tags.location

dry-run

Shows tags and the location where they are used.

Note

You can use more than one formatter during a test run. But in general you have only one formatter that writes to stdout.

The “Mode” column indicates if a formatter is intended to be used in dry-run (--dry-run command-line option) or normal mode.

Then:Refer to the description of the command line functions

}",/content/Feature76.feature," Scenario: Multiple Bad Formatters
      Given a file named ""behave.ini"" with:
        """"""
        [behave.formatters]
        bad_formatter2 = behave4me.bad_formatter:UnknownFormatter
        bad_formatter3 = behave4me.bad_formatter:InvalidFormatter1
        """"""
      When I run ""behave --format=help""
      Then it should pass
      And the command output should contain:
        """"""
        UNAVAILABLE FORMATTERS:
          bad_formatter2  ClassNotFoundError: behave4me.bad_formatter:UnknownFormatter
          bad_formatter3  InvalidClassError: is not a subclass-of Formatter
        """"""
      And note that ""the list of UNAVAILABLE FORMATTERS is sorted-by-name"""
/content/Record77.rst,"{
Scenario:Link:https://behave.readthedocs.io/en/latest/formatters/#formatters,https://behave.readthedocs.io/en/latest/formatters/#user-defined-formatters
Formatters

Given:The following formatters are currently supported:

Name

Mode

Description

help

normal

Shows all registered formatters.

bad_steps

dry-run

Shows BAD STEP-DEFINITIONS (if any exist).

json

normal

JSON dump of test run

json.pretty

normal

JSON dump of test run (human readable)

plain

normal

Very basic formatter with maximum compatibility

pretty

normal

Standard colourised pretty formatter

progress

normal

Shows dotted progress for each executed scenario.

progress2

normal

Shows dotted progress for each executed step.

progress3

normal

Shows detailed progress for each step of a scenario.

rerun

normal

Emits scenario file locations of failing scenarios

sphinx.steps

dry-run

Generate sphinx-based documentation for step definitions.

steps

dry-run

Shows step definitions (step implementations).

steps.catalog

dry-run

Shows non-technical documentation for step definitions.

steps.code

dry-run

Shows executed steps combined with their code.

steps.doc

dry-run

Shows documentation for step definitions.

steps.usage

dry-run

Shows how step definitions are used by steps (in feature files).

tags

dry-run

Shows tags (and how often they are used).

tags.location

dry-run

Shows tags and the location where they are used.

Note

You can use more than one formatter during a test run. But in general you have only one formatter that writes to stdout.

The “Mode” column indicates if a formatter is intended to be used in dry-run (--dry-run command-line option) or normal mode.

Then:Refer to the description of command line arguments
User-Defined Formatters
Behave allows you to provide your own formatter (class):

# -- USE: Formatter class ""Json2Formatter"" in python module ""foo.bar""
# NOTE: Formatter must be importable from python search path.
behave -f foo.bar:Json2Formatter ...
The usage of a user-defined formatter can be simplified by providing an alias name for it in the configuration file:

# -- FILE: behave.ini
# ALIAS SUPPORTS: behave -f json2 ...
# NOTE: Formatter aliases may override builtin formatters.
[behave.formatters]
json2 = foo.bar:Json2Formatter
If your formatter can be configured, you should use the userdata concept to provide them. The formatter should use the attribute schema:

# -- FILE: behave.ini
# SCHEMA: behave.formatter.<FORMATTER_NAME>.<ATTRIBUTE_NAME>
[behave.userdata]
behave.formatter.json2.use_pretty = true

# -- SUPPORTS ALSO:
#    behave -f json2 -D behave.formatter.json2.use_pretty ...

}",/content/Feature77.feature,"Scenario: Feature Setup
        Given a new working directory
        And a file named ""features/steps/steps.py"" with:
            """"""
            from behave import step

            @step('a step passes')
            def step_passes(context):
                pass

            @step('a step fails')
            def step_fails(context):
                assert False, ""XFAIL-STEP""
            """"""


"
/content/Record78.rst,"{
Scenario:Link:https://behave.readthedocs.io/en/latest/formatters/#formatters,https://behave.readthedocs.io/en/latest/formatters/#user-defined-formatters
Formatters

Given:The following formatters are currently supported:

Name

Mode

Description

help

normal

Shows all registered formatters.

bad_steps

dry-run

Shows BAD STEP-DEFINITIONS (if any exist).

json

normal

JSON dump of test run

json.pretty

normal

JSON dump of test run (human readable)

plain

normal

Very basic formatter with maximum compatibility

pretty

normal

Standard colourised pretty formatter

progress

normal

Shows dotted progress for each executed scenario.

progress2

normal

Shows dotted progress for each executed step.

progress3

normal

Shows detailed progress for each step of a scenario.

rerun

normal

Emits scenario file locations of failing scenarios

sphinx.steps

dry-run

Generate sphinx-based documentation for step definitions.

steps

dry-run

Shows step definitions (step implementations).

steps.catalog

dry-run

Shows non-technical documentation for step definitions.

steps.code

dry-run

Shows executed steps combined with their code.

steps.doc

dry-run

Shows documentation for step definitions.

steps.usage

dry-run

Shows how step definitions are used by steps (in feature files).

tags

dry-run

Shows tags (and how often they are used).

tags.location

dry-run

Shows tags and the location where they are used.

Note

You can use more than one formatter during a test run. But in general you have only one formatter that writes to stdout.

The “Mode” column indicates if a formatter is intended to be used in dry-run (--dry-run command-line option) or normal mode.

Then:Refer to the description of command line arguments
User-Defined Formatters
Behave allows you to provide your own formatter (class):

# -- USE: Formatter class ""Json2Formatter"" in python module ""foo.bar""
# NOTE: Formatter must be importable from python search path.
behave -f foo.bar:Json2Formatter ...
The usage of a user-defined formatter can be simplified by providing an alias name for it in the configuration file:

# -- FILE: behave.ini
# ALIAS SUPPORTS: behave -f json2 ...
# NOTE: Formatter aliases may override builtin formatters.
[behave.formatters]
json2 = foo.bar:Json2Formatter
If your formatter can be configured, you should use the userdata concept to provide them. The formatter should use the attribute schema:

# -- FILE: behave.ini
# SCHEMA: behave.formatter.<FORMATTER_NAME>.<ATTRIBUTE_NAME>
[behave.userdata]
behave.formatter.json2.use_pretty = true

# -- SUPPORTS ALSO:
#    behave -f json2 -D behave.formatter.json2.use_pretty ...

}",/content/Feature78.feature," Scenario: Use JSON formatter on simple feature
        Given a file named ""features/simple_feature_with_name.feature"" with:
            """"""
            Feature: Simple, empty Feature
            """"""
        When I run ""behave -f json.pretty features/simple_feature_with_name.feature""
        Then it should pass with:
            """"""
            0 features passed, 0 failed, 1 skipped
            0 scenarios passed, 0 failed, 0 skipped
            """"""
        And the command output should contain:
            """"""
            [
              {
                ""keyword"": ""Feature"",
                ""location"": ""features/simple_feature_with_name.feature:1"",
                ""name"": ""Simple, empty Feature"",
                ""status"": ""skipped"",
                ""tags"": []
              }
            ]
            """""""
/content/Record79.rst,"{
Scenario:Link:https://behave.readthedocs.io/en/latest/formatters/#formatters,https://behave.readthedocs.io/en/latest/formatters/#user-defined-formatters
Formatters

Given:The following formatters are currently supported:

Name

Mode

Description

help

normal

Shows all registered formatters.

bad_steps

dry-run

Shows BAD STEP-DEFINITIONS (if any exist).

json

normal

JSON dump of test run

json.pretty

normal

JSON dump of test run (human readable)

plain

normal

Very basic formatter with maximum compatibility

pretty

normal

Standard colourised pretty formatter

progress

normal

Shows dotted progress for each executed scenario.

progress2

normal

Shows dotted progress for each executed step.

progress3

normal

Shows detailed progress for each step of a scenario.

rerun

normal

Emits scenario file locations of failing scenarios

sphinx.steps

dry-run

Generate sphinx-based documentation for step definitions.

steps

dry-run

Shows step definitions (step implementations).

steps.catalog

dry-run

Shows non-technical documentation for step definitions.

steps.code

dry-run

Shows executed steps combined with their code.

steps.doc

dry-run

Shows documentation for step definitions.

steps.usage

dry-run

Shows how step definitions are used by steps (in feature files).

tags

dry-run

Shows tags (and how often they are used).

tags.location

dry-run

Shows tags and the location where they are used.

Note

You can use more than one formatter during a test run. But in general you have only one formatter that writes to stdout.

The “Mode” column indicates if a formatter is intended to be used in dry-run (--dry-run command-line option) or normal mode.

Then:Refer to the description of command line arguments
User-Defined Formatters
Behave allows you to provide your own formatter (class):

# -- USE: Formatter class ""Json2Formatter"" in python module ""foo.bar""
# NOTE: Formatter must be importable from python search path.
behave -f foo.bar:Json2Formatter ...
The usage of a user-defined formatter can be simplified by providing an alias name for it in the configuration file:

# -- FILE: behave.ini
# ALIAS SUPPORTS: behave -f json2 ...
# NOTE: Formatter aliases may override builtin formatters.
[behave.formatters]
json2 = foo.bar:Json2Formatter
If your formatter can be configured, you should use the userdata concept to provide them. The formatter should use the attribute schema:

# -- FILE: behave.ini
# SCHEMA: behave.formatter.<FORMATTER_NAME>.<ATTRIBUTE_NAME>
[behave.userdata]
behave.formatter.json2.use_pretty = true

# -- SUPPORTS ALSO:
#    behave -f json2 -D behave.formatter.json2.use_pretty ...

}",/content/Feature79.feature," Scenario: Use JSON formatter on simple feature with description
        Given a file named ""features/simple_feature_with_description.feature"" with:
            """"""
            Feature: Simple feature with description

                First feature description line.
                Second feature description line.

                Third feature description line (following an empty line).
            """"""
        When I run ""behave -f json.pretty features/simple_feature_with_description.feature""
        Then it should pass with:
            """"""
            0 features passed, 0 failed, 1 skipped
            0 scenarios passed, 0 failed, 0 skipped
            """"""
        And the command output should contain:
            """"""
            [
              {
                ""description"": [
                  ""First feature description line."",
                  ""Second feature description line."",
                  ""Third feature description line (following an empty line).""
                ],
                ""keyword"": ""Feature"",
                ""location"": ""features/simple_feature_with_description.feature:1"",
                ""name"": ""Simple feature with description"",
                ""status"": ""skipped"",
                ""tags"": []
              }
            ]
            """""""
/content/Record80.rst,"{
Scenario:Link:https://behave.readthedocs.io/en/latest/formatters/#formatters,https://behave.readthedocs.io/en/latest/formatters/#user-defined-formatters
Formatters

Given:The following formatters are currently supported:

Name

Mode

Description

help

normal

Shows all registered formatters.

bad_steps

dry-run

Shows BAD STEP-DEFINITIONS (if any exist).

json

normal

JSON dump of test run

json.pretty

normal

JSON dump of test run (human readable)

plain

normal

Very basic formatter with maximum compatibility

pretty

normal

Standard colourised pretty formatter

progress

normal

Shows dotted progress for each executed scenario.

progress2

normal

Shows dotted progress for each executed step.

progress3

normal

Shows detailed progress for each step of a scenario.

rerun

normal

Emits scenario file locations of failing scenarios

sphinx.steps

dry-run

Generate sphinx-based documentation for step definitions.

steps

dry-run

Shows step definitions (step implementations).

steps.catalog

dry-run

Shows non-technical documentation for step definitions.

steps.code

dry-run

Shows executed steps combined with their code.

steps.doc

dry-run

Shows documentation for step definitions.

steps.usage

dry-run

Shows how step definitions are used by steps (in feature files).

tags

dry-run

Shows tags (and how often they are used).

tags.location

dry-run

Shows tags and the location where they are used.

Note

You can use more than one formatter during a test run. But in general you have only one formatter that writes to stdout.

The “Mode” column indicates if a formatter is intended to be used in dry-run (--dry-run command-line option) or normal mode.

Then:Refer to the description of command line arguments
User-Defined Formatters
Behave allows you to provide your own formatter (class):

# -- USE: Formatter class ""Json2Formatter"" in python module ""foo.bar""
# NOTE: Formatter must be importable from python search path.
behave -f foo.bar:Json2Formatter ...
The usage of a user-defined formatter can be simplified by providing an alias name for it in the configuration file:

# -- FILE: behave.ini
# ALIAS SUPPORTS: behave -f json2 ...
# NOTE: Formatter aliases may override builtin formatters.
[behave.formatters]
json2 = foo.bar:Json2Formatter
If your formatter can be configured, you should use the userdata concept to provide them. The formatter should use the attribute schema:

# -- FILE: behave.ini
# SCHEMA: behave.formatter.<FORMATTER_NAME>.<ATTRIBUTE_NAME>
[behave.userdata]
behave.formatter.json2.use_pretty = true

# -- SUPPORTS ALSO:
#    behave -f json2 -D behave.formatter.json2.use_pretty ...

}",/content/Feature80.feature,"Scenario: Use JSON formatter on simple feature with tags
        Given a file named ""features/simple_feature_with_tags.feature"" with:
            """"""
            @foo @bar
            Feature: Simple feature with tags
            """"""
        When I run ""behave -f json.pretty features/simple_feature_with_tags.feature""
        Then it should pass with:
            """"""
            0 features passed, 0 failed, 1 skipped
            0 scenarios passed, 0 failed, 0 skipped
            """"""
        And the command output should contain:
            """"""
            [
              {
                ""keyword"": ""Feature"",
                ""location"": ""features/simple_feature_with_tags.feature:2"",
                ""name"": ""Simple feature with tags"",
                ""status"": ""skipped"",
                ""tags"": [
                  ""foo"",
                  ""bar""
                ]
              }
            ]
            """""""
/content/Record81.rst,"{
Scenario:Link:https://behave.readthedocs.io/en/latest/formatters/#formatters,https://behave.readthedocs.io/en/latest/formatters/#user-defined-formatters
Formatters

Given:The following formatters are currently supported:

Name

Mode

Description

help

normal

Shows all registered formatters.

bad_steps

dry-run

Shows BAD STEP-DEFINITIONS (if any exist).

json

normal

JSON dump of test run

json.pretty

normal

JSON dump of test run (human readable)

plain

normal

Very basic formatter with maximum compatibility

pretty

normal

Standard colourised pretty formatter

progress

normal

Shows dotted progress for each executed scenario.

progress2

normal

Shows dotted progress for each executed step.

progress3

normal

Shows detailed progress for each step of a scenario.

rerun

normal

Emits scenario file locations of failing scenarios

sphinx.steps

dry-run

Generate sphinx-based documentation for step definitions.

steps

dry-run

Shows step definitions (step implementations).

steps.catalog

dry-run

Shows non-technical documentation for step definitions.

steps.code

dry-run

Shows executed steps combined with their code.

steps.doc

dry-run

Shows documentation for step definitions.

steps.usage

dry-run

Shows how step definitions are used by steps (in feature files).

tags

dry-run

Shows tags (and how often they are used).

tags.location

dry-run

Shows tags and the location where they are used.

Note

You can use more than one formatter during a test run. But in general you have only one formatter that writes to stdout.

The “Mode” column indicates if a formatter is intended to be used in dry-run (--dry-run command-line option) or normal mode.

Then:Refer to the description of command line arguments
User-Defined Formatters
Behave allows you to provide your own formatter (class):

# -- USE: Formatter class ""Json2Formatter"" in python module ""foo.bar""
# NOTE: Formatter must be importable from python search path.
behave -f foo.bar:Json2Formatter ...
The usage of a user-defined formatter can be simplified by providing an alias name for it in the configuration file:

# -- FILE: behave.ini
# ALIAS SUPPORTS: behave -f json2 ...
# NOTE: Formatter aliases may override builtin formatters.
[behave.formatters]
json2 = foo.bar:Json2Formatter
If your formatter can be configured, you should use the userdata concept to provide them. The formatter should use the attribute schema:

# -- FILE: behave.ini
# SCHEMA: behave.formatter.<FORMATTER_NAME>.<ATTRIBUTE_NAME>
[behave.userdata]
behave.formatter.json2.use_pretty = true

# -- SUPPORTS ALSO:
#    behave -f json2 -D behave.formatter.json2.use_pretty ...

}",/content/Feature81.feature,"Scenario: Use JSON formatter with feature and one scenario without steps
        Given a file named ""features/simple_scenario.feature"" with:
            """"""
            Feature:
              Scenario: Simple scenario without steps
            """"""
        When I run ""behave -f json.pretty features/simple_scenario.feature""
        Then it should pass with:
            """"""
            1 feature passed, 0 failed, 0 skipped
            1 scenario passed, 0 failed, 0 skipped
            """"""
        And the command output should contain:
            """"""
            [
              {
                ""elements"": [
                  {
                    ""keyword"": ""Scenario"",
                    ""location"": ""features/simple_scenario.feature:2"",
                    ""name"": ""Simple scenario without steps"",
                    ""status"": ""passed"",
                    ""steps"": [],
                    ""tags"": [],
                    ""type"": ""scenario""
                  }
                ],
                ""keyword"": ""Feature"",
                ""location"": ""features/simple_scenario.feature:1"",
                ""name"": """",
                ""status"": ""passed"",
                ""tags"": []
              }
            ]
            """""""
/content/Record82.rst,"{
Scenario:Link:https://behave.readthedocs.io/en/latest/formatters/#formatters,https://behave.readthedocs.io/en/latest/formatters/#user-defined-formatters
Formatters

Given:The following formatters are currently supported:

Name

Mode

Description

help

normal

Shows all registered formatters.

bad_steps

dry-run

Shows BAD STEP-DEFINITIONS (if any exist).

json

normal

JSON dump of test run

json.pretty

normal

JSON dump of test run (human readable)

plain

normal

Very basic formatter with maximum compatibility

pretty

normal

Standard colourised pretty formatter

progress

normal

Shows dotted progress for each executed scenario.

progress2

normal

Shows dotted progress for each executed step.

progress3

normal

Shows detailed progress for each step of a scenario.

rerun

normal

Emits scenario file locations of failing scenarios

sphinx.steps

dry-run

Generate sphinx-based documentation for step definitions.

steps

dry-run

Shows step definitions (step implementations).

steps.catalog

dry-run

Shows non-technical documentation for step definitions.

steps.code

dry-run

Shows executed steps combined with their code.

steps.doc

dry-run

Shows documentation for step definitions.

steps.usage

dry-run

Shows how step definitions are used by steps (in feature files).

tags

dry-run

Shows tags (and how often they are used).

tags.location

dry-run

Shows tags and the location where they are used.

Note

You can use more than one formatter during a test run. But in general you have only one formatter that writes to stdout.

The “Mode” column indicates if a formatter is intended to be used in dry-run (--dry-run command-line option) or normal mode.

Then:Refer to the description of command line arguments
User-Defined Formatters
Behave allows you to provide your own formatter (class):

# -- USE: Formatter class ""Json2Formatter"" in python module ""foo.bar""
# NOTE: Formatter must be importable from python search path.
behave -f foo.bar:Json2Formatter ...
The usage of a user-defined formatter can be simplified by providing an alias name for it in the configuration file:

# -- FILE: behave.ini
# ALIAS SUPPORTS: behave -f json2 ...
# NOTE: Formatter aliases may override builtin formatters.
[behave.formatters]
json2 = foo.bar:Json2Formatter
If your formatter can be configured, you should use the userdata concept to provide them. The formatter should use the attribute schema:

# -- FILE: behave.ini
# SCHEMA: behave.formatter.<FORMATTER_NAME>.<ATTRIBUTE_NAME>
[behave.userdata]
behave.formatter.json2.use_pretty = true

# -- SUPPORTS ALSO:
#    behave -f json2 -D behave.formatter.json2.use_pretty ...

}",/content/Feature82.feature," Scenario: Use JSON formatter with feature and one scenario with description
        Given a file named ""features/simple_scenario_with_description.feature"" with:
            """"""
            Feature:
              Scenario: Simple scenario with description but without steps

                First scenario description line.
                Second scenario description line.

                Third scenario description line (after an empty line).
            """"""
        When I run ""behave -f json.pretty features/simple_scenario_with_description.feature""
        Then it should pass with:
            """"""
            1 feature passed, 0 failed, 0 skipped
            1 scenario passed, 0 failed, 0 skipped
            """"""
        And the command output should contain:
            """"""
            [
              {
                ""elements"": [
                  {
                    ""description"": [
                      ""First scenario description line."",
                      ""Second scenario description line."",
                      ""Third scenario description line (after an empty line).""
                    ],
                    ""keyword"": ""Scenario"",
                    ""location"": ""features/simple_scenario_with_description.feature:2"",
                    ""name"": ""Simple scenario with description but without steps"",
                    ""status"": ""passed"",
                    ""steps"": [],
                    ""tags"": [],
                    ""type"": ""scenario""
                  }
                ],
                ""keyword"": ""Feature"",
                ""location"": ""features/simple_scenario_with_description.feature:1"",
                ""name"": """",
                ""status"": ""passed"",
                ""tags"": []
              }
            ]
            """""""
/content/Record83.rst,"{
Scenario:Link:https://behave.readthedocs.io/en/latest/formatters/#formatters,https://behave.readthedocs.io/en/latest/formatters/#user-defined-formatters
Formatters

Given:The following formatters are currently supported:

Name

Mode

Description

help

normal

Shows all registered formatters.

bad_steps

dry-run

Shows BAD STEP-DEFINITIONS (if any exist).

json

normal

JSON dump of test run

json.pretty

normal

JSON dump of test run (human readable)

plain

normal

Very basic formatter with maximum compatibility

pretty

normal

Standard colourised pretty formatter

progress

normal

Shows dotted progress for each executed scenario.

progress2

normal

Shows dotted progress for each executed step.

progress3

normal

Shows detailed progress for each step of a scenario.

rerun

normal

Emits scenario file locations of failing scenarios

sphinx.steps

dry-run

Generate sphinx-based documentation for step definitions.

steps

dry-run

Shows step definitions (step implementations).

steps.catalog

dry-run

Shows non-technical documentation for step definitions.

steps.code

dry-run

Shows executed steps combined with their code.

steps.doc

dry-run

Shows documentation for step definitions.

steps.usage

dry-run

Shows how step definitions are used by steps (in feature files).

tags

dry-run

Shows tags (and how often they are used).

tags.location

dry-run

Shows tags and the location where they are used.

Note

You can use more than one formatter during a test run. But in general you have only one formatter that writes to stdout.

The “Mode” column indicates if a formatter is intended to be used in dry-run (--dry-run command-line option) or normal mode.

Then:Refer to the description of command line arguments
User-Defined Formatters
Behave allows you to provide your own formatter (class):

# -- USE: Formatter class ""Json2Formatter"" in python module ""foo.bar""
# NOTE: Formatter must be importable from python search path.
behave -f foo.bar:Json2Formatter ...
The usage of a user-defined formatter can be simplified by providing an alias name for it in the configuration file:

# -- FILE: behave.ini
# ALIAS SUPPORTS: behave -f json2 ...
# NOTE: Formatter aliases may override builtin formatters.
[behave.formatters]
json2 = foo.bar:Json2Formatter
If your formatter can be configured, you should use the userdata concept to provide them. The formatter should use the attribute schema:

# -- FILE: behave.ini
# SCHEMA: behave.formatter.<FORMATTER_NAME>.<ATTRIBUTE_NAME>
[behave.userdata]
behave.formatter.json2.use_pretty = true

# -- SUPPORTS ALSO:
#    behave -f json2 -D behave.formatter.json2.use_pretty ...

}",/content/Feature83.feature,"Scenario: Use JSON formatter with feature and one scenario with tags
        Given a file named ""features/simple_scenario_with_tags.feature"" with:
            """"""
            Feature:

              @foo
              @bar
              Scenario: Simple scenario with tags but without steps
            """"""
        When I run ""behave -f json.pretty features/simple_scenario_with_tags.feature""
        Then it should pass with:
            """"""
            1 feature passed, 0 failed, 0 skipped
            1 scenario passed, 0 failed, 0 skipped
            """"""
        And the command output should contain:
            """"""
            [
              {
                ""elements"": [
                  {
                    ""keyword"": ""Scenario"",
                    ""location"": ""features/simple_scenario_with_tags.feature:5"",
                    ""name"": ""Simple scenario with tags but without steps"",
                    ""status"": ""passed"",
                    ""steps"": [],
                    ""tags"": [
                      ""foo"",
                      ""bar""
                    ],
                    ""type"": ""scenario""
                  }
                ],
                ""keyword"": ""Feature"",
                ""location"": ""features/simple_scenario_with_tags.feature:1"",
                ""name"": """",
                ""status"": ""passed"",
                ""tags"": []
              }
            ]
            """""""
/content/Record84.rst,"{
Scenario:Link:https://behave.readthedocs.io/en/latest/formatters/#formatters,https://behave.readthedocs.io/en/latest/formatters/#user-defined-formatters
Formatters

Given:The following formatters are currently supported:

Name

Mode

Description

help

normal

Shows all registered formatters.

bad_steps

dry-run

Shows BAD STEP-DEFINITIONS (if any exist).

json

normal

JSON dump of test run

json.pretty

normal

JSON dump of test run (human readable)

plain

normal

Very basic formatter with maximum compatibility

pretty

normal

Standard colourised pretty formatter

progress

normal

Shows dotted progress for each executed scenario.

progress2

normal

Shows dotted progress for each executed step.

progress3

normal

Shows detailed progress for each step of a scenario.

rerun

normal

Emits scenario file locations of failing scenarios

sphinx.steps

dry-run

Generate sphinx-based documentation for step definitions.

steps

dry-run

Shows step definitions (step implementations).

steps.catalog

dry-run

Shows non-technical documentation for step definitions.

steps.code

dry-run

Shows executed steps combined with their code.

steps.doc

dry-run

Shows documentation for step definitions.

steps.usage

dry-run

Shows how step definitions are used by steps (in feature files).

tags

dry-run

Shows tags (and how often they are used).

tags.location

dry-run

Shows tags and the location where they are used.

Note

You can use more than one formatter during a test run. But in general you have only one formatter that writes to stdout.

The “Mode” column indicates if a formatter is intended to be used in dry-run (--dry-run command-line option) or normal mode.

Then:Refer to the description of command line arguments
User-Defined Formatters
Behave allows you to provide your own formatter (class):

# -- USE: Formatter class ""Json2Formatter"" in python module ""foo.bar""
# NOTE: Formatter must be importable from python search path.
behave -f foo.bar:Json2Formatter ...
The usage of a user-defined formatter can be simplified by providing an alias name for it in the configuration file:

# -- FILE: behave.ini
# ALIAS SUPPORTS: behave -f json2 ...
# NOTE: Formatter aliases may override builtin formatters.
[behave.formatters]
json2 = foo.bar:Json2Formatter
If your formatter can be configured, you should use the userdata concept to provide them. The formatter should use the attribute schema:

# -- FILE: behave.ini
# SCHEMA: behave.formatter.<FORMATTER_NAME>.<ATTRIBUTE_NAME>
[behave.userdata]
behave.formatter.json2.use_pretty = true

# -- SUPPORTS ALSO:
#    behave -f json2 -D behave.formatter.json2.use_pretty ...

}",/content/Feature84.feature," Scenario: Use JSON formatter and embed binary data in report from two steps
      Given a file named ""features/json_embeddings.feature"" with:
          """"""
          Feature:
            Scenario: Use embeddings
                Given ""foobar"" as plain text
                And ""red"" as plain text
          """"""
      And a file named ""features/steps/json_embeddings_steps.py"" with:
          """"""
          from behave import step

          @step('""{data}"" as plain text')
          def step_string(context, data):
              context.attach(""text/plain"", data.encode(""utf-8""))
          """"""
      When I run ""behave -f json.pretty features/json_embeddings.feature""
      Then it should pass with:
          """"""
          1 feature passed, 0 failed, 0 skipped
          1 scenario passed, 0 failed, 0 skipped
          """"""
      And the command output should contain:
          """"""
                    ""embeddings"": [
                      {
                        ""data"": ""Zm9vYmFy"",
                        ""mime_type"": ""text/plain""
                      }
                    ],
          """"""
      And the command output should contain:
          """"""
                    ""embeddings"": [
                      {
                        ""data"": ""cmVk"",
                        ""mime_type"": ""text/plain""
                      }
                    ],
          """""""
/content/Record85.rst,"{
Scenario:Link:https://behave.readthedocs.io/en/latest/formatters/#formatters,https://behave.readthedocs.io/en/latest/formatters/#user-defined-formatters
Formatters

Given:The following formatters are currently supported:

Name

Mode

Description

help

normal

Shows all registered formatters.

bad_steps

dry-run

Shows BAD STEP-DEFINITIONS (if any exist).

json

normal

JSON dump of test run

json.pretty

normal

JSON dump of test run (human readable)

plain

normal

Very basic formatter with maximum compatibility

pretty

normal

Standard colourised pretty formatter

progress

normal

Shows dotted progress for each executed scenario.

progress2

normal

Shows dotted progress for each executed step.

progress3

normal

Shows detailed progress for each step of a scenario.

rerun

normal

Emits scenario file locations of failing scenarios

sphinx.steps

dry-run

Generate sphinx-based documentation for step definitions.

steps

dry-run

Shows step definitions (step implementations).

steps.catalog

dry-run

Shows non-technical documentation for step definitions.

steps.code

dry-run

Shows executed steps combined with their code.

steps.doc

dry-run

Shows documentation for step definitions.

steps.usage

dry-run

Shows how step definitions are used by steps (in feature files).

tags

dry-run

Shows tags (and how often they are used).

tags.location

dry-run

Shows tags and the location where they are used.

Note

You can use more than one formatter during a test run. But in general you have only one formatter that writes to stdout.

The “Mode” column indicates if a formatter is intended to be used in dry-run (--dry-run command-line option) or normal mode.

Then:Refer to the description of command line arguments
User-Defined Formatters
Behave allows you to provide your own formatter (class):

# -- USE: Formatter class ""Json2Formatter"" in python module ""foo.bar""
# NOTE: Formatter must be importable from python search path.
behave -f foo.bar:Json2Formatter ...
The usage of a user-defined formatter can be simplified by providing an alias name for it in the configuration file:

# -- FILE: behave.ini
# ALIAS SUPPORTS: behave -f json2 ...
# NOTE: Formatter aliases may override builtin formatters.
[behave.formatters]
json2 = foo.bar:Json2Formatter
If your formatter can be configured, you should use the userdata concept to provide them. The formatter should use the attribute schema:

# -- FILE: behave.ini
# SCHEMA: behave.formatter.<FORMATTER_NAME>.<ATTRIBUTE_NAME>
[behave.userdata]
behave.formatter.json2.use_pretty = true

# -- SUPPORTS ALSO:
#    behave -f json2 -D behave.formatter.json2.use_pretty ...

}",/content/Feature85.feature,"Scenario: Use JSON formatter with feature and one scenario with steps
        Given a file named ""features/scenario_with_steps.feature"" with:
            """"""
            Feature:
              Scenario: Simple scenario with with steps
                  Given a step passes
                  When a step passes
                  Then a step passes
                  And a step passes
                  But a step passes
            """"""
        When I run ""behave -f json.pretty features/scenario_with_steps.feature""
        Then it should pass with:
            """"""
            1 feature passed, 0 failed, 0 skipped
            1 scenario passed, 0 failed, 0 skipped
            """"""
        And the command output should contain:
            """"""
            ""steps"": [
                {
                  ""keyword"": ""Given"",
                  ""location"": ""features/scenario_with_steps.feature:2"",
                  ""match"": {
                    ""arguments"": [],
                    ""location"": ""features/steps/steps.py:3""
                  },
                  ""name"": ""a step passes"",
                  ""result"": {
                    ""duration"": XXX,
                    ""status"": ""passed"",
                  }
                  ""step_type"": ""given"",
                },
                {
                  ""keyword"": ""When"",
                  ""location"": ""features/scenario_with_steps.feature:3"",
                  ""match"": {
                    ""arguments"": [],
                    ""location"": ""features/steps/steps.py:3""
                  },
                  ""name"": ""a step passes"",
                  ""result"": {
                    ""duration"": XXX,
                    ""status"": ""passed"",
                  }
                  ""step_type"": ""when"",
                },
                {
                  ""keyword"": ""Then"",
                  ""location"": ""features/scenario_with_steps.feature:4"",
                  ""match"": {
                    ""arguments"": [],
                    ""location"": ""features/steps/steps.py:3""
                  },
                  ""name"": ""a step passes"",
                  ""result"": {
                    ""duration"": XXX,
                    ""status"": ""passed"",
                  }
                  ""step_type"": ""then"",
                },
                {
                  ""keyword"": ""And"",
                  ""location"": ""features/scenario_with_steps.feature:5"",
                  ""match"": {
                    ""arguments"": [],
                    ""location"": ""features/steps/steps.py:3""
                  },
                  ""name"": ""a step passes"",
                  ""result"": {
                    ""duration"": XXX,
                    ""status"": ""passed"",
                  }
                  ""step_type"": ""then"",
                },
                {
                  ""keyword"": ""But"",
                  ""location"": ""features/scenario_with_steps.feature:6"",
                  ""match"": {
                    ""arguments"": [],
                    ""location"": ""features/steps/steps.py:3""
                  },
                  ""name"": ""a step passes"",
                  ""result"": {
                    ""duration"": XXX,
                    ""status"": ""passed"",
                  }
                  ""step_type"": ""then"",
                }
            ],
            """""""
/content/Record86.rst,"{
<Scenario1>:  Cardinality: many this should be a comma-separated list, 
<Given1>:I meet
<When1>: I meet items{Alice, Bob, Charly}
<Then1>:this should be a comma-separated list like:

.. code-block:: gherkin

    Scenario:
        When I meet Alice
         And I meet Alice, Bob, Charly

}",/content/Feature86.feature,"{

  <Scenario1>:  Many list, comma-separated
  <Given1>:I go to a meeting
  <When1>:I meet Alice, Bob, Dodo
  <Then1>:the following persons are present:
      | name   |
      | Alice  |
      | Bob    |
      | Charly |
      | Dodo   |

}
"
/content/Record87.rst,"{
<Scenario1>: Sometimes a solution is needed where a list of one or more items needs
to be parsed. a list should be processed that is separated by the word ""and"",
<Given1>:When I meet
<When1>:I meet Alice, Bob, Charly,and (name)
<Then1>a list should be processed
}",/content/Feature87.feature,"{


  <Scenario1>: Many list with list-separator ""and""
  <Given1>:I go to a meeting
  <When1>:I meet Alice and Bob and Charly
  <Then1>:the following persons are present: name:Alice,Bob,Charly

}
"
/content/Record88.rst,"{
<Scenario1>:Cardinality: Zero (List of Type).Note that the case for zero or more items is not so often needed.

Initially, a comma-separated list is processed, like:

.. code-block:: gherkin

    Scenario:
        When I paint with red, green
<Given1>:zero items 
<When1>: Scenario:
        When I paint with

<Then1>: a comma-separated list is processed
}

 ",/content/Feature88.feature,"
{
<Scenario1>:Empty list, comma-separated
<Given1>:I am a painter
<When1>:I paint with
<Then1>:no colors are used
}
 "
/content/Record89.rst,"{
<Scenario4>: More (List of Type).
<Given4>: I paint and more items 
<When4>:I paint  more items
<Then4>:a list that is separated with the word ""and"" is processed, like:

.. code-block:: gherkin

    Scenario:
        When I paint with red and green
}",/content/Feature89.feature,"{
<Scenario4>: Many list with list-separator ""and""
<Given4>: I am a painter
<When4>:I paint with red and green and blue
<Then4>:the following colors are used color:red and green and blue
}"
/content/Record90.rst,"{
<Scenario3>:Cardinality:  More (List of Type)
<Given3>: I paint and more items
<When3>:I paint with red, green
<Then3>:Initially, a comma-separated list is processed, like:

.. code-block:: gherkin

    Scenario:
        When I paint with red, green
}",/content/Feature90.feature,"
{
<Scenario3>:Many list, comma-separated
<Given3>:I am a painter
<When3>:I paint with red, green
<Then3>:the following colors are used color:red,green

}
"
/content/Record91.rst,"{
<Scenario2>:Cardinality: Zero or More (List) a comma-separated list is processed
<Given2>:I am a painter and more then zero item
<Then2>:I paint with more then zero item.a comma-separated list is processed, like:

.. code-block:: gherkin

    Scenario:
        When I paint with red, green
}",/content/Feature91.feature,"{
<Scenario2>: List with one item, comma-separated
<Given2>:I am a painter
<Then2>:the following colors are used color:blue
}"
/content/Record92.rst,"{
<Scenario>:at least onestep is not matched from Provide the Step Definitions as defined in items: offered_shop_items = [ ""apples"", ""beef"", ""potatoes"", ""pork"" ]
<Given>""I buy {shop_item:ShopItem2}"")
    def step_impl(context, shop_item):
        # EXAMPLE: ""potatoes"" => (selected_index=2, selected_text=""potatoes"")
<When>  shop_item_id = context.shop.shop_item_index2id(selected_index)
<And> context.shopping_cart.append(shop_item_id)
<And> context.shopping_cart.append(shop_item_id)
}",/content/Feature92.feature,"{
<Scenario>: Bad Case -- Undefined step definition for ""diamonds""
<Given> I go to a shop to buy ingredients for a meal
<When> I buy apples
<And> I buy pork
<And> I buy diamonds
}
"
/content/Record93.rst,"{
<Scenario>:all steps is matched from Provide the Step Definitions as defined in items: offered_shop_items = [ ""apples"", ""beef"", ""potatoes"", ""pork"" ]
<Given>""I buy {shop_item:ShopItem2}"")
    def step_impl(context, shop_item):
        # EXAMPLE: ""potatoes"" => (selected_index=2, selected_text=""potatoes"")
<When>  shop_item_id = context.shop.shop_item_index2id(selected_index)
<And> context.shopping_cart.append(shop_item_id)
}",/content/Feature93.feature,"{
<Scenario>: Good Case
<Given>: I go to a shop to buy ingredients for a meal
<When>: I buy apples
<And> I buy beef

<Scenario>: Bad Case -- Undefined step definition for ""diamonds""
<Given> I go to a shop to buy ingredients for a meal
<When> I buy apples
<And> I buy pork
<And> I buy diamonds
}
"
/content/Record94.rst,"{
<Scenario>:
<When>:given enumeration of
    words/strings
<Then> stores them in :py:attr:`parse_yesno.pattern` attribute
}",/content/Feature94.feature,"{
<Scenario>:
<When>: Romeo asks Julia: ""Do you hate me?""
<Then> the answer is ""no""

}"
/content/Record95.rst,"{
<Scenario>:
<When> unique string-based words/strings
<Then> An enumeration maps a number of unique string-based words/strings to values
}",/content/Feature95.feature,"{
<Scenario>:
<When> Romeo asks Julia: ""Do you kiss me?""
<Then> the answer is ""jubilee""
}"
/content/Record96.rst,"{
<Scenario>:
<When>:Given enumeration of
    words/strings
<Then> An enumeration maps a number of unique string-based words/strings to values.stores them in :py:attr:`parse_yesno.pattern` attribute
}",/content/Feature96.feature,"{
<Scenario>:
<When>: Romeo asks Julia: ""Do you love me?""
<Then> the answer is ""yes""
}"
/content/Record97.rst,"{""<Scenario>"":""Goal: Show basics, make first steps Write the Feature Test"",
<Given>""First, install behave then Run behave Now, continue reading to learn how to make the most of behave."",
""<When>"":""The following feature file provides a simple feature with one scenario
in the known ``Given ... When ... Then ...`` language style (BDD).To be able to execute the feature file, you need to provide a thin
automation layer that represents the steps in the feature file
with Python functions. These step functions provide the test automation layer
(fixture code) that interacts with the ``system-under-test`` (SUT)."",
""<Then>"":""Run the Feature Test When you run the feature file from above (with coloring enabled),When you run the feature file from above (with coloring disabled),As alternative you can run the feature with plain formatting
(or another formatter)""}

Links:https://behave.readthedocs.io/en/stable/tutorial.html#features & behave.behave directory tutorial01.rst.txt
",/content/Feature97.feature,"{""<Scenario>"":""Run a simple test"",
""<Given>"":""We have behave installed"",
""<When>"":""We implement a test"",
""<Then>"":""Behave will test it for us!""}
"
/content/Record98.rst,"{
<Scenario1>:Fixtures using a decorator
<Given1>:You can define `Django fixtures`_ using a function decorator. It is merely
a convenient way to keep fixtures close to your steps.
<Then1>:The decorator will
load the fixtures in the ``before_scenario``, as documented above.
}

",/content/Feature98.feature,"{
<Scenario1>:Load fixtures with the decorator
<Given1>:a step with a fixture decorator
<Then1>:the fixture should be loaded
}
"
/content/Record99.rst,"{
<Scenario3>:If you wanted different fixtures for different scenarios
<Given3>:previously set fixtures carry # over to subsequent scenarios.
<Then3>:This fixture would then be loaded before every scenario.
}",/content/Feature99.feature,"{
<Scenario>:Load multiple fixtures and callables
<Given>:a step with multiple fixtures
<Then>:the fixture for the second scenario should be loaded
}"
/content/Record100.rst,"{
<Scenario>:Fixtures included with the decorator will apply to all other steps that  they share a scenario with.
<Given>:You can define `Django fixtures`_ using a function decorator. It is merely
a convenient way to keep fixtures close to your steps.
<Then>:per feature/scenario
}",/content/Feature100.feature,"{
<Scenario>:A Subsequent scenario should only load its fixtures
<Given>:a step with a second fixture decorator
<Then>:I should only have one object
}

"
/content/Record101.rst,"{
<Scenario>:Support for multiple databases.By default, Django only loads fixtures into the ``default`` database.
<Then>: Use ``before_scenario`` to load the fixtures in all of the databases you have
configured if your tests rely on the fixtures being loaded in all of them.

}",/content/Feature101.feature,"<Scenario>:Load fixtures with databases option
<Then>: databases should be set to all database in the Django settings"
/content/Record102.rst,"{
<Scenario>:Fixture Loading
<Then>: behave-django can load your fixtures for you per feature/scenario. There are
two approaches to this * loading the fixtures in ``environment.py``, or
* using a decorator on your step function

}",/content/Feature102.feature,"<Scenario>:Load fixtures
<Then>: the fixture should be loaded"
/content/Record103.rst,"{

<Scenario>:You could also have fixtures per Feature too
<Then>:the sequences should be reset # Resetting fixtures, otherwise previously set fixtures carry # over to subsequent features.

}",/content/Feature103.feature,"<Scenario3>:Load fixtures then reset sequences
<Then3>:the sequences should be reset"
/content/Record104.rst,"{

<Scenario>:in ``environment.py`` we can load our context with the fixtures array.
<Then>:This fixture would then be loaded before every scenario.

}",/content/Feature104.feature,"{
<Scenario>:Load fixtures for this scenario and feature
<Then>:the fixture for the second scenario should be loaded
}"
/content/Record105.rst,"{
<Scenario>:""Using Page Objects With *behave-django* you can use the `Page Object pattern`_ and work on a
natural abstraction layer for the content or behavior your web application
produces.  This is a popular approach to make your tests more stable and
your code easier to read."",
<When1>: 'I am on the Welcome page.A ``PageObject`` instance automatically loads and parses the page you
specify by its ``page`` attribute.'
<Then>:""'You then have access to the following attributes ``request``
    The HTTP request used by the Django test client to fetch the document.
    This is merely a convenient alias for ``response.request``.

``response``
    The Django test client's HTTP response object.  Use this to verify the
    actual HTTP response related to the retrieved document.

``document``
    The parsed content of the response.  This is, technically speaking, a
    `Beautiful Soup`_ object.  You *can* use this to access and verify any
    part of the document content, though it's recommended that you only
    access the elements you specify with the ``elements`` attribute, using
    the appropriate helper methods.""
<And>:""helpers to access your page object's elements:

``get_link(name) -> Link``
    A subdocument representing a HTML anchor link, retrieved from
    ``document`` using the CSS selector specified in ``elements[name]``.
    The returned ``Link`` object provides a ``click()`` method to trigger
    loading the link's URL, which again returns a ``PageObject``.""
<When2>:""I click on the ""About"" link""
<Then2>: ""The returned ``Link`` object provides a ``click()`` method to trigger
loading the link's URL, which again returns a ``PageObject``.The About page is loaded""
}
",/content/Feature105.feature,"{
<Scenario>: Welcome page object returns a valid (Beautiful Soup) document,
<When1>: I instantiate the Welcome page object
<Then>:it provides a valid Beautiful Soup document
<And>:get_link() returns the link subdocument
<When2>:I call click() on the link
<Then2>:it loads a new PageObject

}
"
/content/Record106.rst,"{
<Scenario>:Django’s Test Client
<When>:""If you only use Django's test client""
<Then>:""*behave* tests can run much
quicker with the ``--simple`` command line option.In this case transaction
rollback is used for test automation instead of flushing the database after
each scenario, just like in Django's standard ``TestCase``.

}",/content/Feature106.feature,"{
<Scenario>: Django's test client
When I use django's test client to visit ""/""
Then it should return a successful response
}"
